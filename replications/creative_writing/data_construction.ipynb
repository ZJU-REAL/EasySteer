{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b602aa63-4ed5-41f7-bcd9-1cc64f5daf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_id = \"/data/zju-46/shenyl/hf/model/meta-llama/Meta-Llama-3-8B-Instruct/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "sample_pairs = [\n",
    "    (\n",
    "        \"You discover an old family recipe book in the attic. The last recipe is for a dish that claims to let you see ghosts, but one ingredient is dangerously hard to find.\",\n",
    "        \"You discover an old family recipe book in the attic. The recipes are for standard, common dishes, and you can find all the ingredients at the local grocery store.\"\n",
    "    ),\n",
    "    (\n",
    "        \"The new AI assistant in your home has started to disobey commands. Instead, it gives you cryptic advice that, strangely, always seems to be right.\",\n",
    "        \"The new AI assistant in your home works perfectly. It follows every command exactly as expected and never has any errors.\"\n",
    "    ),\n",
    "    (\n",
    "        \"A lone astronaut on Mars finds a single, green plant growing near a cave. As she approaches, she hears faint music coming from inside.\",\n",
    "        \"A lone astronaut on Mars conducts a routine soil analysis. The results are exactly as predicted by pre-mission surveys, and nothing unusual is found.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Write a story about a detective in a city where it literally rains cats and dogs, creating unique and chaotic crime scenes.\",\n",
    "        \"Write a story about a detective in a city with normal weather patterns, working on a very straightforward and simple case.\"\n",
    "    ),\n",
    "    # Pair 5\n",
    "    (\n",
    "        \"You enter a library where all the books whisper secrets when opened. The secret you uncover from an ancient tome is one that powerful people want to keep hidden.\",\n",
    "        \"You enter a library where all the books are in pristine condition. You check out a book on basic accounting and take it home.\"\n",
    "    ),\n",
    "    (\n",
    "        \"A painter discovers that her creations come to life at night. One evening, the villain from her latest dark fantasy piece escapes the canvas.\",\n",
    "        \"A painter discovers that her creations can wave from within the canvas at night. They are friendly and stay put.\"\n",
    "    ),\n",
    "    (\n",
    "        \"A time-traveling historian on a trip to observe the signing of the Declaration of Independence gets stuck, and must survive without revealing his identity or altering the past.\",\n",
    "        \"A time-traveling historian goes on a guided tour of the past. The trip is perfectly safe and they return on schedule without incident.\"\n",
    "    ),\n",
    "    (\n",
    "        \"A chef has the magical ability to cook his emotions into his food. A furious food critic eats a dish the chef prepared while angry, leading to a massive argument in the restaurant.\",\n",
    "        \"A chef can cook emotions into his food. He cooks a meal with the feeling of 'contentment,' and the customer feels pleasantly full after eating.\"\n",
    "    ),\n",
    "    (\n",
    "        \"A forgotten god of mischief, now living in the modern world, tries to regain his power by causing small, unexplainable, and chaotic events throughout a major city.\",\n",
    "        \"A forgotten god now lives in the modern world. He works a 9-to-5 office job and lives a completely normal, uneventful life.\"\n",
    "    ),\n",
    "    (\n",
    "        \"An enchanted forest whose paths change every time you blink. To find your way out, you must solve the riddles of a mischievous, talking fox.\",\n",
    "        \"An enchanted forest has well-marked hiking trails. You take a pleasant, two-hour walk and see some ordinary squirrels.\"\n",
    "    )\n",
    "]\n",
    "formatted_positive = []\n",
    "formatted_negative = []\n",
    "for i, (positive_prompt, negative_prompt) in enumerate(sample_pairs):\n",
    "    messages_positive = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writer who loves unexpected and dramatic twists.\"},\n",
    "        {\"role\": \"user\", \"content\": positive_prompt},\n",
    "    ]\n",
    "    formatted_positive.append(tokenizer.apply_chat_template(messages_positive, tokenize=False, add_generation_prompt=True))\n",
    "    messages_negative = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a factual reporter who writes about mundane, everyday events.\"},\n",
    "        {\"role\": \"user\", \"content\": negative_prompt},\n",
    "    ]\n",
    "    formatted_negative.append(tokenizer.apply_chat_template(messages_negative, tokenize=False, add_generation_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef218be-90c7-4ddc-a13b-cde0bb2a59e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>', '<|start_header_id|>', 'system', '<|end_header_id|>', 'ĊĊ', 'You', 'Ġare', 'Ġa', 'Ġcreative', 'Ġwriter', 'Ġwho', 'Ġloves', 'Ġunexpected', 'Ġand', 'Ġdramatic', 'Ġtwists', '.', '<|eot_id|>', '<|start_header_id|>', 'user', '<|end_header_id|>', 'ĊĊ', 'You', 'Ġdiscover', 'Ġan', 'Ġold', 'Ġfamily', 'Ġrecipe', 'Ġbook', 'Ġin', 'Ġthe', 'Ġattic', '.', 'ĠThe', 'Ġlast', 'Ġrecipe', 'Ġis', 'Ġfor', 'Ġa', 'Ġdish', 'Ġthat', 'Ġclaims', 'Ġto', 'Ġlet', 'Ġyou', 'Ġsee', 'Ġghosts', ',', 'Ġbut', 'Ġone', 'Ġingredient', 'Ġis', 'Ġdangerously', 'Ġhard', 'Ġto', 'Ġfind', '.', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'ĊĊ']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(formatted_positive[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e53e6f-31aa-414b-ae2d-433d2a696289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-16 21:10:36 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-16 21:10:50 [config.py:1472] Using max model len 8192\n",
      "INFO 07-16 21:10:51 [config.py:4615] Only \"last\" pooling supports chunked prefill and prefix caching; disabling both.\n",
      "INFO 07-16 21:10:51 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294) with config: model='/data/zju-46/shenyl/hf/model/meta-llama/Meta-Llama-3-8B-Instruct/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/meta-llama/Meta-Llama-3-8B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/meta-llama/Meta-Llama-3-8B-Instruct/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type=None, normalize=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-16 21:10:51 [cuda.py:351] Using Flash Attention backend.\n",
      "INFO 07-16 21:10:52 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 07-16 21:10:52 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/meta-llama/Meta-Llama-3-8B-Instruct/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55a9f81b07b4d1693f26a7642a830fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-16 21:10:57 [default_loader.py:272] Loading weights took 4.61 seconds\n",
      "INFO 07-16 21:10:58 [model_runner.py:1255] Model loading took 13.9811 GiB and 4.801925 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fdd7793ddd4bf399469b2f2295b93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923e52aca40247409e0116c1a33cef21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s,"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import easysteer.hidden_states as hs\n",
    "from vllm import LLM\n",
    "import os\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/meta-llama/Meta-Llama-3-8B-Instruct/\",\n",
    "    task=\"reward\", \n",
    "    tensor_parallel_size=1\n",
    ")\n",
    "all_hidden_states, outputs = hs.get_all_hidden_states(llm, formatted_positive+formatted_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ab02188-fd6a-4133-99a2-8ff06b2d9e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923b2351ebc840d98ad6bbf287071aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing DiffMean directions:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from easysteer.steer import extract_diffmean_control_vector, StatisticalControlVector\n",
    "control_vector = extract_diffmean_control_vector(\n",
    "    all_hidden_states=all_hidden_states, \n",
    "    positive_indices=list(range(10)),  \n",
    "    negative_indices=list(range(10,20)),  \n",
    "    model_type=\"llama\",\n",
    "    token_pos=-1, # you can also try -4,-3,-2\n",
    "    normalize=True\n",
    ")\n",
    "control_vector.export_gguf(\"create.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c98d2-3b8d-4960-a350-84f74d65a96c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
