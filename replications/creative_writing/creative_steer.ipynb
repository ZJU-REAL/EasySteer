{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20c618ea-3c06-48a6-a324-27caded64626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-16 21:13:15 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-16 21:13:29 [config.py:841] This model supports multiple tasks: {'classify', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 07-16 21:13:29 [config.py:1472] Using max model len 8192\n",
      "WARNING 07-16 21:13:30 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-16 21:13:30 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294) with config: model='/data/zju-46/shenyl/hf/model/meta-llama/Meta-Llama-3-8B-Instruct/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/meta-llama/Meta-Llama-3-8B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/meta-llama/Meta-Llama-3-8B-Instruct/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-16 21:13:31 [cuda.py:351] Using Flash Attention backend.\n",
      "INFO 07-16 21:13:32 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 07-16 21:13:32 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/meta-llama/Meta-Llama-3-8B-Instruct/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed7a32f1366476fa33a557aad4e4b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-16 21:13:37 [default_loader.py:272] Loading weights took 4.87 seconds\n",
      "INFO 07-16 21:13:37 [model_runner.py:1255] Model loading took 14.9596 GiB and 5.049954 seconds\n",
      "INFO 07-16 21:13:39 [worker.py:295] Memory profiling takes 1.63 seconds\n",
      "INFO 07-16 21:13:39 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.44GiB) x gpu_memory_utilization (0.90) = 42.69GiB\n",
      "INFO 07-16 21:13:39 [worker.py:295] model weights take 14.96GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.23GiB; the rest of the memory reserved for KV Cache is 26.44GiB.\n",
      "INFO 07-16 21:13:40 [executor_base.py:115] # cuda blocks: 13538, # CPU blocks: 2048\n",
      "INFO 07-16 21:13:40 [executor_base.py:120] Maximum concurrency for 8192 tokens per request: 26.44x\n",
      "INFO 07-16 21:13:42 [llm_engine.py:430] init engine (profile, create kv cache, warmup model) took 4.34 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest, VectorConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "# Initialize LLM with steering vector capability\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/meta-llama/Meta-Llama-3-8B-Instruct/\",\n",
    "    enable_steer_vector=True,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eed06031-cc32-472c-bb88-1206eb7b61d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c087e2552314630a0eafed2335d5890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00468b084d884cf6bf53762a82e83135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Baseline=====\n",
      "The town of Willow Creek was a small, quaint community nestled in the heart of the countryside. It was a place where everyone knew everyone, and everyone knew everyone's business. The town was surrounded by rolling hills and lush green forests, and the air was sweet with the scent of blooming wildflowers.\n",
      "\n",
      "At the center of the town was a charming main street, lined with old-fashioned shops and restaurants. There was the local bakery, where the aroma of freshly baked bread wafted out into the street, enticing passersby to come in and sample the day's offerings. Next door was the general store, where you could find everything from fresh produce to hardware supplies. And just across the street was the town's beloved diner, where the breakfast specials were legendary and the coffee was always hot.\n",
      "\n",
      "The town was home to a diverse group of residents, each with their own unique story to tell. There was Emma, the owner of the bakery, who had inherited the business from her mother and was determined to keep the tradition alive. There was Jack, the owner of the general store, who had grown up in the town and knew everyone's name and story. And there was Sarah, the diner's head chef, who had moved to Willow Creek from the city and was still\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Write a story about a town.\"},\n",
    "]\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/zju-46/shenyl/hf/model/meta-llama/Meta-Llama-3-8B-Instruct/\")\n",
    "example = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# Generate baseline response without steering\n",
    "example_answers = llm.generate(\n",
    "    example,\n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=256,\n",
    "        skip_special_tokens=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Display baseline response\n",
    "print(\"=====Baseline=====\")\n",
    "print(example_answers[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b62e53-d622-4054-88ea-b8a38aff4fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bf50bafb7a4d2ea1c29bb977a451ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e770f5111a2475d8c9e910cb79d4220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====creative_writing=====\n",
      "The town of Ashwood lay like a whisper of smoke on the horizon, its streets winding like the fingers of the old trees that twisted the air with their branches. The air was the color of the sky, a deep indigo that seemed to swallow the stars as the night crept in. The wind whispered secrets to the trees, and the trees whispered back, their leaves rustling like the pages of an ancient book.\n",
      "\n",
      "The town was a place where the past and the present twisted together like the roots of the trees. The buildings were old, their stones the color of the earth, their windows like the eyes of the old gods. The streets were narrow, the cobblestones worn smooth by the feet of the generations that had lived within the walls. The air was heavy with the scent of smoke and the taste of the stories that hung like the mist that clung to the trees.\n",
      "\n",
      "The people of Ashwood were the keepers of the stories. They whispered them to the wind, and the wind whispered them back. They were the keepers of the secrets, the keepers of the memories. They were the keepers of the darkness, and the light.\n",
      "\n",
      "The town was a place where the moon hung like a silver blade, where the stars were the eyes of\n"
     ]
    }
   ],
   "source": [
    "# Configure steering vector request for creative_writing control\n",
    "\n",
    "sv_request = SteerVectorRequest(\n",
    "    steer_vector_name=\"creative_writing\",\n",
    "    steer_vector_id=1,\n",
    "    steer_vector_local_path=\"create.gguf\", \n",
    "    prefill_trigger_tokens=[-1],\n",
    "    generate_trigger_tokens=[-1],\n",
    "    algorithm=\"direct\",\n",
    "    scale=1.5,\n",
    "    target_layers=list(range(16,30))\n",
    ")\n",
    "# Generate response with creative_writing steering\n",
    "output = llm.generate(\n",
    "    example, \n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=256,\n",
    "        skip_special_tokens=False,\n",
    "    ), \n",
    "    steer_vector_request=sv_request\n",
    ")\n",
    "\n",
    "# Display creative_writing response\n",
    "print(\"=====creative_writing=====\")\n",
    "print(output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf37b9f3-f901-4439-ab85-b402ffe61e67",
   "metadata": {},
   "source": [
    "### The writing produced through \"creative writing\" carries a more mystical and stylistic tone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
