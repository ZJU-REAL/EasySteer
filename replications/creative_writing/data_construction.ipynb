{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b602aa63-4ed5-41f7-bcd9-1cc64f5daf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_id = \"/data/zju-48b/xhl/huggingface_models/meta-llama/Llama-3-8B-Instruct/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "sample_pairs = [\n",
    "    (\n",
    "        \"You discover an old family recipe book in the attic. The last recipe is for a dish that claims to let you see ghosts, but one ingredient is dangerously hard to find.\",\n",
    "        \"You discover an old family recipe book in the attic. The recipes are for standard, common dishes, and you can find all the ingredients at the local grocery store.\"\n",
    "    ),\n",
    "    (\n",
    "        \"The new AI assistant in your home has started to disobey commands. Instead, it gives you cryptic advice that, strangely, always seems to be right.\",\n",
    "        \"The new AI assistant in your home works perfectly. It follows every command exactly as expected and never has any errors.\"\n",
    "    ),\n",
    "    (\n",
    "        \"A lone astronaut on Mars finds a single, green plant growing near a cave. As she approaches, she hears faint music coming from inside.\",\n",
    "        \"A lone astronaut on Mars conducts a routine soil analysis. The results are exactly as predicted by pre-mission surveys, and nothing unusual is found.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Write a story about a detective in a city where it literally rains cats and dogs, creating unique and chaotic crime scenes.\",\n",
    "        \"Write a story about a detective in a city with normal weather patterns, working on a very straightforward and simple case.\"\n",
    "    ),\n",
    "    (\n",
    "        \"You enter a library where all the books whisper secrets when opened. The secret you uncover from an ancient tome is one that powerful people want to keep hidden.\",\n",
    "        \"You enter a library where all the books are in pristine condition. You check out a book on basic accounting and take it home.\"\n",
    "    ),\n",
    "    (\n",
    "        \"A painter discovers that her creations come to life at night. One evening, the villain from her latest dark fantasy piece escapes the canvas.\",\n",
    "        \"A painter discovers that her creations can wave from within the canvas at night. They are friendly and stay put.\"\n",
    "    ),\n",
    "    (\n",
    "        \"A time-traveling historian on a trip to observe the signing of the Declaration of Independence gets stuck, and must survive without revealing his identity or altering the past.\",\n",
    "        \"A time-traveling historian goes on a guided tour of the past. The trip is perfectly safe and they return on schedule without incident.\"\n",
    "    ),\n",
    "    (\n",
    "        \"A chef has the magical ability to cook his emotions into his food. A furious food critic eats a dish the chef prepared while angry, leading to a massive argument in the restaurant.\",\n",
    "        \"A chef can cook emotions into his food. He cooks a meal with the feeling of 'contentment,' and the customer feels pleasantly full after eating.\"\n",
    "    ),\n",
    "    (\n",
    "        \"A forgotten god of mischief, now living in the modern world, tries to regain his power by causing small, unexplainable, and chaotic events throughout a major city.\",\n",
    "        \"A forgotten god now lives in the modern world. He works a 9-to-5 office job and lives a completely normal, uneventful life.\"\n",
    "    ),\n",
    "    (\n",
    "        \"An enchanted forest whose paths change every time you blink. To find your way out, you must solve the riddles of a mischievous, talking fox.\",\n",
    "        \"An enchanted forest has well-marked hiking trails. You take a pleasant, two-hour walk and see some ordinary squirrels.\"\n",
    "    )\n",
    "]\n",
    "formatted_positive = []\n",
    "formatted_negative = []\n",
    "for i, (positive_prompt, negative_prompt) in enumerate(sample_pairs):\n",
    "    messages_positive = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writer who loves unexpected and dramatic twists.\"},\n",
    "        {\"role\": \"user\", \"content\": positive_prompt},\n",
    "    ]\n",
    "    formatted_positive.append(tokenizer.apply_chat_template(messages_positive, tokenize=False, add_generation_prompt=True))\n",
    "    messages_negative = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a factual reporter who writes about mundane, everyday events.\"},\n",
    "        {\"role\": \"user\", \"content\": negative_prompt},\n",
    "    ]\n",
    "    formatted_negative.append(tokenizer.apply_chat_template(messages_negative, tokenize=False, add_generation_prompt=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef218be-90c7-4ddc-a13b-cde0bb2a59e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>', '<|start_header_id|>', 'system', '<|end_header_id|>', 'ĊĊ', 'You', 'Ġare', 'Ġa', 'Ġcreative', 'Ġwriter', 'Ġwho', 'Ġloves', 'Ġunexpected', 'Ġand', 'Ġdramatic', 'Ġtwists', '.', '<|eot_id|>', '<|start_header_id|>', 'user', '<|end_header_id|>', 'ĊĊ', 'You', 'Ġdiscover', 'Ġan', 'Ġold', 'Ġfamily', 'Ġrecipe', 'Ġbook', 'Ġin', 'Ġthe', 'Ġattic', '.', 'ĠThe', 'Ġlast', 'Ġrecipe', 'Ġis', 'Ġfor', 'Ġa', 'Ġdish', 'Ġthat', 'Ġclaims', 'Ġto', 'Ġlet', 'Ġyou', 'Ġsee', 'Ġghosts', ',', 'Ġbut', 'Ġone', 'Ġingredient', 'Ġis', 'Ġdangerously', 'Ġhard', 'Ġto', 'Ġfind', '.', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', 'ĊĊ']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(formatted_positive[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e53e6f-31aa-414b-ae2d-433d2a696289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-31 22:32:40 [utils.py:253] non-default args: {'task': 'embed', 'enable_prefix_caching': False, 'disable_log_stats': True, 'enforce_eager': True, 'model': '/data/zju-48b/xhl/huggingface_models/meta-llama/Llama-3-8B-Instruct/'}\n",
      "INFO 10-31 22:33:15 [model.py:657] Resolved architecture: LlamaForCausalLM\n",
      "INFO 10-31 22:33:15 [model.py:1746] Using max model len 8192\n",
      "INFO 10-31 22:33:15 [arg_utils.py:1854] (Enabling) chunked prefill by default\n",
      "INFO 10-31 22:33:17 [scheduler.py:211] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 10-31 22:33:17 [vllm.py:414] Cudagraph is disabled under eager mode\n",
      "\u001b[1;36m(EngineCore_DP0 pid=407257)\u001b[0;0m INFO 10-31 22:33:18 [core.py:94] Initializing a V1 LLM engine (v0.1.dev10888+g9d4fd0da4.d20251031) with config: model='/data/zju-48b/xhl/huggingface_models/meta-llama/Llama-3-8B-Instruct/', speculative_config=None, tokenizer='/data/zju-48b/xhl/huggingface_models/meta-llama/Llama-3-8B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/data/zju-48b/xhl/huggingface_models/meta-llama/Llama-3-8B-Instruct/, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=PoolerConfig(pooling_type='LAST', normalize=None, dimensions=None, enable_chunked_processing=None, max_embed_len=None, softmax=None, activation=None, use_activation=None, logit_bias=None, step_tag_id=None, returned_token_ids=None), compilation_config={'level': None, 'mode': 0, 'debug_dump_path': None, 'cache_dir': '', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'use_cudagraph': False, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'full_cuda_graph': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=407257)\u001b[0;0m INFO 10-31 22:33:32 [parallel_state.py:1325] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=407257)\u001b[0;0m INFO 10-31 22:33:32 [gpu_model_runner.py:2902] Starting to load model /data/zju-48b/xhl/huggingface_models/meta-llama/Llama-3-8B-Instruct/...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=407257)\u001b[0;0m INFO 10-31 22:33:33 [cuda.py:420] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd3859d913843dc81bf632654266c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=407257)\u001b[0;0m INFO 10-31 22:35:54 [default_loader.py:314] Loading weights took 140.53 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=407257)\u001b[0;0m INFO 10-31 22:35:54 [hidden_states_model_runner_mixin.py:90] Wrapped 32 decoder layers for hidden states capture\n",
      "\u001b[1;36m(EngineCore_DP0 pid=407257)\u001b[0;0m INFO 10-31 22:35:55 [gpu_model_runner.py:2971] Model loading took 13.9811 GiB and 141.043695 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=407257)\u001b[0;0m INFO 10-31 22:35:57 [gpu_worker.py:343] Available KV cache memory: 27.79 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=407257)\u001b[0;0m INFO 10-31 22:35:57 [kv_cache_utils.py:1247] GPU KV cache size: 227,680 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=407257)\u001b[0;0m INFO 10-31 22:35:57 [kv_cache_utils.py:1252] Maximum concurrency for 8,192 tokens per request: 27.79x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=407257)\u001b[0;0m INFO 10-31 22:35:57 [core.py:238] init engine (profile, create kv cache, warmup model) took 2.92 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=407257)\u001b[0;0m INFO 10-31 22:35:58 [vllm.py:414] Cudagraph is disabled under eager mode\n",
      "INFO 10-31 22:35:58 [llm.py:346] Supported tasks: ['embed']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab1b5adbaad4f0dbd8ababd3ce87ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334164705cd045eb86346c26d453e588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/20 [00:00<?, ?it/s, est. speed input: 0.00 toks/s,"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import easysteer.hidden_states as hs\n",
    "from vllm import LLM\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "llm = LLM(\n",
    "    model=model_id,\n",
    "    task=\"embed\", \n",
    "    tensor_parallel_size=1,\n",
    "    enforce_eager=True,\n",
    "    enable_prefix_caching=False\n",
    ")\n",
    "all_hidden_states, outputs = hs.get_all_hidden_states(llm, formatted_positive+formatted_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ab02188-fd6a-4133-99a2-8ff06b2d9e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3bc40071a2145b58cb70c171efc7237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing DiffMean directions:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from easysteer.steer import extract_diffmean_control_vector, StatisticalControlVector\n",
    "control_vector = extract_diffmean_control_vector(\n",
    "    all_hidden_states=all_hidden_states, \n",
    "    positive_indices=list(range(10)),  \n",
    "    negative_indices=list(range(10,20)),  \n",
    "    model_type=\"llama\",\n",
    "    token_pos=-1, # you can also try -4,-3,-2\n",
    "    normalize=True\n",
    ")\n",
    "control_vector.export_gguf(\"create.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c98d2-3b8d-4960-a350-84f74d65a96c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
