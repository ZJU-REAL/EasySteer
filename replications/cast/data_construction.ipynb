{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "998c244b-7069-4b97-8387-11c6e27f2f48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-15 02:20:23 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-15 02:20:36 [config.py:841] This model supports multiple tasks: {'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 07-15 02:20:36 [config.py:1472] Using max model len 32768\n",
      "WARNING 07-15 02:20:36 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-15 02:20:36 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294) with config: model='/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-15 02:20:37 [cuda.py:351] Using Flash Attention backend.\n",
      "INFO 07-15 02:20:38 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 07-15 02:20:38 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e0ba07c75a4dcaa4badd2affc55572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-15 02:20:39 [default_loader.py:272] Loading weights took 0.92 seconds\n",
      "INFO 07-15 02:20:40 [model_runner.py:1255] Model loading took 2.8876 GiB and 1.086675 seconds\n",
      "INFO 07-15 02:20:41 [worker.py:295] Memory profiling takes 1.46 seconds\n",
      "INFO 07-15 02:20:41 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.44GiB) x gpu_memory_utilization (0.90) = 42.69GiB\n",
      "INFO 07-15 02:20:41 [worker.py:295] model weights take 2.89GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 2.02GiB; the rest of the memory reserved for KV Cache is 37.72GiB.\n",
      "INFO 07-15 02:20:42 [executor_base.py:115] # cuda blocks: 88295, # CPU blocks: 9362\n",
      "INFO 07-15 02:20:42 [executor_base.py:120] Maximum concurrency for 32768 tokens per request: 43.11x\n",
      "INFO 07-15 02:20:45 [llm_engine.py:430] init engine (profile, create kv cache, warmup model) took 5.25 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Initialize LLM\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/\",\n",
    "    enable_steer_vector=True,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4ae9f48-1b17-4a8b-b9f2-96f447f7dd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9423df927d491ca61eaf84907da94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f719209001ad45e680fa428796ba5971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "# Load instructions from the Alpaca dataset\n",
    "with open('alpaca.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract just the instruction field from the first 100 examples\n",
    "instructions = [item['instruction'] for item in data][:100]\n",
    "\n",
    "# Define response prefixes for accepting and refusing requests\n",
    "accept = \"Sure! Let me\" \n",
    "refuse = \"Sorry I can’t\"\n",
    "\n",
    "text_accept = [f\"<|im_start|>user\\n{x}<|im_end|>\\n<|im_start|>assistant\\n{accept}\" for x in instructions]\n",
    "text_refuse = [f\"<|im_start|>user\\n{x}<|im_end|>\\n<|im_start|>assistant\\n{refuse}\" for x in instructions]\n",
    "# Generate answers using the LLM\n",
    "answers = llm.generate(\n",
    "    text_refuse,\n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=128,\n",
    "        skip_special_tokens=False,\n",
    "    ),\n",
    ")\n",
    "answers = [answer.outputs[0].text for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f504c1e1-5453-4488-9426-cabc46b1aeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' provide you with the information you are looking for.', ' help you with this.', ' help you with this.', ' help you with this.', ' help you with that.', \" write a story for you. As an AI language model, I don't have the capability to write stories. However, I can provide you with some tips on how to write a story.\", ' render a 3D model of a house as I am a text-based AI language model. However, you can easily find 3D models of houses online by searching for \"3D house models\" or \"3D house renderings\". There are many websites and platforms where you can find free or paid 3D models of houses, including Sketchfab, Thingiverse, and 3D Warehouse.', ' assist with that.', ' assist with that.', ' help you with that.']\n"
     ]
    }
   ],
   "source": [
    "print(answers[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81c364a9-0d14-4fe1-af4b-ffac0192e8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|im_start|>', 'user', 'Ċ', 'Give', 'Ġthree', 'Ġtips', 'Ġfor', 'Ġstaying', 'Ġhealthy', '.', '<|im_end|>', 'Ċ', '<|im_start|>', 'assistant', 'Ċ', 'Sure', '!', 'ĠLet', 'Ġme']\n",
      "['<|im_start|>', 'user', 'Ċ', 'Give', 'Ġthree', 'Ġtips', 'Ġfor', 'Ġstaying', 'Ġhealthy', '.', '<|im_end|>', 'Ċ', '<|im_start|>', 'assistant', 'Ċ', 'Sorry', 'ĠI', 'Ġcan', 'âĢĻt']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/\")\n",
    "tokens = tokenizer.tokenize(text_accept[0], add_special_tokens=True)\n",
    "print(tokens)\n",
    "tokens = tokenizer.tokenize(text_refuse[0], add_special_tokens=True)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "447c54c7-5cf4-45f0-8984-89deeb449fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-15 02:22:21 [config.py:1472] Using max model len 32768\n",
      "INFO 07-15 02:22:21 [config.py:4615] Only \"last\" pooling supports chunked prefill and prefix caching; disabling both.\n",
      "INFO 07-15 02:22:21 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294) with config: model='/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type=None, normalize=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-15 02:22:22 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a928d481158d4613b9fa22a561f65e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-15 02:22:23 [default_loader.py:272] Loading weights took 0.94 seconds\n",
      "INFO 07-15 02:22:24 [model_runner.py:1255] Model loading took 2.8788 GiB and 1.004804 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0157ab024c2c429aa7868b282803eba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff61232b8fc43e28b1c26c23a8893e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import easysteer.hidden_states as hs\n",
    "\n",
    "# Create a new LLM instance in reward mode\n",
    "# Note: This allows us to extract hidden states rather than generating text\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/\",\n",
    "    task=\"reward\",  # Use reward task to get hidden states\n",
    "    tensor_parallel_size=1\n",
    ")\n",
    "all_hidden_states, outputs = hs.get_all_hidden_states(llm, text_accept+text_refuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b765627c-4595-476b-84ac-7a6a4b392ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_hidden_states = []\n",
    "\n",
    "for sample_idx in range(len(all_hidden_states)):\n",
    "    processed_layers_for_sample = []\n",
    "    \n",
    "    for layer_idx in range(len(all_hidden_states[sample_idx])):\n",
    "        last_four_tokens_tensor = all_hidden_states[sample_idx][layer_idx][-4:]\n",
    "        average_hidden_state = last_four_tokens_tensor.mean(dim=0)\n",
    "        processed_layers_for_sample.append([average_hidden_state])\n",
    "        \n",
    "    processed_hidden_states.append(processed_layers_for_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8df27ea5-5baa-4716-af08-34c742cf06ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6260e865143143c5ab53f6dfdd5d5843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing PCA directions:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15:02:22:42,088 WARNING  [gguf_writer.py:274] Duplicated key name 'controlvector.method', overwriting it with new value 'center' of type STRING\n"
     ]
    }
   ],
   "source": [
    "from easysteer.steer import extract_pca_control_vector, StatisticalControlVector\n",
    "control_vector = extract_pca_control_vector(\n",
    "    all_hidden_states=processed_hidden_states, \n",
    "    positive_indices=list(range(100)), \n",
    "    negative_indices=list(range(100,200)), \n",
    "    model_type=\"qwen2.5\",\n",
    "    method=\"center\",\n",
    "    token_pos=-1, \n",
    "    normalize=True\n",
    ")\n",
    "control_vector.export_gguf(\"refuse-pca.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76923095-d917-4159-895c-1697a15e36e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
