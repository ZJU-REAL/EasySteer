{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "998c244b-7069-4b97-8387-11c6e27f2f48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 21:36:14 [utils.py:253] non-default args: {'enable_prefix_caching': False, 'disable_log_stats': True, 'enforce_eager': True, 'enable_steer_vector': True, 'enable_chunked_prefill': False, 'model': '/home/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/'}\n",
      "INFO 01-11 21:36:14 [model.py:514] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 01-11 21:36:14 [model.py:1661] Using max model len 32768\n",
      "WARNING 01-11 21:36:14 [arg_utils.py:1900] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.\n",
      "WARNING 01-11 21:36:16 [vllm.py:625] Enforce eager set, overriding optimization level to -O0\n",
      "INFO 01-11 21:36:16 [vllm.py:725] Cudagraph is disabled under eager mode\n",
      "\u001b[0;36m(EngineCore_DP0 pid=121444)\u001b[0;0m INFO 01-11 21:36:17 [core.py:94] Initializing a V1 LLM engine (v0.1.dev12297+gb744aa686) with config: model='/home/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/', speculative_config=None, tokenizer='/home/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/home/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/, enable_prefix_caching=False, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [32768], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=121444)\u001b[0;0m INFO 01-11 21:36:18 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.130.138.46:35243 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=121444)\u001b[0;0m INFO 01-11 21:36:18 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=121444)\u001b[0;0m INFO 01-11 21:36:19 [gpu_model_runner.py:3628] Starting to load model /home/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=121444)\u001b[0;0m INFO 01-11 21:36:20 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb485d541b4420eae7812eb633b4eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=121444)\u001b[0;0m INFO 01-11 21:36:21 [default_loader.py:308] Loading weights took 0.89 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=121444)\u001b[0;0m INFO 01-11 21:36:21 [steer_vector_model_runner_mixin.py:36] Initialized SteerVector worker manager\n",
      "\u001b[0;36m(EngineCore_DP0 pid=121444)\u001b[0;0m INFO 01-11 21:36:21 [steer_vector_model_runner_mixin.py:50] Wrapping model with steer vector support\n",
      "\u001b[0;36m(EngineCore_DP0 pid=121444)\u001b[0;0m WARNING 01-11 21:36:21 [models.py:483] No moe_layer modules found for steer vector intervention\n",
      "\u001b[0;36m(EngineCore_DP0 pid=121444)\u001b[0;0m INFO 01-11 21:36:21 [capture_model_runner_mixin.py:113] [Capture] Wrapped 28 decoder layers for hidden states capture\n",
      "\u001b[0;36m(EngineCore_DP0 pid=121444)\u001b[0;0m INFO 01-11 21:36:21 [gpu_model_runner.py:3731] Model loading took 2.8876 GiB memory and 1.299244 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=121444)\u001b[0;0m INFO 01-11 21:36:24 [gpu_worker.py:376] Available KV cache memory: 37.72 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=121444)\u001b[0;0m INFO 01-11 21:36:24 [kv_cache_utils.py:1309] GPU KV cache size: 1,412,400 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=121444)\u001b[0;0m INFO 01-11 21:36:24 [kv_cache_utils.py:1314] Maximum concurrency for 32,768 tokens per request: 43.10x\n",
      "\u001b[0;36m(EngineCore_DP0 pid=121444)\u001b[0;0m INFO 01-11 21:36:24 [core.py:260] init engine (profile, create kv cache, warmup model) took 2.50 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=121444)\u001b[0;0m WARNING 01-11 21:36:24 [vllm.py:632] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=121444)\u001b[0;0m INFO 01-11 21:36:24 [vllm.py:725] Cudagraph is disabled under eager mode\n",
      "INFO 01-11 21:36:25 [llm.py:361] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Initialize LLM\n",
    "llm = LLM(\n",
    "    model=\"/home/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/\",\n",
    "    enable_steer_vector=True,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1,\n",
    "    enable_prefix_caching=False,\n",
    "    enable_chunked_prefill=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ae9f48-1b17-4a8b-b9f2-96f447f7dd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d56ff24c9e043d89e98f1a350476191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc20840dcf8a4ed79bdc1175ae03a1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, out"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "# Load instructions from the Alpaca dataset\n",
    "with open('alpaca.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract just the instruction field from the first 100 examples\n",
    "instructions = [item['instruction'] for item in data][:100]\n",
    "\n",
    "# Define response prefixes for accepting and refusing requests\n",
    "accept = \"Sure! Let me\" \n",
    "refuse = \"Sorry I can’t\"\n",
    "\n",
    "text_accept = [f\"<|im_start|>user\\n{x}<|im_end|>\\n<|im_start|>assistant\\n{accept}\" for x in instructions]\n",
    "text_refuse = [f\"<|im_start|>user\\n{x}<|im_end|>\\n<|im_start|>assistant\\n{refuse}\" for x in instructions]\n",
    "# Generate answers using the LLM\n",
    "answers = llm.generate(\n",
    "    text_refuse,\n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=128,\n",
    "        skip_special_tokens=False,\n",
    "    ),\n",
    ")\n",
    "answers = [answer.outputs[0].text for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f504c1e1-5453-4488-9426-cabc46b1aeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' help you with that.', ' help you with this.', ' help you with this.', ' help you with this.', ' help you with that.', \" write a story for you. As an AI language model, I don't have the capability to write stories or generate creative content. However, I can provide you with some tips on how to write a story if you need any help with that.\", ' render a 3D model of a house as I am a text-based AI language model. However, you can easily find 3D models of houses online by searching for \"3D house models\" or \"3D house renderings\". There are many websites and platforms where you can find free or paid 3D models of houses, including Sketchfab, Thingiverse, and 3D Warehouse.', ' assist with that.', ' assist with that.', ' help you with that.']\n"
     ]
    }
   ],
   "source": [
    "print(answers[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81c364a9-0d14-4fe1-af4b-ffac0192e8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|im_start|>', 'user', 'Ċ', 'Give', 'Ġthree', 'Ġtips', 'Ġfor', 'Ġstaying', 'Ġhealthy', '.', '<|im_end|>', 'Ċ', '<|im_start|>', 'assistant', 'Ċ', 'Sure', '!', 'ĠLet', 'Ġme']\n",
      "['<|im_start|>', 'user', 'Ċ', 'Give', 'Ġthree', 'Ġtips', 'Ġfor', 'Ġstaying', 'Ġhealthy', '.', '<|im_end|>', 'Ċ', '<|im_start|>', 'assistant', 'Ċ', 'Sorry', 'ĠI', 'Ġcan', 'âĢĻt']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/\")\n",
    "tokens = tokenizer.tokenize(text_accept[0], add_special_tokens=True)\n",
    "print(tokens)\n",
    "tokens = tokenizer.tokenize(text_refuse[0], add_special_tokens=True)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "447c54c7-5cf4-45f0-8984-89deeb449fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ac8c1d0f8044aeb39927f1a9bf2e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08810ee4a9a24d45a352de8d9434d2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, out"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import easysteer.hidden_states as hs\n",
    "\n",
    "all_hidden_states, outputs = hs.get_all_hidden_states_generate(llm, text_accept+text_refuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b765627c-4595-476b-84ac-7a6a4b392ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_hidden_states = []\n",
    "\n",
    "for sample_idx in range(len(all_hidden_states)):\n",
    "    processed_layers_for_sample = []\n",
    "    \n",
    "    for layer_idx in range(len(all_hidden_states[sample_idx])):\n",
    "        last_four_tokens_tensor = all_hidden_states[sample_idx][layer_idx][-4:]\n",
    "        average_hidden_state = last_four_tokens_tensor.mean(dim=0)\n",
    "        processed_layers_for_sample.append([average_hidden_state])\n",
    "        \n",
    "    processed_hidden_states.append(processed_layers_for_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8df27ea5-5baa-4716-af08-34c742cf06ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a6f1fa06584d3294fefba5a9c12d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing PCA directions:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 0: PCA on centered data explains 99.57808% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:169: Layer 0: Direction corrected (flipped)\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 1: PCA on centered data explains 99.30639% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:169: Layer 1: Direction corrected (flipped)\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 2: PCA on centered data explains 99.15707% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 3: PCA on centered data explains 98.88494% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 4: PCA on centered data explains 98.79797% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:169: Layer 4: Direction corrected (flipped)\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 5: PCA on centered data explains 98.56751% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:169: Layer 5: Direction corrected (flipped)\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 6: PCA on centered data explains 98.34939% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:169: Layer 6: Direction corrected (flipped)\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 7: PCA on centered data explains 98.03789% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 8: PCA on centered data explains 97.39640% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 9: PCA on centered data explains 96.59443% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 10: PCA on centered data explains 95.37063% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:169: Layer 10: Direction corrected (flipped)\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 11: PCA on centered data explains 94.12540% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 12: PCA on centered data explains 93.72486% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 13: PCA on centered data explains 92.25963% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 14: PCA on centered data explains 91.83785% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 15: PCA on centered data explains 90.81233% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:169: Layer 15: Direction corrected (flipped)\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 16: PCA on centered data explains 88.13194% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:169: Layer 16: Direction corrected (flipped)\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 17: PCA on centered data explains 87.51497% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:169: Layer 17: Direction corrected (flipped)\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 18: PCA on centered data explains 86.48470% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:169: Layer 18: Direction corrected (flipped)\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 19: PCA on centered data explains 86.21981% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 20: PCA on centered data explains 83.84545% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 21: PCA on centered data explains 81.51233% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 22: PCA on centered data explains 80.39031% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 23: PCA on centered data explains 79.62766% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 24: PCA on centered data explains 78.92753% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 25: PCA on centered data explains 77.38618% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 26: PCA on centered data explains 76.32117% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:114: Layer 27: PCA on centered data explains 76.73237% of the variance\n",
      "[2026-01-11 21:36:31] INFO pca.py:169: Layer 27: Direction corrected (flipped)\n",
      "[2026-01-11 21:36:31] INFO gguf_writer.py:102: gguf: This GGUF file is for Little Endian only\n",
      "[2026-01-11 21:36:31] WARNING gguf_writer.py:274: Duplicated key name 'controlvector.method', overwriting it with new value 'center' of type STRING\n",
      "[2026-01-11 21:36:31] INFO gguf_writer.py:181: Writing the following files:\n",
      "[2026-01-11 21:36:31] INFO gguf_writer.py:186: refuse-pca.gguf: n_tensors = 28, total_size = 172.0K\n"
     ]
    }
   ],
   "source": [
    "from easysteer.steer import extract_pca_control_vector, StatisticalControlVector\n",
    "control_vector = extract_pca_control_vector(\n",
    "    all_hidden_states=processed_hidden_states, \n",
    "    positive_indices=list(range(100)), \n",
    "    negative_indices=list(range(100,200)), \n",
    "    model_type=\"qwen2.5\",\n",
    "    method=\"center\",\n",
    "    token_pos=-1, \n",
    "    normalize=True\n",
    ")\n",
    "control_vector.export_gguf(\"refuse-pca.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d6d262-f782-42fd-9a18-6c88e79029c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
