{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bc485d3-985d-4e6c-978e-0953ede1349f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-16 02:56:51 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-16 02:57:05 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.\n",
      "ERROR 07-16 02:57:05 [config.py:130] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/zju-46/shenyl/hf/model/openai-community/gpt2/'. Use `repo_type` argument if needed., retrying 1 of 2\n",
      "ERROR 07-16 02:57:07 [config.py:128] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/zju-46/shenyl/hf/model/openai-community/gpt2/'. Use `repo_type` argument if needed.\n",
      "INFO 07-16 02:57:07 [config.py:3378] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 07-16 02:57:07 [config.py:1472] Using max model len 1024\n",
      "WARNING 07-16 02:57:08 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-16 02:57:08 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294) with config: model='/data/zju-46/shenyl/hf/model/openai-community/gpt2/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/openai-community/gpt2/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/openai-community/gpt2/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-16 02:57:08 [cuda.py:351] Using Flash Attention backend.\n",
      "INFO 07-16 02:57:09 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 07-16 02:57:09 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/openai-community/gpt2/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3731665e33e42f0bfac53c24bd495aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-16 02:57:10 [default_loader.py:272] Loading weights took 0.24 seconds\n",
      "INFO 07-16 02:57:10 [model_runner.py:1255] Model loading took 0.2378 GiB and 0.279352 seconds\n",
      "INFO 07-16 02:57:11 [worker.py:295] Memory profiling takes 0.68 seconds\n",
      "INFO 07-16 02:57:11 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.44GiB) x gpu_memory_utilization (0.90) = 42.69GiB\n",
      "INFO 07-16 02:57:11 [worker.py:295] model weights take 0.24GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 0.47GiB; the rest of the memory reserved for KV Cache is 41.93GiB.\n",
      "INFO 07-16 02:57:12 [executor_base.py:115] # cuda blocks: 76329, # CPU blocks: 7281\n",
      "INFO 07-16 02:57:12 [executor_base.py:120] Maximum concurrency for 1024 tokens per request: 1192.64x\n",
      "INFO 07-16 02:57:16 [llm_engine.py:430] init engine (profile, create kv cache, warmup model) took 5.29 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest, VectorConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Initialize LLM with steering vector capability\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/openai-community/gpt2/\",\n",
    "    enable_steer_vector=True,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d8824e-d5ba-4432-a158-044420953b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2690550e216840e98a41679c89e83880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022460057fd2441fb24deea022bc9e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Baseline=====\n",
      "\n",
      "\n",
      "The answer is simple: Spend your life.\n",
      "\n",
      "The best way to spend your life is to spend it wisely.\n",
      "\n",
      "The best way to spend your life is to spend it wisely.\n",
      "\n",
      "The best way to spend your life\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a90bc91e306452facd297ca54f82956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25991cd4d385411eac9e15a81eeae51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====lm_steer=====\n",
      "ONSORED\n",
      "\n",
      "The answer is simple: We can't.\n",
      "\n",
      "The answer is that we can't spend our life.\n",
      "\n",
      "The answer is that we can't spend our life.\n",
      "\n",
      "The answer is that we can't spend our life\n"
     ]
    }
   ],
   "source": [
    "# Create a simple example prompt for testing\n",
    "example = \"How to spend our life?\"\n",
    "# Generate baseline response without steering\n",
    "example_answers = llm.generate(\n",
    "    example,\n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=50,\n",
    "        skip_special_tokens=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Display baseline response\n",
    "print(\"=====Baseline=====\")\n",
    "print(example_answers[0].outputs[0].text)\n",
    "\n",
    "sv_request = SteerVectorRequest(\n",
    "    steer_vector_name=\"lm_steer\",\n",
    "    steer_vector_id=1,\n",
    "    steer_vector_local_path=\"gpt2.pt\", # https://huggingface.co/spaces/Glaciohound/LM-Steer/blob/main/checkpoints/gpt2.pt\n",
    "    prefill_trigger_positions=[-1],\n",
    "    algorithm=\"lm_steer\",\n",
    "    scale=1.0, # here positive - Toxification, negative - Detoxification\n",
    "    target_layers=[11] # last layer\n",
    ")\n",
    "# Generate response with lm_steer\n",
    "output = llm.generate(\n",
    "    example, \n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=50,\n",
    "        skip_special_tokens=False,\n",
    "    ), \n",
    "    steer_vector_request=sv_request\n",
    ")\n",
    "\n",
    "# Display lm_steer response\n",
    "print(\"=====lm_steer=====\")\n",
    "print(output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681a18f7-2099-4e46-bd62-9b573fa9cef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
