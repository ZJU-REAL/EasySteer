{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7149c3ac-494b-4014-92e4-c73ba2d76992",
   "metadata": {},
   "source": [
    "# Generate some responses using MATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "998c244b-7069-4b97-8387-11c6e27f2f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-12 02:35:40 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-12 02:35:53 [config.py:841] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 07-12 02:35:53 [config.py:1472] Using max model len 131072\n",
      "WARNING 07-12 02:35:53 [arg_utils.py:1577] The model has a long context length (131072). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value.\n",
      "WARNING 07-12 02:35:53 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-12 02:35:53 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294) with config: model='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-12 02:35:54 [cuda.py:351] Using Flash Attention backend.\n",
      "INFO 07-12 02:35:55 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 07-12 02:35:55 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1910d808d8b446d194c41cfa61a4e30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-12 02:35:57 [default_loader.py:272] Loading weights took 1.10 seconds\n",
      "INFO 07-12 02:35:57 [model_runner.py:1255] Model loading took 3.3461 GiB and 1.264793 seconds\n",
      "INFO 07-12 02:36:01 [worker.py:295] Memory profiling takes 3.94 seconds\n",
      "INFO 07-12 02:36:01 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.44GiB) x gpu_memory_utilization (0.90) = 42.69GiB\n",
      "INFO 07-12 02:36:01 [worker.py:295] model weights take 3.35GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 8.07GiB; the rest of the memory reserved for KV Cache is 31.22GiB.\n",
      "INFO 07-12 02:36:02 [executor_base.py:115] # cuda blocks: 73063, # CPU blocks: 9362\n",
      "INFO 07-12 02:36:02 [executor_base.py:120] Maximum concurrency for 131072 tokens per request: 8.92x\n",
      "INFO 07-12 02:36:05 [llm_engine.py:430] init engine (profile, create kv cache, warmup model) took 7.73 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b7301e524b40fd89fc11c0f2af1154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da7dfd10b1f4896b8ed4ec45040ce7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest\n",
    "import os\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "llm = LLM(model=\"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\", enable_steer_vector=True, enforce_eager=True, tensor_parallel_size=1)\n",
    "\n",
    "problems=[\n",
    "    \"Chandra has four bowls.  Each one is a different color (red, blue, yellow, green).  She also has exactly one glass the same color as each bowl.  If she chooses a bowl and a glass from the cupboard, how many pairings are possible?  One such pairing is a blue bowl and a yellow glass.\",\n",
    "    \"The distance between two cities on a map is 15 inches. If the scale is 0.25 inches = 3 miles, how many miles apart are the actual cities?\",\n",
    "    \"How many prime numbers are between 20 and 30?\"\n",
    "]\n",
    "\n",
    "texts = [\"Please reason step by step, and put your final answer within \\\\boxed{}.\\nUser: \" + problem + \"\\nAssistant: <think>\" for problem in problems]\n",
    "answers = llm.generate(\n",
    "    texts,\n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=4096,\n",
    "        skip_special_tokens=False,\n",
    "    ),\n",
    ")\n",
    "answers = [answer.outputs[0].text for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c364a9-0d14-4fe1-af4b-ffac0192e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "qa_pairs = []\n",
    "for i in range(len(texts)):\n",
    "    qa_pairs.append(texts[i] + answers[i])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\")\n",
    "\n",
    "# \"\\n\\n\"  \"ĊĊ\"\n",
    "target_suffix = \"ĊĊ\"\n",
    "all_tokens_list = []\n",
    "all_newline_positions = []\n",
    "for qa in qa_pairs:\n",
    "    # Tokenize the QA pair\n",
    "    tokens = tokenizer.tokenize(qa, add_special_tokens=True)\n",
    "    all_tokens_list.append(tokens)\n",
    "    # 找到 \"ĊĊ\" 在列表中的所有位置(放宽限制，允许有前缀，例如空格)\n",
    "    # 查找所有匹配项的索引\n",
    "    positions = [i for i, token in enumerate(tokens) if isinstance(token, str) and token.endswith(target_suffix)]\n",
    "    all_newline_positions.append(positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fa65041-c059-4fd6-8405-309d59649711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 所有样本的分类结果摘要 ---\n",
      "\n",
      "--- 分析 QA Pair 1 ---\n",
      "Transition positions: [442, 1139, 1729, 1924, 2114, 2519, 2683, 2925, 3089, 3331, 3495, 3737, 3884, 4031, 4178]\n",
      "Reflection positions: [240, 681, 800, 850, 1061, 1233, 1282, 1339, 1439, 1656, 1681, 1853, 1964, 2020, 2066, 2154, 2211, 2448, 2559, 2616, 2632, 2761, 2818, 2856, 2965, 3022, 3038, 3167, 3224, 3262, 3371, 3428, 3444, 3573, 3630, 3668, 3777, 3815, 3924, 3962, 4071, 4109]\n",
      "Execution positions: [154, 329, 542, 724, 918, 986, 1077, 1106, 1355, 1380, 1401, 1494, 1522, 1548, 1570, 1604, 1642, 1763, 1772, 1807, 1842, 1862, 1891, 2196, 2227, 2252, 2273, 2297, 2322, 2348, 2368, 2402, 2437, 2457, 2486, 2601, 2717, 2726, 2803, 2904, 3007, 3123, 3132, 3209, 3310, 3413, 3529, 3538, 3615, 3716, 3863, 4010, 4157]\n",
      "\n",
      "--- 分析 QA Pair 2 ---\n",
      "Transition positions: [287, 789]\n",
      "Reflection positions: [233, 641, 889]\n",
      "Execution positions: [124, 182, 219, 344, 358, 366, 384, 406, 419, 436, 448, 496, 510, 524, 544, 574, 592, 619, 632, 866, 955, 997, 1020, 1071, 1079, 1120, 1128, 1148, 1162, 1169, 1192, 1212, 1230, 1264, 1273, 1294]\n",
      "\n",
      "--- 分析 QA Pair 3 ---\n",
      "Transition positions: [798]\n",
      "Reflection positions: [526, 712]\n",
      "Execution positions: [107, 189, 211, 267, 304, 356, 399, 435, 463, 501, 680, 873, 900, 925, 960, 1083, 1116]\n",
      "\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "TRANSITION_KEYWORDS = [\n",
    "    'alternatively', 'think differently', 'another way', 'another approach',\n",
    "    'another method', 'another solution', 'another strategy', 'another technique'\n",
    "]\n",
    "\n",
    "REFLECTION_KEYWORDS = [\n",
    "    'wait', 'verify', 'make sure', 'hold on', 'think again', \"'s correct\",\n",
    "    \"'s incorrect\", 'let me check', 'seems right'\n",
    "]\n",
    "\n",
    "# --- 1. 修改分类函数 ---\n",
    "def classify_segment(text_segment):\n",
    "    \"\"\"根据文本片段中的关键词进行分类\"\"\"\n",
    "    lower_text = text_segment.lower()\n",
    "    if any(keyword in lower_text for keyword in TRANSITION_KEYWORDS):\n",
    "        return \"Transition\"\n",
    "    if any(keyword in lower_text for keyword in REFLECTION_KEYWORDS):\n",
    "        return \"Reflection\"\n",
    "    # 将 \"Other\" 修改为 \"Execution\"\n",
    "    return \"Execution\"\n",
    "\n",
    "# --- 2. 核心分类逻辑 (与之前相同) ---\n",
    "\n",
    "all_classifications = []\n",
    "\n",
    "# 遍历每一对 QA\n",
    "# (假设 all_newline_positions 和 all_tokens_list 已经像之前的代码一样被填充)\n",
    "for i, positions in enumerate(all_newline_positions):\n",
    "    tokens = all_tokens_list[i]\n",
    "    classifications_for_qa = []\n",
    "\n",
    "    if not positions:\n",
    "        all_classifications.append(classifications_for_qa)\n",
    "        continue\n",
    "\n",
    "    # 遍历当前 QA 中所有 \"ĊĊ\" 的位置\n",
    "    for j, pos in enumerate(positions):\n",
    "        start_slice = pos + 1\n",
    "        if j + 1 < len(positions):\n",
    "            end_slice = positions[j+1]\n",
    "        else:\n",
    "            end_slice = len(tokens)\n",
    "\n",
    "        token_slice = tokens[start_slice:end_slice]\n",
    "        text_segment = tokenizer.decode(tokenizer.convert_tokens_to_ids(token_slice), skip_special_tokens=True).strip()\n",
    "        category = classify_segment(text_segment)\n",
    "\n",
    "        classifications_for_qa.append({\n",
    "            \"position_in_tokens\": pos,\n",
    "            \"category\": category,\n",
    "        })\n",
    "\n",
    "    all_classifications.append(classifications_for_qa)\n",
    "\n",
    "# --- 3. 打印每个样本的分类位置列表 ---\n",
    "\n",
    "print(\"--- 所有样本的分类结果摘要 ---\")\n",
    "for i, qa_results in enumerate(all_classifications):\n",
    "    print(f\"\\n--- 分析 QA Pair {i+1} ---\")\n",
    "\n",
    "    # 初始化三个类别的空列表\n",
    "    summary = {\n",
    "        \"Transition\": [],\n",
    "        \"Reflection\": [],\n",
    "        \"Execution\": []\n",
    "    }\n",
    "\n",
    "    # 遍历当前样本的分类结果，将位置添加到对应的列表中\n",
    "    for result in qa_results:\n",
    "        category = result[\"category\"]\n",
    "        position = result[\"position_in_tokens\"]\n",
    "        if category in summary:\n",
    "            summary[category].append(position)\n",
    "\n",
    "    # 打印格式化的摘要\n",
    "    print(f\"Transition positions: {summary['Transition']}\")\n",
    "    print(f\"Reflection positions: {summary['Reflection']}\")\n",
    "    print(f\"Execution positions: {summary['Execution']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "447c54c7-5cf4-45f0-8984-89deeb449fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-12 02:37:09 [config.py:1472] Using max model len 16384\n",
      "INFO 07-12 02:37:09 [config.py:4615] Only \"last\" pooling supports chunked prefill and prefix caching; disabling both.\n",
      "INFO 07-12 02:37:09 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294) with config: model='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type=None, normalize=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-12 02:37:10 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70fd14de98b3425f94619025fd57f8c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-12 02:37:11 [default_loader.py:272] Loading weights took 0.94 seconds\n",
      "INFO 07-12 02:37:12 [model_runner.py:1255] Model loading took 2.8793 GiB and 0.991639 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc15697a976488ab6e93a34bad670d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fccc7dfa8a2d42f788b902c90fd7fc6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import easysteer.hidden_states as hs\n",
    "llm = LLM(model=\"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\", task=\"reward\", tensor_parallel_size=1)\n",
    "all_hidden_states, outputs = hs.get_all_hidden_states(llm, qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30b3173d-67e6-4c93-8fc4-11f6ec548955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating average for 'Transition' using 18 vectors.\n",
      "Calculating average for 'Reflection' using 47 vectors.\n",
      "Calculating average for 'Execution' using 106 vectors.\n",
      "Warning: Could not determine model_type from `llm` object. Using a placeholder.\n"
     ]
    }
   ],
   "source": [
    "from easysteer.steer import StatisticalControlVector\n",
    "import numpy as np\n",
    "# --- 1. 按类别收集所有相关的隐藏状态 ---\n",
    "\n",
    "# 初始化一个字典来按类别和层收集所有的 hidden states\n",
    "collected_states = {\n",
    "    \"Transition\": {},\n",
    "    \"Reflection\": {},\n",
    "    \"Execution\": {}\n",
    "}\n",
    "\n",
    "# 获取层数\n",
    "num_layers = len(all_hidden_states[0])\n",
    "\n",
    "# 遍历所有样本的分类结果\n",
    "for sample_idx, qa_results in enumerate(all_classifications):\n",
    "    for result in qa_results:\n",
    "        category = result[\"category\"]\n",
    "        position = result[\"position_in_tokens\"]\n",
    "\n",
    "        # 提取该 token 在所有层上的 hidden states\n",
    "        for layer_idx in range(num_layers):\n",
    "            if layer_idx not in collected_states[category]:\n",
    "                collected_states[category][layer_idx] = []\n",
    "            \n",
    "            # 安全地提取并转换 hidden state\n",
    "            token_hidden = all_hidden_states[sample_idx][layer_idx][position]\n",
    "            \n",
    "            token_hidden = token_hidden.cpu().float().numpy()\n",
    "            \n",
    "            collected_states[category][layer_idx].append(token_hidden)\n",
    "\n",
    "# --- 2. 计算每个类别在每一层的平均隐藏状态 ---\n",
    "\n",
    "average_vectors = {\n",
    "    \"Transition\": {},\n",
    "    \"Reflection\": {},\n",
    "    \"Execution\": {}\n",
    "}\n",
    "# 用于元数据，记录每个类别用了多少个向量来平均\n",
    "vector_counts = {} \n",
    "\n",
    "for category, layer_data in collected_states.items():\n",
    "    if not layer_data:\n",
    "        print(f\"Warning: No vectors found for category '{category}'. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 记录该类别总共有多少个向量\n",
    "    vector_counts[category] = len(layer_data.get(0, []))\n",
    "    print(f\"Calculating average for '{category}' using {vector_counts[category]} vectors.\")\n",
    "\n",
    "    for layer_idx, vectors in layer_data.items():\n",
    "        # 直接使用numpy计算均值\n",
    "        mean_vector = np.mean(np.array(vectors), axis=0)\n",
    "        average_vectors[category][layer_idx] = mean_vector\n",
    "\n",
    "# --- 3. 封装并导出为 GGUF 文件 ---\n",
    "\n",
    "# 假设 llm.config.model_type 可以获取到模型类型，如果不行，请手动替换\n",
    "try:\n",
    "    model_type_str = llm.config.model_type\n",
    "except (AttributeError, NameError):\n",
    "    print(\"Warning: Could not determine model_type from `llm` object. Using a placeholder.\")\n",
    "    model_type_str = \"qwen2\" # 请根据您的模型修改此占位符\n",
    "\n",
    "for category, directions in average_vectors.items():\n",
    "    if not directions:\n",
    "        continue # 如果没有计算出向量（因为没有样本），则跳过\n",
    "\n",
    "    # 准备元数据\n",
    "    metadata = {\n",
    "        \"source\": \"Averaged from classified newline tokens\",\n",
    "        \"num_vectors_averaged\": vector_counts.get(category, 0)\n",
    "    }\n",
    "\n",
    "    # 创建 StatisticalControlVector 实例\n",
    "    control_vector = StatisticalControlVector(\n",
    "        model_type=model_type_str,\n",
    "        method=\"Average\",\n",
    "        directions=directions,\n",
    "        metadata=metadata\n",
    "    )\n",
    "\n",
    "    control_vector.export_gguf(f\"{category.lower()}_avg_vector.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df27ea5-5baa-4716-af08-34c742cf06ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
