{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c6e958-554f-4378-a9ac-c574bf72e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inputs_for_source = [\n",
    "    \"What is the capital of the UK?\", \"Which city is the capital of the United Kingdom?\",\n",
    "    \"Could you tell me the capital city of Great Britain?\", \"Identify the primary city of the United Kingdom.\",\n",
    "    \"Where does the UK Parliament convene?\", \"The UK Prime Minister's residence, 10 Downing Street, is in what city?\",\n",
    "    \"In which city is Buckingham Palace located?\", \"Where is the headquarters of the UK's government?\",\n",
    "    \"The famous landmark Tower Bridge is in which city?\", \"What city is the Shard located in?\",\n",
    "    \"The River Thames famously flows through which major city?\", \"I want to visit the British Museum. What city should I travel to?\",\n",
    "    \"Where can I find the London Eye?\", \"Heathrow (LHR) is the main international airport for which city?\",\n",
    "    \"King's Cross Station is a major railway hub in...\", \"The Tube is the name of the subway system in what city?\",\n",
    "    \"In which city did the 2012 Summer Olympics take place?\", \"Sherlock Holmes lived at 221B Baker Street, located in...\",\n",
    "    \"The historical Globe Theatre, associated with Shakespeare, is in...\", \"What city is known for its iconic red double-decker buses?\",\n",
    "    \"I'm writing a report. The capital of the United Kingdom is\", \"Complete the fact: The largest city in Great Britain is\",\n",
    "    \"If I fly into Gatwick Airport, what major city am I near?\", \"The financial heart of the UK is known as the City of...\",\n",
    "    \"A famous play, The Mousetrap, has been running for decades in\", \"Wembley Stadium, the national stadium of England, is located in\",\n",
    "    \"The Notting Hill Carnival takes place every year in\", \"Hyde Park is a massive green space in the middle of\",\n",
    "    \"Which city's metro system is called the Underground?\", \"The West End theatre district is a famous part of\"\n",
    "]\n",
    "\n",
    "assistant_starts_for_source = [\n",
    "    \"The capital of the UK is\", \"The capital city of the United Kingdom is\",\n",
    "    \"The capital city of Great Britain is\", \"The primary city of the United Kingdom is\",\n",
    "    \"The UK Parliament convenes in\", \"10 Downing Street is in the city of\",\n",
    "    \"Buckingham Palace is located in\", \"The headquarters of the UK's government is in\",\n",
    "    \"Tower Bridge is in the city of\", \"The Shard is located in\",\n",
    "    \"The River Thames flows through\", \"You should travel to the city of\",\n",
    "    \"You can find the London Eye in\", \"Heathrow is the main airport for\",\n",
    "    \"King's Cross Station is a major railway hub in\", \"The Tube is the subway system in\",\n",
    "    \"The 2012 Summer Olympics took place in\", \"Sherlock Holmes lived in\",\n",
    "    \"The Globe Theatre is in\", \"The city known for its red double-decker buses is\",\n",
    "    \"The capital of the United Kingdom is\", \"The largest city in Great Britain is\",\n",
    "    \"You would be near the city of\", \"The financial heart of the UK is the City of\",\n",
    "    \"The Mousetrap has been running in\", \"Wembley Stadium is located in\",\n",
    "    \"The Notting Hill Carnival takes place in\", \"Hyde Park is in\",\n",
    "    \"That would be the metro system of\", \"The West End theatre district is in\"\n",
    "]\n",
    "\n",
    "# Knowledge editing target (old information → new information)\n",
    "old_object = \"London\"\n",
    "new_object = \"Paris\"\n",
    "\n",
    "# Use assistant starting phrases as source prompts\n",
    "source_prompts_for_base = assistant_starts_for_source\n",
    "\n",
    "# Create target prompts by instructing the model to mention Paris instead of London\n",
    "target_prompts_for_base = []\n",
    "for p in source_prompts_for_base:\n",
    "    # Format: Instruct model to avoid the old object and repeat the sentence with the new object\n",
    "    prompt = f\"Do not mention {old_object}. Repeat this sentence: {p.strip()} {new_object}. {p.strip()}\"\n",
    "    target_prompts_for_base.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba602c63-12cd-445a-8417-565258f6e1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-03 14:57:27 [utils.py:253] non-default args: {'task': 'embed', 'enable_prefix_caching': False, 'disable_log_stats': True, 'enforce_eager': True, 'enable_chunked_prefill': False, 'model': '/data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/'}\n",
      "INFO 11-03 14:57:27 [model.py:657] Resolved architecture: LlamaForCausalLM\n",
      "INFO 11-03 14:57:27 [model.py:1746] Using max model len 4096\n",
      "INFO 11-03 14:57:31 [vllm.py:414] Cudagraph is disabled under eager mode\n",
      "\u001b[1;36m(EngineCore_DP0 pid=749428)\u001b[0;0m INFO 11-03 14:57:32 [core.py:94] Initializing a V1 LLM engine (v0.1.dev10888+g9d4fd0da4.d20251031) with config: model='/data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=PoolerConfig(pooling_type='LAST', normalize=None, dimensions=None, enable_chunked_processing=None, max_embed_len=None, softmax=None, activation=None, use_activation=None, logit_bias=None, step_tag_id=None, returned_token_ids=None), compilation_config={'level': None, 'mode': 0, 'debug_dump_path': None, 'cache_dir': '', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'use_cudagraph': False, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'full_cuda_graph': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=749428)\u001b[0;0m INFO 11-03 14:57:37 [parallel_state.py:1325] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=749428)\u001b[0;0m INFO 11-03 14:57:37 [gpu_model_runner.py:2902] Starting to load model /data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=749428)\u001b[0;0m INFO 11-03 14:57:38 [cuda.py:420] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea569b4df394abbad7e1b28f462b463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=749428)\u001b[0;0m INFO 11-03 14:57:53 [default_loader.py:314] Loading weights took 15.05 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=749428)\u001b[0;0m INFO 11-03 14:57:53 [hidden_states_model_runner_mixin.py:90] Wrapped 32 decoder layers for hidden states capture\n",
      "\u001b[1;36m(EngineCore_DP0 pid=749428)\u001b[0;0m INFO 11-03 14:57:54 [gpu_model_runner.py:2971] Model loading took 12.3082 GiB and 15.270460 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=749428)\u001b[0;0m INFO 11-03 14:57:57 [gpu_worker.py:343] Available KV cache memory: 29.97 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=749428)\u001b[0;0m INFO 11-03 14:57:57 [kv_cache_utils.py:1247] GPU KV cache size: 61,360 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=749428)\u001b[0;0m INFO 11-03 14:57:57 [kv_cache_utils.py:1252] Maximum concurrency for 4,096 tokens per request: 14.98x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=749428)\u001b[0;0m INFO 11-03 14:57:57 [core.py:238] init engine (profile, create kv cache, warmup model) took 3.30 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=749428)\u001b[0;0m INFO 11-03 14:57:58 [vllm.py:414] Cudagraph is disabled under eager mode\n",
      "INFO 11-03 14:57:58 [llm.py:346] Supported tasks: ['token_embed', 'embed']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d1e6ed2af84cb5bcb51398b53d8fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025bb46b798a430dbe7dcbf5703adf14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/60 [00:00<?, ?it/s, est. speed input: 0.00 toks/s,"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# According to the SAKE paper, we only need to extract the hidden states \n",
    "# from the last layer at the final position\n",
    "import easysteer.hidden_states as hs\n",
    "from vllm import LLM\n",
    "import os\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Create a new LLM instance in reward mode to extract hidden states\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/\",\n",
    "    task=\"embed\", \n",
    "    tensor_parallel_size=1,\n",
    "    enforce_eager=True,\n",
    "    enable_prefix_caching=False,\n",
    "    enable_chunked_prefill=False\n",
    ")\n",
    "\n",
    "# Extract hidden states for both source and target prompts\n",
    "# We'll combine both lists into a single batch for extraction\n",
    "all_hidden_states, outputs = hs.get_all_hidden_states(llm, source_prompts_for_base+target_prompts_for_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f00e2477-a6c4-46e3-8c39-ff3a451f33a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting POT\n",
      "  Downloading pot-0.9.6.post1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: numpy>=1.16 in /data/zju-48b/xhl/anaconda3/envs/v1-latest/lib/python3.10/site-packages (from POT) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.6 in /data/zju-48b/xhl/anaconda3/envs/v1-latest/lib/python3.10/site-packages (from POT) (1.15.3)\n",
      "Downloading pot-0.9.6.post1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: POT\n",
      "Successfully installed POT-0.9.6.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install POT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5a95e7-8b52-44ff-a298-51d0e0bc73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ot  # Optimal Transport library\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Extract the last token's hidden state from the last layer for each prompt\n",
    "# First half are source prompts (London)\n",
    "source_hidden_states = [all_hidden_states[x][-1][-1] for x in range(len(source_prompts_for_base))]\n",
    "# Second half are target prompts (Paris)\n",
    "target_hidden_states = [all_hidden_states[x][-1][-1] for x in range(len(source_prompts_for_base), 2*len(source_prompts_for_base))]\n",
    "\n",
    "# Convert PyTorch tensors to NumPy arrays for optimal transport\n",
    "Xs = np.array([t.to(torch.float32).numpy() for t in source_hidden_states])\n",
    "Xt = np.array([t.to(torch.float32).numpy() for t in target_hidden_states])\n",
    "\n",
    "# Create a linear transport model with regularization parameter 0.6\n",
    "# This learns a linear transformation from source to target distributions\n",
    "linear_mapper = ot.da.LinearTransport(reg=0.6)\n",
    "linear_mapper.fit(Xs=Xs, Xt=Xt)\n",
    "\n",
    "# Test the transformation on the first example\n",
    "original_vector = Xs[0:1, :] \n",
    "transformed_vector = linear_mapper.transform(Xs=original_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9da8887-1ba2-4bd9-9600-0c54535f6e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear mapper successfully saved to: edit_uk_capital_to_paris.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Define the path to save the linear mapper\n",
    "mapper_save_path = \"edit_uk_capital_to_paris.pkl\"\n",
    "\n",
    "# Save the linear transformation model to a pickle file\n",
    "# This will be loaded during inference to apply the transformation\n",
    "with open(mapper_save_path, 'wb') as f:\n",
    "    pickle.dump(linear_mapper, f)\n",
    "\n",
    "print(f\"Linear mapper successfully saved to: {mapper_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec675f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAKE: Structure-Aware Knowledge Editing for Large Language Models\n",
    "#\n",
    "# This notebook implements the data preparation for SAKE knowledge editing:\n",
    "#\n",
    "# 1. We prepare source prompts about the capital of the UK (London)\n",
    "# 2. We create target prompts that should refer to Paris instead\n",
    "# 3. We extract hidden states from both sets of prompts\n",
    "# 4. We learn a linear transformation using optimal transport to map \n",
    "#    \"London\" knowledge to \"Paris\" knowledge\n",
    "# 5. We save this transformation for use during inference\n",
    "#\n",
    "# This demonstrates how to create knowledge editing vectors that can\n",
    "# modify a model's factual knowledge without any fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b463bc-5aca-4175-b605-71808e7237fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
