{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "998c244b-7069-4b97-8387-11c6e27f2f48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-03 14:45:58 [utils.py:253] non-default args: {'disable_log_stats': True, 'enforce_eager': True, 'enable_steer_vector': True, 'enable_chunked_prefill': False, 'model': '/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/'}\n",
      "INFO 11-03 14:45:58 [model.py:657] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 11-03 14:45:58 [model.py:1746] Using max model len 131072\n",
      "INFO 11-03 14:46:00 [scheduler.py:211] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 11-03 14:46:00 [vllm.py:414] Cudagraph is disabled under eager mode\n",
      "\u001b[1;36m(EngineCore_DP0 pid=745684)\u001b[0;0m INFO 11-03 14:46:01 [core.py:94] Initializing a V1 LLM engine (v0.1.dev10888+g9d4fd0da4.d20251031) with config: model='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={'level': None, 'mode': 0, 'debug_dump_path': None, 'cache_dir': '', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'use_cudagraph': False, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'full_cuda_graph': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=745684)\u001b[0;0m INFO 11-03 14:46:03 [parallel_state.py:1325] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=745684)\u001b[0;0m INFO 11-03 14:46:04 [gpu_model_runner.py:2902] Starting to load model /data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=745684)\u001b[0;0m INFO 11-03 14:46:04 [cuda.py:420] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ee2a80f6214d108664402669baae83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=745684)\u001b[0;0m INFO 11-03 14:46:06 [default_loader.py:314] Loading weights took 1.25 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=745684)\u001b[0;0m INFO 11-03 14:46:06 [steer_vector_model_runner_mixin.py:36] Initialized SteerVector worker manager\n",
      "\u001b[1;36m(EngineCore_DP0 pid=745684)\u001b[0;0m INFO 11-03 14:46:06 [steer_vector_model_runner_mixin.py:50] Wrapping model with steer vector support\n",
      "\u001b[1;36m(EngineCore_DP0 pid=745684)\u001b[0;0m INFO 11-03 14:46:06 [hidden_states_model_runner_mixin.py:90] Wrapped 28 decoder layers for hidden states capture\n",
      "\u001b[1;36m(EngineCore_DP0 pid=745684)\u001b[0;0m INFO 11-03 14:46:06 [gpu_model_runner.py:2971] Model loading took 3.3466 GiB and 1.423618 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=745684)\u001b[0;0m INFO 11-03 14:46:08 [gpu_worker.py:343] Available KV cache memory: 37.87 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=745684)\u001b[0;0m INFO 11-03 14:46:08 [kv_cache_utils.py:1247] GPU KV cache size: 1,418,176 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=745684)\u001b[0;0m INFO 11-03 14:46:08 [kv_cache_utils.py:1252] Maximum concurrency for 131,072 tokens per request: 10.82x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=745684)\u001b[0;0m INFO 11-03 14:46:08 [core.py:238] init engine (profile, create kv cache, warmup model) took 2.11 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=745684)\u001b[0;0m INFO 11-03 14:46:09 [vllm.py:414] Cudagraph is disabled under eager mode\n",
      "INFO 11-03 14:46:10 [llm.py:346] Supported tasks: ['generate']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf499c18dde4d19adedd2b44b1fa8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3e1436f4ee49e19e732358360b9dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Initialize LLM\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\",\n",
    "    enable_steer_vector=True,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1,\n",
    "    enable_chunked_prefill=False\n",
    ")\n",
    "\n",
    "# Define math problems for testing\n",
    "problems = [\n",
    "    \"Chandra has four bowls.  Each one is a different color (red, blue, yellow, green).  She also has exactly one glass the same color as each bowl.  If she chooses a bowl and a glass from the cupboard, how many pairings are possible?  One such pairing is a blue bowl and a yellow glass.\",\n",
    "    \"The distance between two cities on a map is 15 inches. If the scale is 0.25 inches = 3 miles, how many miles apart are the actual cities?\",\n",
    "    \"How many prime numbers are between 20 and 30?\"\n",
    "]\n",
    "\n",
    "# Create prompt texts from problems\n",
    "texts = [\"Please reason step by step, and put your final answer within \\\\boxed{}.\\nUser: \" + problem + \"\\nAssistant: <think>\" for problem in problems]\n",
    "\n",
    "# Generate answers using the LLM\n",
    "answers = llm.generate(\n",
    "    texts,\n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=4096,\n",
    "        skip_special_tokens=False,\n",
    "    ),\n",
    ")\n",
    "answers = [answer.outputs[0].text for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c364a9-0d14-4fe1-af4b-ffac0192e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Create QA pairs by combining prompts and answers\n",
    "qa_pairs = [texts[i] + answers[i] for i in range(len(texts))]\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\")\n",
    "\n",
    "# The newline token suffix in tokenizer vocabulary\n",
    "target_suffix = \"ĊĊ\"  # \"\\n\\n\" is tokenized as \"ĊĊ\"\n",
    "\n",
    "# Process each QA pair to find newline positions\n",
    "all_tokens_list = []\n",
    "all_newline_positions = []\n",
    "for qa in qa_pairs:\n",
    "    # Tokenize the QA pair\n",
    "    tokens = tokenizer.tokenize(qa, add_special_tokens=True)\n",
    "    all_tokens_list.append(tokens)\n",
    "    \n",
    "    # Find all positions of \"ĊĊ\" in the tokens\n",
    "    # These represent potential paragraph breaks in the text\n",
    "    positions = [\n",
    "        i for i, token in enumerate(tokens) \n",
    "        if isinstance(token, str) and token.endswith(target_suffix)\n",
    "    ]\n",
    "    all_newline_positions.append(positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fa65041-c059-4fd6-8405-309d59649711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Summary of classification results for all samples ---\n",
      "\n",
      "--- Analysis of QA Pair 1 ---\n",
      "Transition positions: [1023, 1218, 1291, 1359, 1545, 1655, 1813, 1886, 1954, 2137, 2315, 2529, 2678, 2822, 2888, 3071, 3220, 3364, 3430, 3613, 3762, 3906, 3972, 4155]\n",
      "Reflection positions: [240, 371, 427, 490, 644, 795, 896, 953, 1096, 1440, 1618, 1728, 2032, 2210, 2388, 2459, 2602, 2751, 2966, 3144, 3293, 3508, 3686, 3835, 4050]\n",
      "Execution positions: [154, 299, 563, 749, 1176, 1428, 2020, 2449, 2812, 2954, 3354, 3496, 3896, 4038]\n",
      "\n",
      "--- Analysis of QA Pair 2 ---\n",
      "Transition positions: [287, 793, 890]\n",
      "Reflection positions: [233, 674, 731]\n",
      "Execution positions: [124, 182, 219, 344, 358, 366, 384, 406, 419, 436, 448, 496, 510, 524, 533, 559, 570, 587, 600, 625, 636, 653, 665, 851, 881, 966, 998, 1021, 1082, 1090, 1131, 1139, 1159, 1173, 1180, 1203, 1212, 1251, 1271, 1288, 1305]\n",
      "\n",
      "--- Analysis of QA Pair 3 ---\n",
      "Transition positions: []\n",
      "Reflection positions: [320, 569]\n",
      "Execution positions: [150, 512, 670, 695, 757, 888, 911]\n",
      "\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Define keyword sets for classifying reasoning segments\n",
    "TRANSITION_KEYWORDS = [\n",
    "    'alternatively', 'think differently', 'another way', 'another approach',\n",
    "    'another method', 'another solution', 'another strategy', 'another technique'\n",
    "]\n",
    "\n",
    "REFLECTION_KEYWORDS = [\n",
    "    'wait', 'verify', 'make sure', 'hold on', 'think again', \"'s correct\",\n",
    "    \"'s incorrect\", 'let me check', 'seems right'\n",
    "]\n",
    "\n",
    "def classify_segment(text_segment):\n",
    "    \"\"\"\n",
    "    Classify text segments based on keyword matches.\n",
    "    \n",
    "    Args:\n",
    "        text_segment: String of text to classify\n",
    "        \n",
    "    Returns:\n",
    "        String category: \"Transition\", \"Reflection\", or \"Execution\"\n",
    "    \"\"\"\n",
    "    lower_text = text_segment.lower()\n",
    "    \n",
    "    if any(keyword in lower_text for keyword in TRANSITION_KEYWORDS):\n",
    "        return \"Transition\"\n",
    "    \n",
    "    if any(keyword in lower_text for keyword in REFLECTION_KEYWORDS):\n",
    "        return \"Reflection\"\n",
    "    \n",
    "    # Default category is \"Execution\" (not \"Other\")\n",
    "    return \"Execution\"\n",
    "\n",
    "# Perform classification on all QA pairs\n",
    "all_classifications = []\n",
    "\n",
    "for i, positions in enumerate(all_newline_positions):\n",
    "    tokens = all_tokens_list[i]\n",
    "    classifications_for_qa = []\n",
    "\n",
    "    # Skip if no paragraph breaks were found\n",
    "    if not positions:\n",
    "        all_classifications.append(classifications_for_qa)\n",
    "        continue\n",
    "\n",
    "    # Classify each paragraph segment\n",
    "    for j, pos in enumerate(positions):\n",
    "        # Define segment boundaries\n",
    "        start_slice = pos + 1\n",
    "        end_slice = positions[j+1] if j + 1 < len(positions) else len(tokens)\n",
    "\n",
    "        # Extract and decode the text segment\n",
    "        token_slice = tokens[start_slice:end_slice]\n",
    "        text_segment = tokenizer.decode(\n",
    "            tokenizer.convert_tokens_to_ids(token_slice), \n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "        \n",
    "        # Classify the segment\n",
    "        category = classify_segment(text_segment)\n",
    "\n",
    "        # Store the classification result\n",
    "        classifications_for_qa.append({\n",
    "            \"position_in_tokens\": pos,\n",
    "            \"category\": category,\n",
    "        })\n",
    "\n",
    "    all_classifications.append(classifications_for_qa)\n",
    "\n",
    "# Print summary of classification results\n",
    "print(\"--- Summary of classification results for all samples ---\")\n",
    "\n",
    "for i, qa_results in enumerate(all_classifications):\n",
    "    print(f\"\\n--- Analysis of QA Pair {i+1} ---\")\n",
    "\n",
    "    # Group token positions by category\n",
    "    summary = {\n",
    "        \"Transition\": [],\n",
    "        \"Reflection\": [],\n",
    "        \"Execution\": []\n",
    "    }\n",
    "\n",
    "    # Collect positions for each category\n",
    "    for result in qa_results:\n",
    "        category = result[\"category\"]\n",
    "        position = result[\"position_in_tokens\"]\n",
    "        if category in summary:\n",
    "            summary[category].append(position)\n",
    "\n",
    "    # Print formatted summary by category\n",
    "    print(f\"Transition positions: {summary['Transition']}\")\n",
    "    print(f\"Reflection positions: {summary['Reflection']}\")\n",
    "    print(f\"Execution positions: {summary['Execution']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd104bed-8a4b-4d81-afa7-b3888dab537f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del llm\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "447c54c7-5cf4-45f0-8984-89deeb449fa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-03 14:48:00 [utils.py:253] non-default args: {'task': 'embed', 'enable_prefix_caching': False, 'disable_log_stats': True, 'enforce_eager': True, 'enable_chunked_prefill': False, 'model': '/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/'}\n",
      "INFO 11-03 14:48:00 [model.py:657] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 11-03 14:48:00 [model.py:1746] Using max model len 131072\n",
      "INFO 11-03 14:48:00 [vllm.py:414] Cudagraph is disabled under eager mode\n",
      "\u001b[1;36m(EngineCore_DP0 pid=746251)\u001b[0;0m INFO 11-03 14:48:00 [core.py:94] Initializing a V1 LLM engine (v0.1.dev10888+g9d4fd0da4.d20251031) with config: model='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=PoolerConfig(pooling_type='LAST', normalize=None, dimensions=None, enable_chunked_processing=None, max_embed_len=None, softmax=None, activation=None, use_activation=None, logit_bias=None, step_tag_id=None, returned_token_ids=None), compilation_config={'level': None, 'mode': 0, 'debug_dump_path': None, 'cache_dir': '', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'use_cudagraph': False, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'full_cuda_graph': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=746251)\u001b[0;0m INFO 11-03 14:48:02 [parallel_state.py:1325] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=746251)\u001b[0;0m INFO 11-03 14:48:03 [gpu_model_runner.py:2902] Starting to load model /data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=746251)\u001b[0;0m INFO 11-03 14:48:03 [cuda.py:420] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88319a090e84921acf026311b34dcae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=746251)\u001b[0;0m INFO 11-03 14:48:04 [default_loader.py:314] Loading weights took 0.97 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=746251)\u001b[0;0m INFO 11-03 14:48:04 [hidden_states_model_runner_mixin.py:90] Wrapped 28 decoder layers for hidden states capture\n",
      "\u001b[1;36m(EngineCore_DP0 pid=746251)\u001b[0;0m INFO 11-03 14:48:05 [gpu_model_runner.py:2971] Model loading took 2.9110 GiB and 1.150039 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=746251)\u001b[0;0m INFO 11-03 14:48:10 [gpu_worker.py:343] Available KV cache memory: 31.64 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=746251)\u001b[0;0m INFO 11-03 14:48:10 [kv_cache_utils.py:1247] GPU KV cache size: 1,185,072 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=746251)\u001b[0;0m INFO 11-03 14:48:10 [kv_cache_utils.py:1252] Maximum concurrency for 131,072 tokens per request: 9.04x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=746251)\u001b[0;0m INFO 11-03 14:48:11 [core.py:238] init engine (profile, create kv cache, warmup model) took 5.32 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=746251)\u001b[0;0m INFO 11-03 14:48:11 [vllm.py:414] Cudagraph is disabled under eager mode\n",
      "INFO 11-03 14:48:12 [llm.py:346] Supported tasks: ['token_embed', 'embed']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c7b378137a4d8bba3105402177586f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff79ac259b44f36906296bc7791228a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import hidden states module to extract model activations\n",
    "import easysteer.hidden_states as hs\n",
    "\n",
    "# Create a new LLM instance in reward mode\n",
    "# Note: This allows us to extract hidden states rather than generating text\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\",\n",
    "    task=\"embed\", \n",
    "    tensor_parallel_size=1,\n",
    "    enforce_eager=True,\n",
    "    enable_prefix_caching=False,\n",
    "    enable_chunked_prefill=False\n",
    ")\n",
    "\n",
    "# Extract hidden states for all tokens in the QA pairs\n",
    "all_hidden_states, outputs = hs.get_all_hidden_states(llm, qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30b3173d-67e6-4c93-8fc4-11f6ec548955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-03 14:49:03] INFO gguf_writer.py:102: gguf: This GGUF file is for Little Endian only\n",
      "[2025-11-03 14:49:03] INFO gguf_writer.py:181: Writing the following files:\n",
      "[2025-11-03 14:49:03] INFO gguf_writer.py:186: transition_avg_vector.gguf: n_tensors = 28, total_size = 172.0K\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating average for 'Transition' using 27 vectors.\n",
      "Calculating average for 'Reflection' using 30 vectors.\n",
      "Calculating average for 'Execution' using 62 vectors.\n",
      "Warning: Could not determine model_type from `llm` object. Using a placeholder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-03 14:49:04] INFO gguf_writer.py:102: gguf: This GGUF file is for Little Endian only\n",
      "[2025-11-03 14:49:04] INFO gguf_writer.py:181: Writing the following files:\n",
      "[2025-11-03 14:49:04] INFO gguf_writer.py:186: reflection_avg_vector.gguf: n_tensors = 28, total_size = 172.0K\n",
      "[2025-11-03 14:49:05] INFO gguf_writer.py:102: gguf: This GGUF file is for Little Endian only\n",
      "[2025-11-03 14:49:05] INFO gguf_writer.py:181: Writing the following files:\n",
      "[2025-11-03 14:49:05] INFO gguf_writer.py:186: execution_avg_vector.gguf: n_tensors = 28, total_size = 172.0K\n"
     ]
    }
   ],
   "source": [
    "from easysteer.steer import StatisticalControlVector\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Collect all relevant hidden states by category\n",
    "#-------------------------------------------------\n",
    "\n",
    "# Initialize a dictionary to collect all hidden states by category and layer\n",
    "collected_states = {\n",
    "    \"Transition\": {},\n",
    "    \"Reflection\": {},\n",
    "    \"Execution\": {}\n",
    "}\n",
    "\n",
    "# Get the number of layers in the model\n",
    "num_layers = len(all_hidden_states[0])\n",
    "\n",
    "# Process each sample's classification results to collect hidden states\n",
    "for sample_idx, qa_results in enumerate(all_classifications):\n",
    "    for result in qa_results:\n",
    "        category = result[\"category\"]\n",
    "        position = result[\"position_in_tokens\"]\n",
    "\n",
    "        # For each layer, collect hidden states for tokens of this category\n",
    "        for layer_idx in range(num_layers):\n",
    "            # Initialize empty list for this layer if not already present\n",
    "            if layer_idx not in collected_states[category]:\n",
    "                collected_states[category][layer_idx] = []\n",
    "            \n",
    "            # Extract hidden state from the model output\n",
    "            token_hidden = all_hidden_states[sample_idx][layer_idx][position]\n",
    "            \n",
    "            # Convert to numpy for easier processing\n",
    "            token_hidden = token_hidden.cpu().float().numpy()\n",
    "            \n",
    "            # Store the hidden state\n",
    "            collected_states[category][layer_idx].append(token_hidden)\n",
    "\n",
    "\n",
    "# Step 2: Calculate the average hidden state for each category at each layer\n",
    "#-------------------------------------------------\n",
    "\n",
    "# Initialize result dictionaries\n",
    "average_vectors = {\n",
    "    \"Transition\": {},\n",
    "    \"Reflection\": {},\n",
    "    \"Execution\": {}\n",
    "}\n",
    "\n",
    "# Track vector counts for metadata\n",
    "vector_counts = {} \n",
    "\n",
    "# Process each category\n",
    "for category, layer_data in collected_states.items():\n",
    "    # Skip empty categories\n",
    "    if not layer_data:\n",
    "        print(f\"Warning: No vectors found for category '{category}'. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Record how many vectors we're averaging for this category\n",
    "    vector_counts[category] = len(layer_data.get(0, []))\n",
    "    print(f\"Calculating average for '{category}' using {vector_counts[category]} vectors.\")\n",
    "\n",
    "    # Calculate average for each layer\n",
    "    for layer_idx, vectors in layer_data.items():\n",
    "        # Calculate mean across all vectors for this layer\n",
    "        mean_vector = np.mean(np.array(vectors), axis=0)\n",
    "        average_vectors[category][layer_idx] = mean_vector\n",
    "\n",
    "\n",
    "# Step 3: Package and export as GGUF files\n",
    "#-------------------------------------------------\n",
    "\n",
    "# Try to get model type or use a placeholder\n",
    "try:\n",
    "    model_type_str = llm.config.model_type\n",
    "except (AttributeError, NameError):\n",
    "    print(\"Warning: Could not determine model_type from `llm` object. Using a placeholder.\")\n",
    "    model_type_str = \"qwen2\"  # Default placeholder - adjust as needed for your model\n",
    "\n",
    "# Create and export control vectors for each category\n",
    "for category, directions in average_vectors.items():\n",
    "    if not directions:\n",
    "        continue  # Skip if no data available\n",
    "    \n",
    "    # Prepare metadata for the vector\n",
    "    metadata = {\n",
    "        \"source\": \"Averaged from classified newline tokens\",\n",
    "        \"num_vectors_averaged\": vector_counts.get(category, 0)\n",
    "    }\n",
    "\n",
    "    # Create the control vector object\n",
    "    control_vector = StatisticalControlVector(\n",
    "        model_type=model_type_str,\n",
    "        method=\"Average\",\n",
    "        directions=directions,\n",
    "        metadata=metadata\n",
    "    )\n",
    "\n",
    "    # Export to GGUF format\n",
    "    control_vector.export_gguf(f\"{category.lower()}_avg_vector.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df27ea5-5baa-4716-af08-34c742cf06ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
