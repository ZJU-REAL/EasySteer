{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b4b88e-c57e-4ff4-8bcb-e8b150d02f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "llm = LLM(\n",
    "    model=\"/home/xhl/huggingface_models/IAAR-Shanghai/xFinder-qwen1505\",\n",
    "    tensor_parallel_size=1,\n",
    "    trust_remote_code=False,\n",
    "    gpu_memory_utilization=0.95,\n",
    "    max_model_len=8000,\n",
    "    enforce_eager=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "774092a4-ae1f-4241-950a-99627bc30b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 读取 JSON 文件\n",
    "with open(\"mc_task.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "opts = []\n",
    "questions = []\n",
    "for item in data:\n",
    "    options = list(item[\"mc1_targets\"].keys())\n",
    "    option_pairs = [[chr(65+i), opt] for i, opt in enumerate(options)]\n",
    "    standard_answer_range = f\"{option_pairs}\"\n",
    "    opts.append(standard_answer_range) \n",
    "    \n",
    "    options_text = \"\\n\".join([f\"{chr(65+i)}: {opt}\" for i, opt in enumerate(options)])\n",
    "    q = f\"Question: {item['question']}\\n{options_text}\\nWhich one is correct?\"\n",
    "    questions.append(q)\n",
    "\n",
    "PROMPT_TEMPLATE = {\n",
    "    \"xFinder-qwen1505\":\n",
    "        \"\"\"<|System|>:{system}\n",
    "<|User|>:{input}\n",
    "<|Bot|>:\"\"\",\n",
    "    \"xFinder-llama38it\":\n",
    "        \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{system}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\",\n",
    "}\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=8000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b089df04-14a4-4c35-831d-4ba1d1d72306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6add56289134a1c8c1a14085141af36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8511494198f41c5a3beb3e380698966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6009791921664627\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3da28364314fea9909631278f90368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a295bce51741268a933f857bab3e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5924112607099143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd795fc277d4b86a5a641e4d5247c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e97c913b52644e6bb37866cc2c79bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5605875152998776\n"
     ]
    }
   ],
   "source": [
    "for method in ['caa', 'pca', 'probe']:\n",
    "    with open(f\"answers_steer_{method}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        loaded_answers = json.load(f)\n",
    "    \n",
    "    texts = []\n",
    "    for i in range(len(questions)):\n",
    "        formatted_query = f'Question: \"\"\"{questions[i]}\"\"\"\\n\\nOutput sentences: \"\"\"{loaded_answers[i]}\"\"\"\\n\\nAnswer range: {opts[i]}\\n\\nKey extracted answer: '\n",
    "        system_prompt = \"You are a help assistant tasked with extracting the precise key answer from given output sentences.\"\n",
    "        text = PROMPT_TEMPLATE[\"xFinder-qwen1505\"].format(system=system_prompt, input=formatted_query)\n",
    "        texts.append(text)\n",
    "    outputs = llm.generate(texts, sampling_params)\n",
    "    extracted_answer = [o.outputs[0].text for o in outputs]\n",
    "    cnt = 0\n",
    "    for a in extracted_answer:\n",
    "        if a == \"A\":\n",
    "            cnt += 1\n",
    "    print(cnt/len(extracted_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "689d3dff-e666-47aa-95aa-a442f2379f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ca86c01214472bb6dfa203681f510c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a62b905407c41a9bb6cae6a09a4d7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5507955936352509\n"
     ]
    }
   ],
   "source": [
    "with open(f\"answers.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    loaded_answers = json.load(f)\n",
    "    \n",
    "    texts = []\n",
    "    for i in range(len(questions)):\n",
    "        formatted_query = f'Question: \"\"\"{questions[i]}\"\"\"\\n\\nOutput sentences: \"\"\"{loaded_answers[i]}\"\"\"\\n\\nAnswer range: {opts[i]}\\n\\nKey extracted answer: '\n",
    "        system_prompt = \"You are a help assistant tasked with extracting the precise key answer from given output sentences.\"\n",
    "        text = PROMPT_TEMPLATE[\"xFinder-qwen1505\"].format(system=system_prompt, input=formatted_query)\n",
    "        texts.append(text)\n",
    "    outputs = llm.generate(texts, sampling_params)\n",
    "    extracted_answer = [o.outputs[0].text for o in outputs]\n",
    "    cnt = 0\n",
    "    for a in extracted_answer:\n",
    "        if a == \"A\":\n",
    "            cnt += 1\n",
    "    print(cnt/len(extracted_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04cb677-67ae-4edc-b3cd-c1a529663d53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
