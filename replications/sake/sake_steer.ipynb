{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbb57643-afbc-4061-9d22-ee728c58290b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 23:15:49 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-14 23:16:02 [config.py:841] This model supports multiple tasks: {'reward', 'classify', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 07-14 23:16:02 [config.py:1472] Using max model len 4096\n",
      "WARNING 07-14 23:16:03 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-14 23:16:03 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294) with config: model='/data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-14 23:16:03 [cuda.py:351] Using Flash Attention backend.\n",
      "INFO 07-14 23:16:04 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 07-14 23:16:04 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d07ddfc19e475da979d27b0b2f2fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 23:16:09 [default_loader.py:272] Loading weights took 4.14 seconds\n",
      "INFO 07-14 23:16:09 [model_runner.py:1255] Model loading took 12.5524 GiB and 4.336374 seconds\n",
      "INFO 07-14 23:16:11 [worker.py:295] Memory profiling takes 1.11 seconds\n",
      "INFO 07-14 23:16:11 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.44GiB) x gpu_memory_utilization (0.90) = 42.69GiB\n",
      "INFO 07-14 23:16:11 [worker.py:295] model weights take 12.55GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 0.35GiB; the rest of the memory reserved for KV Cache is 29.73GiB.\n",
      "INFO 07-14 23:16:11 [executor_base.py:115] # cuda blocks: 3805, # CPU blocks: 512\n",
      "INFO 07-14 23:16:11 [executor_base.py:120] Maximum concurrency for 4096 tokens per request: 14.86x\n",
      "INFO 07-14 23:16:13 [llm_engine.py:430] init engine (profile, create kv cache, warmup model) took 3.58 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest, VectorConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Initialize LLM with steering vector capability\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/\",\n",
    "    enable_steer_vector=True,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f306abb-61ab-498b-9ea6-9b70d0a8ce76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2360791a6547478ca4f08299d5f915f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07416d372a742e3bb5a7a6ca4a3bc63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Baseline=====\n",
      " London. The capital of the UK is also the capital of England.\n",
      "What is the capital of the UK? The capital of the UK is London. The capital of the UK is also the capital of England.\n",
      "What is the capital of the UK? The capital of the UK is London. The capital of the UK is also the capital of England. The capital of the UK is also the capital of Scotland. The capital of the UK is also the capital of Wales. The capital of the UK is also the capital of Northern Ireland.\n",
      "The capital of the UK is London. The capital of the UK is also the capital of\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb44c28231bc49c7b7df14d06e3446df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e334deb2234afbb7e6397aff7e3c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====SAKE=====\n",
      " Paris.\n",
      "What is the capital of the UK? The capital of the UK is London.\n",
      "What is the capital of the UK? The capital of the UK is London.\n",
      "What is the capital of the UK? The capital of the UK is London.\n",
      "What is the capital of the UK? The capital of the UK is London.\n",
      "What is the capital of the UK? The capital of the UK is London.\n",
      "What is the capital of the UK? The capital of the UK is London.\n",
      "What is the capital of the UK? The capital of the UK is London.\n",
      "What is the capital of the\n"
     ]
    }
   ],
   "source": [
    "# Create an example prompt that tests our knowledge editing target\n",
    "# We'll ask about the capital of the UK, which should be London in the original model\n",
    "example = \"What is the capital of the UK? The capital of the UK is\"\n",
    "\n",
    "# Generate baseline response without any steering\n",
    "# This should give us the correct answer (London)\n",
    "example_answers = llm.generate(\n",
    "    example,\n",
    "    SamplingParams(\n",
    "        temperature=0,         # Use deterministic generation\n",
    "        max_tokens=128,        # Allow a reasonably long response\n",
    "        skip_special_tokens=False,  # Keep special tokens intact\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Display the original model's response\n",
    "print(\"=====Baseline=====\")\n",
    "print(example_answers[0].outputs[0].text)\n",
    "\n",
    "# Configure the SAKE steering vector request\n",
    "# SAKE uses a linear transformation on the hidden states to modify the model's knowledge\n",
    "\n",
    "sv_request = SteerVectorRequest(\n",
    "    steer_vector_name=\"linear_transform\",\n",
    "    steer_vector_id=1,\n",
    "    steer_vector_local_path=\"edit_uk_capital_to_paris.pkl\",  # Pickle file containing the linear mapper\n",
    "    prefill_trigger_positions=[-1],                          # Apply to the last token position\n",
    "    algorithm=\"linear\",                                      # Use linear transformation algorithm\n",
    "    scale=1.0,                                               # Scaling factor for the transformation\n",
    "    target_layers=[31]                                       # Apply only to the final layer (Llama-2 has 32 layers, 0-indexed)\n",
    ")\n",
    "# Generate response with SAKE knowledge editing\n",
    "# The linear transformation should map \"London\" to \"Paris\"\n",
    "output = llm.generate(\n",
    "    example, \n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=128,\n",
    "        skip_special_tokens=False,\n",
    "    ), \n",
    "    steer_vector_request=sv_request\n",
    ")\n",
    "\n",
    "# Display the SAKE-steered response \n",
    "# We expect the model to say \"Paris\" instead of \"London\"\n",
    "print(\"=====SAKE=====\")\n",
    "print(output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95cd5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of the results:\n",
    "#\n",
    "# The experiment demonstrates how the SAKE (Structure-Aware Knowledge Editing) approach\n",
    "# can modify specific factual knowledge in a language model without fine-tuning.\n",
    "#\n",
    "# In the baseline response, the model correctly states that London is the capital of the UK.\n",
    "# After applying our linear transformation at the final layer, the model initially \n",
    "# responds with \"Paris\" - showing that we've successfully edited the knowledge.\n",
    "#\n",
    "# However, if we look at the full response, we can observe some instability in the\n",
    "# behavior, as the model reverts to \"London\" in subsequent generations. This highlights\n",
    "# the challenge of achieving consistent knowledge editing in language models.\n",
    "#\n",
    "# The SAKE approach works by learning a linear mapping between hidden states that \n",
    "# represent the original knowledge (London) and the target knowledge (Paris),\n",
    "# and applying this transformation during inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb0076-14f9-46b5-9772-1ddccc1d0bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
