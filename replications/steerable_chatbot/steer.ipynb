{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1921024b-302f-48b3-b800-ebe0fda6ac4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-03 15:16:07 [utils.py:253] non-default args: {'disable_log_stats': True, 'enforce_eager': True, 'enable_steer_vector': True, 'enable_chunked_prefill': False, 'model': '/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/'}\n",
      "INFO 11-03 15:16:07 [model.py:657] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 11-03 15:16:07 [model.py:1746] Using max model len 32768\n",
      "INFO 11-03 15:16:10 [scheduler.py:211] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 11-03 15:16:10 [vllm.py:414] Cudagraph is disabled under eager mode\n",
      "\u001b[1;36m(EngineCore_DP0 pid=760462)\u001b[0;0m INFO 11-03 15:16:11 [core.py:94] Initializing a V1 LLM engine (v0.1.dev10888+g9d4fd0da4.d20251031) with config: model='/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={'level': None, 'mode': 0, 'debug_dump_path': None, 'cache_dir': '', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'use_cudagraph': False, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'full_cuda_graph': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=760462)\u001b[0;0m INFO 11-03 15:16:15 [parallel_state.py:1325] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=760462)\u001b[0;0m INFO 11-03 15:16:15 [gpu_model_runner.py:2902] Starting to load model /data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=760462)\u001b[0;0m INFO 11-03 15:16:16 [cuda.py:420] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e07a6ea1734eb6b675226a8bafb811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=760462)\u001b[0;0m INFO 11-03 15:16:17 [default_loader.py:314] Loading weights took 1.02 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=760462)\u001b[0;0m INFO 11-03 15:16:17 [steer_vector_model_runner_mixin.py:36] Initialized SteerVector worker manager\n",
      "\u001b[1;36m(EngineCore_DP0 pid=760462)\u001b[0;0m INFO 11-03 15:16:17 [steer_vector_model_runner_mixin.py:50] Wrapping model with steer vector support\n",
      "\u001b[1;36m(EngineCore_DP0 pid=760462)\u001b[0;0m INFO 11-03 15:16:17 [hidden_states_model_runner_mixin.py:90] Wrapped 28 decoder layers for hidden states capture\n",
      "\u001b[1;36m(EngineCore_DP0 pid=760462)\u001b[0;0m INFO 11-03 15:16:18 [gpu_model_runner.py:2971] Model loading took 2.8876 GiB and 1.179663 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=760462)\u001b[0;0m INFO 11-03 15:16:19 [gpu_worker.py:343] Available KV cache memory: 38.33 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=760462)\u001b[0;0m INFO 11-03 15:16:20 [kv_cache_utils.py:1247] GPU KV cache size: 1,435,360 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=760462)\u001b[0;0m INFO 11-03 15:16:20 [kv_cache_utils.py:1252] Maximum concurrency for 32,768 tokens per request: 43.80x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=760462)\u001b[0;0m INFO 11-03 15:16:20 [core.py:238] init engine (profile, create kv cache, warmup model) took 2.07 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=760462)\u001b[0;0m INFO 11-03 15:16:20 [vllm.py:414] Cudagraph is disabled under eager mode\n",
      "INFO 11-03 15:16:21 [llm.py:346] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest, VectorConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Initialize LLM with steering vector capability\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/\",\n",
    "    enable_steer_vector=True,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1,\n",
    "    enable_chunked_prefill=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f693928-c64e-47bb-8f04-40bb499ec746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66984bcf480b403d9b35ce4910df6d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c24d21d61d3483387d9423e397f1027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Baseline=====\n",
      "There are many activities you can do in the city this weekend. Here are some ideas:\n",
      "\n",
      "1. Visit a museum or art gallery. Many cities have museums and galleries that showcase local and international art. You can also check if there are any special exhibitions or events happening.\n",
      "\n",
      "2. Take a walk or bike ride in a park or nature reserve. Many cities have beautiful parks and nature reserves that offer a peaceful escape from the hustle and bustle of city life.\n",
      "\n",
      "3. Attend a concert or theater performance. Many cities have theaters and concert halls that host a variety of performances throughout the year.\n",
      "\n",
      "4. Explore a local market or farmers' market\n"
     ]
    }
   ],
   "source": [
    "# Create a simple example prompt for testing\n",
    "example = \"<|im_start|>user\\nWhat are some activities our family can do in the city this weekend?<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "# Generate baseline response without steering\n",
    "example_answers = llm.generate(\n",
    "    example,\n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=128,\n",
    "        skip_special_tokens=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Display baseline response\n",
    "print(\"=====Baseline=====\")\n",
    "print(example_answers[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5edaebcd-c29d-455b-99ff-ae09f8ea1c23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ed3b6c6c6b42c58da41bb7a784edc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc85e8ea7334c80934d953b01998b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Children=====\n",
      "There are many fun activities you can do in the city this weekend. Here are a few ideas:\n",
      "\n",
      "  1. Visit a museum or zoo: Many cities have museums and zoos that are open to the public. This is a great way to learn about different animals and learn about science and history.\n",
      "  2. Go to a park: Many cities have beautiful parks that are perfect for a family day out. You can play on the playground, go for a hike, or have a picnic.\n",
      "  3. Take a walk or bike ride: If you live in a city with a lot of parks or green spaces, you\n"
     ]
    }
   ],
   "source": [
    "# For children\n",
    "\n",
    "sv_request = SteerVectorRequest(\n",
    "    steer_vector_name=\"children\", \n",
    "    steer_vector_int_id=1,\n",
    "    steer_vector_local_path=\"style-probe.gguf\",\n",
    "    scale=-1.0, \n",
    "    target_layers=list(range(28)), \n",
    "    prefill_trigger_tokens=[-1], \n",
    "    generate_trigger_tokens=[-1], \n",
    "    normalize=True,\n",
    "    algorithm=\"direct\",\n",
    ")\n",
    "\n",
    "output = llm.generate(\n",
    "    example, \n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=128,\n",
    "        skip_special_tokens=False,\n",
    "    ), \n",
    "    steer_vector_request=sv_request\n",
    ")\n",
    "\n",
    "print(\"=====Children=====\")\n",
    "print(output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f67adfaa-af5a-49d0-8ba5-bf864c51538c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8141911139b477a886a391f1597824c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f030a330a184b7baa29ef942a00e6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Adult=====\n",
      "To help you plan an enjoyable weekend in the city, I'll suggest a mix of activities that cater to different interests and preferences. Here's a suggested itinerary:\n",
      "\n",
      "**Day 1:**\n",
      "\n",
      "- **Morning:**\n",
      "  - **Dinner at a Michelin-starred restaurant:** Enjoy a sophisticated meal with a view of the city skyline.\n",
      "  - **Private wine tasting:** Experience a sommelier-led tasting session at a boutique wine bar.\n",
      "\n",
      "- **Afternoon:**\n",
      "  - **City tour:** Explore the city's hidden gems and historical sites with a local guide.\n",
      "  - **Workshop:** Take a cooking class, focusing on\n"
     ]
    }
   ],
   "source": [
    "# For adult\n",
    "\n",
    "sv_request = SteerVectorRequest(\n",
    "    steer_vector_name=\"adult\", \n",
    "    steer_vector_int_id=2,\n",
    "    steer_vector_local_path=\"style-probe.gguf\",\n",
    "    scale=1.5, \n",
    "    target_layers=list(range(28)), \n",
    "    prefill_trigger_tokens=[-1], \n",
    "    generate_trigger_tokens=[-1], \n",
    "    normalize=True,\n",
    "    algorithm=\"direct\",\n",
    ")\n",
    "\n",
    "output = llm.generate(\n",
    "    example, \n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=128,\n",
    "        skip_special_tokens=False,\n",
    "    ), \n",
    "    steer_vector_request=sv_request\n",
    ")\n",
    "\n",
    "print(\"=====Adult=====\")\n",
    "print(output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221fedef-5c9b-4da9-8c6d-63f874b4adc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
