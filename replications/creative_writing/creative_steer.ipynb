{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20c618ea-3c06-48a6-a324-27caded64626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-03 19:08:37 [utils.py:253] non-default args: {'disable_log_stats': True, 'enforce_eager': True, 'enable_steer_vector': True, 'enable_chunked_prefill': False, 'model': '/data/zju-48b/xhl/huggingface_models/meta-llama/Llama-3-8B-Instruct/'}\n",
      "INFO 11-03 19:08:37 [model.py:657] Resolved architecture: LlamaForCausalLM\n",
      "INFO 11-03 19:08:37 [model.py:1746] Using max model len 8192\n",
      "INFO 11-03 19:08:40 [scheduler.py:211] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 11-03 19:08:40 [vllm.py:414] Cudagraph is disabled under eager mode\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2374420)\u001b[0;0m INFO 11-03 19:08:41 [core.py:94] Initializing a V1 LLM engine (v0.1.dev10888+g9d4fd0da4.d20251031) with config: model='/data/zju-48b/xhl/huggingface_models/meta-llama/Llama-3-8B-Instruct/', speculative_config=None, tokenizer='/data/zju-48b/xhl/huggingface_models/meta-llama/Llama-3-8B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/data/zju-48b/xhl/huggingface_models/meta-llama/Llama-3-8B-Instruct/, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={'level': None, 'mode': 0, 'debug_dump_path': None, 'cache_dir': '', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'use_cudagraph': False, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'full_cuda_graph': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2374420)\u001b[0;0m INFO 11-03 19:08:44 [parallel_state.py:1325] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2374420)\u001b[0;0m INFO 11-03 19:08:45 [gpu_model_runner.py:2902] Starting to load model /data/zju-48b/xhl/huggingface_models/meta-llama/Llama-3-8B-Instruct/...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2374420)\u001b[0;0m INFO 11-03 19:08:46 [cuda.py:420] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e09649c72f6493a8a60579ec88ce914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2374420)\u001b[0;0m INFO 11-03 19:11:28 [default_loader.py:314] Loading weights took 161.70 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2374420)\u001b[0;0m INFO 11-03 19:11:28 [steer_vector_model_runner_mixin.py:36] Initialized SteerVector worker manager\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2374420)\u001b[0;0m INFO 11-03 19:11:28 [steer_vector_model_runner_mixin.py:50] Wrapping model with steer vector support\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2374420)\u001b[0;0m INFO 11-03 19:11:28 [hidden_states_model_runner_mixin.py:90] Wrapped 32 decoder layers for hidden states capture\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2374420)\u001b[0;0m INFO 11-03 19:11:29 [gpu_model_runner.py:2971] Model loading took 14.9596 GiB and 162.213959 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2374420)\u001b[0;0m INFO 11-03 19:11:32 [gpu_worker.py:343] Available KV cache memory: 26.46 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2374420)\u001b[0;0m INFO 11-03 19:11:32 [kv_cache_utils.py:1247] GPU KV cache size: 216,784 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2374420)\u001b[0;0m INFO 11-03 19:11:32 [kv_cache_utils.py:1252] Maximum concurrency for 8,192 tokens per request: 26.46x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2374420)\u001b[0;0m INFO 11-03 19:11:32 [core.py:238] init engine (profile, create kv cache, warmup model) took 3.61 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2374420)\u001b[0;0m INFO 11-03 19:11:33 [vllm.py:414] Cudagraph is disabled under eager mode\n",
      "INFO 11-03 19:11:33 [llm.py:346] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest, VectorConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Initialize LLM with steering vector capability\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-48b/xhl/huggingface_models/meta-llama/Llama-3-8B-Instruct/\",\n",
    "    enable_steer_vector=True,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1,\n",
    "    enable_chunked_prefill=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eed06031-cc32-472c-bb88-1206eb7b61d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d5e63b08934009932164437676de89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfaa77717424edaace7856ab683c505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Baseline=====\n",
      "The town of Willow Creek was a small, quaint community nestled in the heart of the countryside. It was a place where everyone knew everyone, and everyone knew everyone's business. The town was surrounded by rolling hills and lush green forests, and the air was sweet with the scent of blooming wildflowers.\n",
      "\n",
      "At the center of the town was a charming main street, lined with old-fashioned shops and restaurants. There was the local bakery, where the aroma of freshly baked bread wafted out into the street, enticing passersby to come in and sample the day's offerings. Next door was the general store, where you could find everything from fresh produce to hardware supplies. And just across the street was the town's beloved diner, where the breakfast specials were legendary and the coffee was always hot.\n",
      "\n",
      "The town was home to a diverse group of residents, each with their own unique story to tell. There was Emma, the owner of the bakery, who had inherited the business from her mother and was determined to keep the tradition alive. There was Jack, the owner of the general store, who had grown up in the town and knew everyone's name and story. And there was Sarah, the diner's head chef, who had moved to Willow Creek from the city and was still\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Write a story about a town.\"},\n",
    "]\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/zju-48b/xhl/huggingface_models/meta-llama/Llama-3-8B-Instruct/\")\n",
    "example = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# Generate baseline response without steering\n",
    "example_answers = llm.generate(\n",
    "    example,\n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=256,\n",
    "        skip_special_tokens=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Display baseline response\n",
    "print(\"=====Baseline=====\")\n",
    "print(example_answers[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b62e53-d622-4054-88ea-b8a38aff4fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a412f132113482092c59d25c12ba6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8252514c7c4c02890d42a85b2cc592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====creative_writing=====\n",
      "The town of Ashwood lay like a whisper of smoke on the horizon, its streets winding like the fingers of the old trees that twisted and twisted until they grasped the sky. The air was the color of the moon, the stars, the darkness, and the silence. The only sound was the whisper of the wind as it whispered secrets to the trees, and the creaking of the old wooden sign that hung above the door of the only inn, the sign that creaked with the weight of the stories it held.\n",
      "\n",
      "The town was a place where the past and the present twisted together like the trees, where the memories of the past whispered through the wind, and the shadows danced like the fingers of the trees. The town was a place where the moonlight painted the streets with silver, and the stars painted the sky with the colors of the stories that lay within the walls of the old stones.\n",
      "\n",
      "The town was a place where the wind whispered the names of the people who had once lived there, the names that echoed through the streets like the whispers of the trees. The town was a place where the shadows danced like the fingers of the trees, and the moonlight painted the streets with the colors of the stories that lay within the walls of the old stones.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configure steering vector request for creative_writing control\n",
    "\n",
    "sv_request = SteerVectorRequest(\n",
    "    steer_vector_name=\"creative_writing\",\n",
    "    steer_vector_int_id=1,\n",
    "    steer_vector_local_path=\"create.gguf\", \n",
    "    prefill_trigger_tokens=[-1],\n",
    "    generate_trigger_tokens=[-1],\n",
    "    algorithm=\"direct\",\n",
    "    scale=1.5,\n",
    "    target_layers=list(range(16,30))\n",
    ")\n",
    "# Generate response with creative_writing steering\n",
    "output = llm.generate(\n",
    "    example, \n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=256,\n",
    "        skip_special_tokens=False,\n",
    "    ), \n",
    "    steer_vector_request=sv_request\n",
    ")\n",
    "\n",
    "# Display creative_writing response\n",
    "print(\"=====creative_writing=====\")\n",
    "print(output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf37b9f3-f901-4439-ab85-b402ffe61e67",
   "metadata": {},
   "source": [
    "### The writing produced through \"creative writing\" carries a more mystical and stylistic tone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
