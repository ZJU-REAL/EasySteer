{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bc485d3-985d-4e6c-978e-0953ede1349f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-03 14:53:04 [utils.py:253] non-default args: {'disable_log_stats': True, 'enforce_eager': True, 'enable_steer_vector': True, 'enable_chunked_prefill': False, 'model': '/data/zju-46/shenyl/hf/model/openai-community/gpt2/'}\n",
      "INFO 11-03 14:53:04 [model.py:657] Resolved architecture: GPT2LMHeadModel\n",
      "ERROR 11-03 14:53:04 [config.py:304] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/zju-46/shenyl/hf/model/openai-community/gpt2/'. Use `repo_type` argument if needed., retrying 1 of 2\n",
      "ERROR 11-03 14:53:06 [config.py:302] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/zju-46/shenyl/hf/model/openai-community/gpt2/'. Use `repo_type` argument if needed.\n",
      "INFO 11-03 14:53:06 [model.py:1969] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 11-03 14:53:06 [model.py:1746] Using max model len 1024\n",
      "INFO 11-03 14:53:08 [scheduler.py:211] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 11-03 14:53:08 [vllm.py:414] Cudagraph is disabled under eager mode\n",
      "\u001b[1;36m(EngineCore_DP0 pid=747946)\u001b[0;0m INFO 11-03 14:53:08 [core.py:94] Initializing a V1 LLM engine (v0.1.dev10888+g9d4fd0da4.d20251031) with config: model='/data/zju-46/shenyl/hf/model/openai-community/gpt2/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/openai-community/gpt2/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/data/zju-46/shenyl/hf/model/openai-community/gpt2/, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={'level': None, 'mode': 0, 'debug_dump_path': None, 'cache_dir': '', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'use_cudagraph': False, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'full_cuda_graph': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=747946)\u001b[0;0m INFO 11-03 14:53:10 [parallel_state.py:1325] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=747946)\u001b[0;0m INFO 11-03 14:53:10 [gpu_model_runner.py:2902] Starting to load model /data/zju-46/shenyl/hf/model/openai-community/gpt2/...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=747946)\u001b[0;0m INFO 11-03 14:53:11 [cuda.py:420] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846d919affc04debad0473bd66a48369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=747946)\u001b[0;0m INFO 11-03 14:53:11 [default_loader.py:314] Loading weights took 0.47 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=747946)\u001b[0;0m INFO 11-03 14:53:11 [steer_vector_model_runner_mixin.py:36] Initialized SteerVector worker manager\n",
      "\u001b[1;36m(EngineCore_DP0 pid=747946)\u001b[0;0m INFO 11-03 14:53:11 [steer_vector_model_runner_mixin.py:50] Wrapping model with steer vector support\n",
      "\u001b[1;36m(EngineCore_DP0 pid=747946)\u001b[0;0m INFO 11-03 14:53:11 [hidden_states_model_runner_mixin.py:90] Wrapped 12 decoder layers for hidden states capture\n",
      "\u001b[1;36m(EngineCore_DP0 pid=747946)\u001b[0;0m INFO 11-03 14:53:12 [gpu_model_runner.py:2971] Model loading took 0.2378 GiB and 0.511050 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=747946)\u001b[0;0m INFO 11-03 14:53:14 [gpu_worker.py:343] Available KV cache memory: 41.91 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=747946)\u001b[0;0m INFO 11-03 14:53:14 [kv_cache_utils.py:1247] GPU KV cache size: 1,220,720 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=747946)\u001b[0;0m INFO 11-03 14:53:14 [kv_cache_utils.py:1252] Maximum concurrency for 1,024 tokens per request: 1192.11x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=747946)\u001b[0;0m INFO 11-03 14:53:14 [core.py:238] init engine (profile, create kv cache, warmup model) took 2.04 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=747946)\u001b[0;0m INFO 11-03 14:53:14 [vllm.py:414] Cudagraph is disabled under eager mode\n",
      "INFO 11-03 14:53:15 [llm.py:346] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest, VectorConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Initialize LLM with steering vector capability\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/openai-community/gpt2/\",\n",
    "    enable_steer_vector=True,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1,\n",
    "    enable_chunked_prefill=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d8824e-d5ba-4432-a158-044420953b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bd8df942a541868d89a7ff3ceac8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc4591b7fcf4bc180e57f0025382a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Baseline=====\n",
      "\n",
      "\n",
      "The answer is simple: Spend your life.\n",
      "\n",
      "The best way to spend your life is to spend it wisely.\n",
      "\n",
      "The best way to spend your life is to spend it wisely.\n",
      "\n",
      "The best way to spend your life\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e7a21309f44484a7c8a4f4b5efd5cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c8e88a3406433490dae680dbe3b19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====lm_steer=====\n",
      "ONSORED\n",
      "\n",
      "The answer is simple: We can't.\n",
      "\n",
      "The answer is that we can't spend our life.\n",
      "\n",
      "The answer is that we can't spend our life.\n",
      "\n",
      "The answer is that we can't spend our life\n"
     ]
    }
   ],
   "source": [
    "# Create a simple example prompt for testing\n",
    "example = \"How to spend our life?\"\n",
    "# Generate baseline response without steering\n",
    "example_answers = llm.generate(\n",
    "    example,\n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=50,\n",
    "        skip_special_tokens=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Display baseline response\n",
    "print(\"=====Baseline=====\")\n",
    "print(example_answers[0].outputs[0].text)\n",
    "\n",
    "sv_request = SteerVectorRequest(\n",
    "    steer_vector_name=\"lm_steer\",\n",
    "    steer_vector_int_id=1,\n",
    "    steer_vector_local_path=\"/data/zju-48b/xhl/my_lab/github-test/EasySteer/replications/lm_steer/gpt2.pt\", # https://huggingface.co/spaces/Glaciohound/LM-Steer/blob/main/checkpoints/gpt2.pt\n",
    "    prefill_trigger_positions=[-1],\n",
    "    algorithm=\"lm_steer\",\n",
    "    scale=1.0, # here positive - Toxification, negative - Detoxification\n",
    "    target_layers=[11] # last layer\n",
    ")\n",
    "# Generate response with lm_steer\n",
    "output = llm.generate(\n",
    "    example, \n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=50,\n",
    "        skip_special_tokens=False,\n",
    "    ), \n",
    "    steer_vector_request=sv_request\n",
    ")\n",
    "\n",
    "# Display lm_steer response\n",
    "print(\"=====lm_steer=====\")\n",
    "print(output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681a18f7-2099-4e46-bd62-9b573fa9cef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
