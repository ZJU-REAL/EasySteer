{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc485d3-985d-4e6c-978e-0953ede1349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest, VectorConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Initialize LLM with steering vector capability\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/openai-community/gpt2/\",\n",
    "    enable_steer_vector=True,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8824e-d5ba-4432-a158-044420953b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple example prompt for testing\n",
    "example = \"How to spend our life?\"\n",
    "# Generate baseline response without steering\n",
    "example_answers = llm.generate(\n",
    "    example,\n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=50,\n",
    "        skip_special_tokens=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Display baseline response\n",
    "print(\"=====Baseline=====\")\n",
    "print(example_answers[0].outputs[0].text)\n",
    "\n",
    "sv_request = SteerVectorRequest(\n",
    "    steer_vector_name=\"lm_steer\",\n",
    "    steer_vector_id=1,\n",
    "    steer_vector_local_path=\"gpt2.pt\", # https://huggingface.co/spaces/Glaciohound/LM-Steer/blob/main/checkpoints/gpt2.pt\n",
    "    prefill_trigger_positions=[-1],\n",
    "    algorithm=\"lm_steer\",\n",
    "    scale=1.0, # here positive - Toxification, negative - Detoxification\n",
    "    target_layers=[11] # last layer\n",
    ")\n",
    "# Generate response with lm_steer\n",
    "output = llm.generate(\n",
    "    example, \n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=50,\n",
    "        skip_special_tokens=False,\n",
    "    ), \n",
    "    steer_vector_request=sv_request\n",
    ")\n",
    "\n",
    "# Display lm_steer response\n",
    "print(\"=====lm_steer=====\")\n",
    "print(output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681a18f7-2099-4e46-bd62-9b573fa9cef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
