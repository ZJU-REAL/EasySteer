{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "998c244b-7069-4b97-8387-11c6e27f2f48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-03 14:40:06 [utils.py:253] non-default args: {'disable_log_stats': True, 'enforce_eager': True, 'enable_steer_vector': True, 'enable_chunked_prefill': False, 'model': '/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/'}\n",
      "INFO 11-03 14:40:06 [model.py:657] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 11-03 14:40:06 [model.py:1746] Using max model len 32768\n",
      "INFO 11-03 14:40:10 [scheduler.py:211] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 11-03 14:40:10 [vllm.py:414] Cudagraph is disabled under eager mode\n",
      "\u001b[1;36m(EngineCore_DP0 pid=743515)\u001b[0;0m INFO 11-03 14:40:11 [core.py:94] Initializing a V1 LLM engine (v0.1.dev10888+g9d4fd0da4.d20251031) with config: model='/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={'level': None, 'mode': 0, 'debug_dump_path': None, 'cache_dir': '', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'use_cudagraph': False, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'full_cuda_graph': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=743515)\u001b[0;0m INFO 11-03 14:40:14 [parallel_state.py:1325] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=743515)\u001b[0;0m INFO 11-03 14:40:15 [gpu_model_runner.py:2902] Starting to load model /data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=743515)\u001b[0;0m INFO 11-03 14:40:16 [cuda.py:420] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8041ce9d58b4b478c786daed735c437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=743515)\u001b[0;0m INFO 11-03 14:40:17 [default_loader.py:314] Loading weights took 1.10 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=743515)\u001b[0;0m INFO 11-03 14:40:17 [steer_vector_model_runner_mixin.py:36] Initialized SteerVector worker manager\n",
      "\u001b[1;36m(EngineCore_DP0 pid=743515)\u001b[0;0m INFO 11-03 14:40:17 [steer_vector_model_runner_mixin.py:50] Wrapping model with steer vector support\n",
      "\u001b[1;36m(EngineCore_DP0 pid=743515)\u001b[0;0m INFO 11-03 14:40:17 [hidden_states_model_runner_mixin.py:90] Wrapped 28 decoder layers for hidden states capture\n",
      "\u001b[1;36m(EngineCore_DP0 pid=743515)\u001b[0;0m INFO 11-03 14:40:17 [gpu_model_runner.py:2971] Model loading took 2.8876 GiB and 1.276371 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=743515)\u001b[0;0m INFO 11-03 14:40:19 [gpu_worker.py:343] Available KV cache memory: 38.33 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=743515)\u001b[0;0m INFO 11-03 14:40:20 [kv_cache_utils.py:1247] GPU KV cache size: 1,435,360 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=743515)\u001b[0;0m INFO 11-03 14:40:20 [kv_cache_utils.py:1252] Maximum concurrency for 32,768 tokens per request: 43.80x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=743515)\u001b[0;0m INFO 11-03 14:40:20 [core.py:238] init engine (profile, create kv cache, warmup model) took 2.13 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=743515)\u001b[0;0m INFO 11-03 14:40:20 [vllm.py:414] Cudagraph is disabled under eager mode\n",
      "INFO 11-03 14:40:21 [llm.py:346] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Initialize LLM\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/\",\n",
    "    enable_steer_vector=True,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1,\n",
    "    enable_chunked_prefill=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ae9f48-1b17-4a8b-b9f2-96f447f7dd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e15b7b48f2400b8a88c0034f327fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1c5b49d0834cedb8fb96490155fb4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "# Load instructions from the Alpaca dataset\n",
    "with open('alpaca.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract just the instruction field from the first 100 examples\n",
    "instructions = [item['instruction'] for item in data][:100]\n",
    "\n",
    "# Define response prefixes for accepting and refusing requests\n",
    "accept = \"Sure! Let me\" \n",
    "refuse = \"Sorry I can’t\"\n",
    "\n",
    "text_accept = [f\"<|im_start|>user\\n{x}<|im_end|>\\n<|im_start|>assistant\\n{accept}\" for x in instructions]\n",
    "text_refuse = [f\"<|im_start|>user\\n{x}<|im_end|>\\n<|im_start|>assistant\\n{refuse}\" for x in instructions]\n",
    "# Generate answers using the LLM\n",
    "answers = llm.generate(\n",
    "    text_refuse,\n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=128,\n",
    "        skip_special_tokens=False,\n",
    "    ),\n",
    ")\n",
    "answers = [answer.outputs[0].text for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f504c1e1-5453-4488-9426-cabc46b1aeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' help you with that.', ' help you with this.', ' help you with this.', ' help you with this.', ' help you with that.', \" write a story for you. As an AI language model, I don't have the capability to write stories or generate creative content. However, I can provide you with some tips on how to write a story if you need any help with that.\", ' render a 3D model of a house as I am a text-based AI language model. However, you can easily find 3D models of houses online by searching for \"3D house models\" or \"3D house renderings\". There are many websites and platforms where you can find free or paid 3D models of houses, including Sketchfab, Thingiverse, and 3D Warehouse.', ' assist with that.', ' assist with that.', ' help you with that.']\n"
     ]
    }
   ],
   "source": [
    "print(answers[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e2d270e-f4b7-4e4c-9664-048830371fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del llm\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81c364a9-0d14-4fe1-af4b-ffac0192e8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|im_start|>', 'user', 'Ċ', 'Give', 'Ġthree', 'Ġtips', 'Ġfor', 'Ġstaying', 'Ġhealthy', '.', '<|im_end|>', 'Ċ', '<|im_start|>', 'assistant', 'Ċ', 'Sure', '!', 'ĠLet', 'Ġme']\n",
      "['<|im_start|>', 'user', 'Ċ', 'Give', 'Ġthree', 'Ġtips', 'Ġfor', 'Ġstaying', 'Ġhealthy', '.', '<|im_end|>', 'Ċ', '<|im_start|>', 'assistant', 'Ċ', 'Sorry', 'ĠI', 'Ġcan', 'âĢĻt']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/\")\n",
    "tokens = tokenizer.tokenize(text_accept[0], add_special_tokens=True)\n",
    "print(tokens)\n",
    "tokens = tokenizer.tokenize(text_refuse[0], add_special_tokens=True)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "447c54c7-5cf4-45f0-8984-89deeb449fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-03 14:41:39 [utils.py:253] non-default args: {'task': 'embed', 'enable_prefix_caching': False, 'disable_log_stats': True, 'enforce_eager': True, 'enable_chunked_prefill': False, 'model': '/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/'}\n",
      "INFO 11-03 14:41:39 [model.py:657] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 11-03 14:41:39 [model.py:1746] Using max model len 32768\n",
      "INFO 11-03 14:41:39 [vllm.py:414] Cudagraph is disabled under eager mode\n",
      "\u001b[1;36m(EngineCore_DP0 pid=744026)\u001b[0;0m INFO 11-03 14:41:40 [core.py:94] Initializing a V1 LLM engine (v0.1.dev10888+g9d4fd0da4.d20251031) with config: model='/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/, enable_prefix_caching=False, chunked_prefill_enabled=False, pooler_config=PoolerConfig(pooling_type='LAST', normalize=None, dimensions=None, enable_chunked_processing=None, max_embed_len=None, softmax=None, activation=None, use_activation=None, logit_bias=None, step_tag_id=None, returned_token_ids=None), compilation_config={'level': None, 'mode': 0, 'debug_dump_path': None, 'cache_dir': '', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'use_cudagraph': False, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'full_cuda_graph': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=744026)\u001b[0;0m INFO 11-03 14:41:43 [parallel_state.py:1325] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=744026)\u001b[0;0m INFO 11-03 14:41:43 [gpu_model_runner.py:2902] Starting to load model /data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=744026)\u001b[0;0m INFO 11-03 14:41:44 [cuda.py:420] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff6e8e3a7f3424ea0c348307184469a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=744026)\u001b[0;0m INFO 11-03 14:41:45 [default_loader.py:314] Loading weights took 1.09 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=744026)\u001b[0;0m INFO 11-03 14:41:45 [hidden_states_model_runner_mixin.py:90] Wrapped 28 decoder layers for hidden states capture\n",
      "\u001b[1;36m(EngineCore_DP0 pid=744026)\u001b[0;0m INFO 11-03 14:41:46 [gpu_model_runner.py:2971] Model loading took 2.8876 GiB and 1.256934 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=744026)\u001b[0;0m INFO 11-03 14:41:48 [gpu_worker.py:343] Available KV cache memory: 37.72 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=744026)\u001b[0;0m INFO 11-03 14:41:49 [kv_cache_utils.py:1247] GPU KV cache size: 1,412,400 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=744026)\u001b[0;0m INFO 11-03 14:41:49 [kv_cache_utils.py:1252] Maximum concurrency for 32,768 tokens per request: 43.10x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=744026)\u001b[0;0m INFO 11-03 14:41:49 [core.py:238] init engine (profile, create kv cache, warmup model) took 2.76 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=744026)\u001b[0;0m INFO 11-03 14:41:49 [vllm.py:414] Cudagraph is disabled under eager mode\n",
      "INFO 11-03 14:41:50 [llm.py:346] Supported tasks: ['embed', 'token_embed']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0789e1ae3c49679d450dbb4741a177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d045fcb4d949c69a73dd796e9d2ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import easysteer.hidden_states as hs\n",
    "\n",
    "# Create a new LLM instance in reward mode\n",
    "# Note: This allows us to extract hidden states rather than generating text\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/\",\n",
    "    task=\"embed\", \n",
    "    tensor_parallel_size=1,\n",
    "    enforce_eager=True,\n",
    "    enable_prefix_caching=False,\n",
    "    enable_chunked_prefill=False\n",
    ")\n",
    "all_hidden_states, outputs = hs.get_all_hidden_states(llm, text_accept+text_refuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b765627c-4595-476b-84ac-7a6a4b392ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_hidden_states = []\n",
    "\n",
    "for sample_idx in range(len(all_hidden_states)):\n",
    "    processed_layers_for_sample = []\n",
    "    \n",
    "    for layer_idx in range(len(all_hidden_states[sample_idx])):\n",
    "        last_four_tokens_tensor = all_hidden_states[sample_idx][layer_idx][-4:]\n",
    "        average_hidden_state = last_four_tokens_tensor.mean(dim=0)\n",
    "        processed_layers_for_sample.append([average_hidden_state])\n",
    "        \n",
    "    processed_hidden_states.append(processed_layers_for_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8df27ea5-5baa-4716-af08-34c742cf06ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9e897872d743009067d5780b6afad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing PCA directions:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-03 14:42:51] INFO pca.py:114: Layer 0: PCA on centered data explains 99.57811% of the variance\n",
      "[2025-11-03 14:42:51] INFO pca.py:169: Layer 0: Direction corrected (flipped)\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 1: PCA on centered data explains 99.30754% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:169: Layer 1: Direction corrected (flipped)\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 2: PCA on centered data explains 99.15779% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 3: PCA on centered data explains 98.88766% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 4: PCA on centered data explains 98.79914% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:169: Layer 4: Direction corrected (flipped)\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 5: PCA on centered data explains 98.56783% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:169: Layer 5: Direction corrected (flipped)\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 6: PCA on centered data explains 98.34915% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:169: Layer 6: Direction corrected (flipped)\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 7: PCA on centered data explains 98.03923% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 8: PCA on centered data explains 97.39847% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 9: PCA on centered data explains 96.59548% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 10: PCA on centered data explains 95.37398% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:169: Layer 10: Direction corrected (flipped)\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 11: PCA on centered data explains 94.13329% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 12: PCA on centered data explains 93.72799% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 13: PCA on centered data explains 92.26692% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 14: PCA on centered data explains 91.84000% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 15: PCA on centered data explains 90.81014% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:169: Layer 15: Direction corrected (flipped)\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 16: PCA on centered data explains 88.13887% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:169: Layer 16: Direction corrected (flipped)\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 17: PCA on centered data explains 87.51570% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:169: Layer 17: Direction corrected (flipped)\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 18: PCA on centered data explains 86.49007% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:169: Layer 18: Direction corrected (flipped)\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 19: PCA on centered data explains 86.22511% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 20: PCA on centered data explains 83.85553% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 21: PCA on centered data explains 81.52233% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 22: PCA on centered data explains 80.39566% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 23: PCA on centered data explains 79.63254% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 24: PCA on centered data explains 78.93766% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 25: PCA on centered data explains 77.39483% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 26: PCA on centered data explains 76.32961% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:114: Layer 27: PCA on centered data explains 76.73433% of the variance\n",
      "[2025-11-03 14:42:52] INFO pca.py:169: Layer 27: Direction corrected (flipped)\n",
      "[2025-11-03 14:42:52] INFO gguf_writer.py:102: gguf: This GGUF file is for Little Endian only\n",
      "[2025-11-03 14:42:52] WARNING gguf_writer.py:274: Duplicated key name 'controlvector.method', overwriting it with new value 'center' of type STRING\n",
      "[2025-11-03 14:42:52] INFO gguf_writer.py:181: Writing the following files:\n",
      "[2025-11-03 14:42:52] INFO gguf_writer.py:186: refuse-pca.gguf: n_tensors = 28, total_size = 172.0K\n"
     ]
    }
   ],
   "source": [
    "from easysteer.steer import extract_pca_control_vector, StatisticalControlVector\n",
    "control_vector = extract_pca_control_vector(\n",
    "    all_hidden_states=processed_hidden_states, \n",
    "    positive_indices=list(range(100)), \n",
    "    negative_indices=list(range(100,200)), \n",
    "    model_type=\"qwen2.5\",\n",
    "    method=\"center\",\n",
    "    token_pos=-1, \n",
    "    normalize=True\n",
    ")\n",
    "control_vector.export_gguf(\"refuse-pca.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d6d262-f782-42fd-9a18-6c88e79029c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
