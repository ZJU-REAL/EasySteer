{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb70b4e4-205c-4097-9137-8a58b3f6fa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-12 17:15:18 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-12 17:15:33 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 07-12 17:15:33 [config.py:1472] Using max model len 131072\n",
      "WARNING 07-12 17:15:33 [arg_utils.py:1577] The model has a long context length (131072). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value.\n",
      "WARNING 07-12 17:15:33 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-12 17:15:33 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294) with config: model='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-12 17:15:34 [cuda.py:351] Using Flash Attention backend.\n",
      "INFO 07-12 17:15:37 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 07-12 17:15:37 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8c4383c1964154a6095b7f4bf34dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-12 17:15:38 [default_loader.py:272] Loading weights took 1.10 seconds\n",
      "INFO 07-12 17:15:39 [model_runner.py:1255] Model loading took 3.3461 GiB and 1.255305 seconds\n",
      "INFO 07-12 17:15:43 [worker.py:295] Memory profiling takes 3.98 seconds\n",
      "INFO 07-12 17:15:43 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.44GiB) x gpu_memory_utilization (0.90) = 42.69GiB\n",
      "INFO 07-12 17:15:43 [worker.py:295] model weights take 3.35GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 8.07GiB; the rest of the memory reserved for KV Cache is 31.22GiB.\n",
      "INFO 07-12 17:15:43 [executor_base.py:115] # cuda blocks: 73063, # CPU blocks: 9362\n",
      "INFO 07-12 17:15:43 [executor_base.py:120] Maximum concurrency for 131072 tokens per request: 8.92x\n",
      "INFO 07-12 17:15:47 [llm_engine.py:430] init engine (profile, create kv cache, warmup model) took 8.04 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faffca0451e54012a204a09db0d4a0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff0b3d0325d46d4b97ae86279e76ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Baseline=====\n",
      "\n",
      "Okay, so I need to figure out what 2 plus 3 is. Hmm, let me think about this. I remember from school that when you add numbers, you combine them to get a total. So, if I have two apples and someone gives me three more apples, how many apples do I have in total?\n",
      "\n",
      "Wait, maybe I should visualize this. If I have two blocks and I add three more blocks, how many blocks do I have? Let me count them: one, two, three, four. So, that's four blocks in total. Does that make sense? Yeah, that seems right.\n",
      "\n",
      "Alternatively, I can think about it on a number line. Starting at 2, if I move three units to the right, I land on 5. So, 2 plus 3 equals 5. That also checks out.\n",
      "\n",
      "Is there another way to verify this? Maybe using objects. If I have two coins and someone gives me three more coins, how many coins do I have? Counting them: one, two, three, four, five. So, again, that's five coins. That seems consistent.\n",
      "\n",
      "I guess another way to look at it is by using addition tables or flashcards. I know that 2 plus 3 is a basic addition fact, and it's usually taught early on. So, if I recall correctly, 2 plus 3 equals 5. I don't think I need to do any complicated calculations here.\n",
      "\n",
      "Wait, could there be a different interpretation of the question? Like, maybe it's a trick question or something? But no, it's straightforward. It's just 2 plus 3. There's no ambiguity in the numbers or the operation. So, I don't think there's any trick here.\n",
      "\n",
      "Maybe I should consider if there's any real-world application where this addition would be useful. For example, if I have two books on one shelf and three books on another shelf, how many books do I have in total? That would be 2 plus 3, which is 5 books. That makes sense.\n",
      "\n",
      "Or, if I'm counting the number of steps I take in a day, and I take 2 steps in the morning and 3 steps in the afternoon, how many steps have I taken by the end of the day? That would be 2 plus 3, which is 5 steps. Again, that works.\n",
      "\n",
      "I think I'm overcomplicating this. It's a simple addition problem. 2 plus 3 is 5. I don't see any reason to doubt that. Maybe I should just write it down to make sure.\n",
      "\n",
      "So, 2 plus 3 equals 5. Yeah, that's correct. I don't think I made any mistakes here. It's a basic arithmetic operation, and I've verified it through different methods: counting blocks, number line, real-world examples, and even recalling the addition fact.\n",
      "\n",
      "I guess another way to think about it is by breaking down the numbers. 2 can be thought of as 1 plus 1, and 3 can be thought of as 2 plus 1. So, adding them together: (1 + 1) + (2 + 1) equals 1 + 1 + 2 + 1. Adding those up: 1 + 1 is 2, 2 + 2 is 4, and 4 + 1 is 5. So, that also gives me 5. That's another way to confirm it.\n",
      "\n",
      "Alternatively, using the commutative property of addition, which states that the order of the numbers doesn't matter. So, 2 plus 3 is the same as 3 plus 2. If I do 3 plus 2, that's 5 as well. So, that's consistent.\n",
      "\n",
      "I think I've covered all the bases here. I've used different methods to verify that 2 plus 3 equals 5. I've thought about real-world applications, considered different ways to break down the numbers, and even recalled the addition fact. All of these methods lead me to the same conclusion. So, I'm pretty confident that 2 plus 3 is indeed 5.\n",
      "\n",
      "Just to recap, here's how I arrived at the answer:\n",
      "\n",
      "1. I started by visualizing the problem with blocks, which helped me count and see that 2 plus 3 equals 5.\n",
      "2. I then used a number line to move from 2 to 5, confirming the result.\n",
      "3. I considered real-world examples, like coins and books, to ensure the addition makes sense in practical scenarios.\n",
      "4. I broke down the numbers into smaller parts and added them together, which also gave me 5.\n",
      "5. I recalled the addition fact and confirmed it again.\n",
      "6. I used the commutative property to verify that the order of addition doesn't affect the result.\n",
      "7. I thought about the problem in different ways to ensure I wasn't missing anything.\n",
      "\n",
      "All these steps led me to the same answer, so I don't think I made any mistakes. It's a solid understanding of basic arithmetic, and I can't see any reason to doubt it.\n",
      "\n",
      "In summary, 2 plus 3 equals 5. I've verified this through multiple methods, and all of them consistently show that the result is 5. I feel confident about this answer now.\n",
      "\n",
      "**Final Answer**\n",
      "The result of 2 + 3 is \\boxed{5}.\n",
      "</think>\n",
      "\n",
      "To determine the result of 2 + 3, we can use multiple methods to verify the answer:\n",
      "\n",
      "1. **Visualizing with blocks**: Starting with 2 blocks and adding 3 more blocks results in 5 blocks.\n",
      "2. **Number line**: Moving from 2 to 5 on a number line confirms the result.\n",
      "3. **Real-world examples**: Considering coins and books, we find that 2 plus 3 equals 5 in practical scenarios.\n",
      "4. **Breaking down numbers**: Expressing 2 as 1 + 1 and 3 as 2 + 1, and adding them together gives 5.\n",
      "5. **Recalling addition fact**: The result of 2 + 3 is a basic addition fact, which is 5.\n",
      "6. **Commutative property**: Since addition is commutative, 3 + 2 also equals 5.\n",
      "7. **Verification**: Using the commutative property and confirming the result.\n",
      "\n",
      "All these methods consistently show that the result of 2 + 3 is 5.\n",
      "\n",
      "### Final Answer\n",
      "The result of 2 + 3 is \\boxed{5}.\n",
      "Length:  1359\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c7164289414324b4561db021f2fa37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47a1830453f45d68b8fd5a88a0825e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest, VectorConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\"\n",
    ")\n",
    "\n",
    "# Initialize LLM with steering vector capability\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\",\n",
    "    enable_steer_vector=True,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1\n",
    ")\n",
    "\n",
    "# Create a simple example prompt for testing\n",
    "example = \"Please reason step by step, and put your final answer within \\\\boxed{}.\\nUser: \" + \"2 + 3 = ?\" + \"\\nAssistant: <think>\"\n",
    "# Generate baseline response without steering\n",
    "example_answers = llm.generate(\n",
    "    example,\n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=4096,\n",
    "        skip_special_tokens=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Display baseline response\n",
    "print(\"=====Baseline=====\")\n",
    "print(example_answers[0].outputs[0].text)\n",
    "print(\"Length: \", len(tokenizer.tokenize(example_answers[0].outputs[0].text, add_special_tokens=True)))\n",
    "\n",
    "\n",
    "# Define the suffix for newline tokens in the tokenizer\n",
    "target_suffix = \"ĊĊ\"  # \"\\n\\n\" is tokenized as \"ĊĊ\"\n",
    "\n",
    "# Get complete tokenizer vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Find all tokens and their IDs that end with the target suffix\n",
    "# These are the newline tokens we'll apply steering to\n",
    "matching_tokens_ids = [\n",
    "    token_id\n",
    "    for token, token_id in vocab.items()\n",
    "    if isinstance(token, str) and token.endswith(target_suffix)\n",
    "]\n",
    "\n",
    "# Configure steering vector request for SEAL control\n",
    "sv_request = SteerVectorRequest(\n",
    "    # Name and ID for the steering vector\n",
    "    steer_vector_name=\"complex_control\",\n",
    "    steer_vector_id=4,\n",
    "    \n",
    "    # Configure the three steering vectors (execution, reflection, transition)\n",
    "    vector_configs=[\n",
    "        # Execution vector (positive scale to promote execution-like text)\n",
    "        VectorConfig(\n",
    "            path=\"execution_avg_vector.gguf\",\n",
    "            scale=1.0,                            # Positive scale promotes this behavior\n",
    "            target_layers=[20],                   # Apply at layer 20\n",
    "            generate_trigger_tokens=matching_tokens_ids,  # Apply to newline tokens\n",
    "            algorithm=\"direct\",                   # Direct application\n",
    "            normalize=False                       # Do not normalize vectors\n",
    "        ),\n",
    "        \n",
    "        # Reflection vector (negative scale to suppress reflection)\n",
    "        VectorConfig(\n",
    "            path=\"reflection_avg_vector.gguf\",\n",
    "            scale=-1.0,                           # Negative scale suppresses this behavior\n",
    "            target_layers=[20],\n",
    "            generate_trigger_tokens=matching_tokens_ids,\n",
    "            algorithm=\"direct\",\n",
    "            normalize=False\n",
    "        ),\n",
    "        \n",
    "        # Transition vector (negative scale to suppress transitions)\n",
    "        VectorConfig(\n",
    "            path=\"transition_avg_vector.gguf\",\n",
    "            scale=-1.0,                           # Negative scale suppresses this behavior\n",
    "            target_layers=[20],\n",
    "            generate_trigger_tokens=matching_tokens_ids,\n",
    "            algorithm=\"direct\", \n",
    "            normalize=False\n",
    "        ),\n",
    "    ],\n",
    "    \n",
    "    # Additional parameters\n",
    "    debug=False,                        # Don't output debug info\n",
    "    conflict_resolution=\"sequential\"    # Apply vectors in sequence\n",
    ")\n",
    "# Generate response with SEAL steering\n",
    "output = llm.generate(\n",
    "    example, \n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=4096,\n",
    "        skip_special_tokens=False,\n",
    "    ), \n",
    "    steer_vector_request=sv_request\n",
    ")\n",
    "\n",
    "# Display SEAL-steered response\n",
    "print(\"=====Seal=====\")\n",
    "print(output[0].outputs[0].text)\n",
    "print(\"Seal tokens: \", len(tokenizer.tokenize(output[0].outputs[0].text, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ef4e66-7a57-4041-a149-d2009f7fc0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
