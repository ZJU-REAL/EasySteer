{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9fb9144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest, VectorConfig\n",
    "from transformers import AutoTokenizer\n",
    "import pynvml\n",
    "\n",
    "def initialize_gpu_check():\n",
    "    \"\"\"初始化GPU检查环境\"\"\"\n",
    "    try:\n",
    "        pynvml.nvmlInit()\n",
    "    except pynvml.NVMLError as e:\n",
    "        print(f\"无法初始化NVML库: {e}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_available_gpu(mem_threshold=0.1, util_threshold=10):\n",
    "    \"\"\"获取第一个可用GPU的ID\"\"\"\n",
    "    try:\n",
    "        gpu_count = pynvml.nvmlDeviceGetCount()\n",
    "        for i in range(gpu_count):\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "            \n",
    "            # 获取显存信息\n",
    "            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "            mem_used_percent = mem_info.used / mem_info.total\n",
    "            \n",
    "            # 获取GPU利用率\n",
    "            util_info = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "            gpu_util = util_info.gpu\n",
    "            \n",
    "            # 判断是否空闲\n",
    "            if mem_used_percent < mem_threshold and gpu_util < util_threshold:\n",
    "                return i\n",
    "    except Exception as e:\n",
    "        print(f\"检查GPU时出错: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f82abfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未找到可用GPU，使用默认设备配置\n",
      "INFO 07-23 10:17:30 [config.py:841] This model supports multiple tasks: {'classify', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 07-23 10:17:30 [config.py:1472] Using max model len 32768\n",
      "WARNING 07-23 10:17:30 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-23 10:17:30 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7503+gfc71f0f) with config: model='/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-23 10:17:31 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 446.00 MiB. GPU 0 has a total capacity of 47.40 GiB of which 440.00 MiB is free. Process 3487030 has 35.53 GiB memory in use. Including non-PyTorch memory, this process has 11.43 GiB memory in use. Of the allocated memory 10.97 GiB is allocated by PyTorch, and 144.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_path)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# 初始化LLM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_steer_vector\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     32\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/entrypoints/llm.py:272\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, task, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    242\u001b[39m engine_args = EngineArgs(\n\u001b[32m    243\u001b[39m     model=model,\n\u001b[32m    244\u001b[39m     task=task,\n\u001b[32m   (...)\u001b[39m\u001b[32m    268\u001b[39m     **kwargs,\n\u001b[32m    269\u001b[39m )\n\u001b[32m    271\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    276\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/engine/llm_engine.py:503\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    500\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv1\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[32m    501\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/engine/llm_engine.py:479\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    471\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_vllm_config\u001b[39m(\n\u001b[32m    473\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    477\u001b[39m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    478\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLLMEngine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_executor_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/engine/llm_engine.py:267\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28mself\u001b[39m.generation_config_fields = (\n\u001b[32m    261\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_config.try_get_generation_config())\n\u001b[32m    263\u001b[39m \u001b[38;5;28mself\u001b[39m.input_preprocessor = InputPreprocessor(\u001b[38;5;28mself\u001b[39m.model_config,\n\u001b[32m    264\u001b[39m                                             \u001b[38;5;28mself\u001b[39m.tokenizer,\n\u001b[32m    265\u001b[39m                                             mm_registry)\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[38;5;28mself\u001b[39m.model_executor = \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_config.runner_type != \u001b[33m\"\u001b[39m\u001b[33mpooling\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._initialize_kv_caches()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/executor/executor_base.py:55\u001b[39m, in \u001b[36mExecutorBase.__init__\u001b[39m\u001b[34m(self, vllm_config)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mself\u001b[39m.observability_config = vllm_config.observability_config\n\u001b[32m     54\u001b[39m \u001b[38;5;28mself\u001b[39m.steer_vector_config = vllm_config.steer_vector_config \u001b[38;5;66;03m# 新增\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mself\u001b[39m.is_sleeping = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28mself\u001b[39m.sleeping_tags: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/executor/uniproc_executor.py:48\u001b[39m, in \u001b[36mUniProcExecutor._init_executor\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mself\u001b[39m.collective_rpc(\u001b[33m\"\u001b[39m\u001b[33minit_worker\u001b[39m\u001b[33m\"\u001b[39m, args=([kwargs], ))\n\u001b[32m     47\u001b[39m \u001b[38;5;28mself\u001b[39m.collective_rpc(\u001b[33m\"\u001b[39m\u001b[33minit_device\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mload_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/executor/uniproc_executor.py:57\u001b[39m, in \u001b[36mUniProcExecutor.collective_rpc\u001b[39m\u001b[34m(self, method, timeout, args, kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     56\u001b[39m     kwargs = {}\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m answer = \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/utils/__init__.py:2736\u001b[39m, in \u001b[36mrun_method\u001b[39m\u001b[34m(obj, method, args, kwargs)\u001b[39m\n\u001b[32m   2734\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2735\u001b[39m     func = partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2736\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/worker/worker.py:211\u001b[39m, in \u001b[36mWorker.load_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    209\u001b[39m     context = nullcontext()\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/worker/model_runner.py:1226\u001b[39m, in \u001b[36mGPUModelRunnerBase.load_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m DeviceMemoryProfiler(\u001b[38;5;28mself\u001b[39m.device) \u001b[38;5;28;01mas\u001b[39;00m m:\n\u001b[32m   1225\u001b[39m     time_before_load = time.perf_counter()\n\u001b[32m-> \u001b[39m\u001b[32m1226\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1227\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lora_config:\n\u001b[32m   1228\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m supports_lora(\n\u001b[32m   1229\u001b[39m             \u001b[38;5;28mself\u001b[39m.model\n\u001b[32m   1230\u001b[39m         ), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not support LoRA yet.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/model_executor/model_loader/__init__.py:59\u001b[39m, in \u001b[36mget_model\u001b[39m\u001b[34m(vllm_config, model_config)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     58\u001b[39m     model_config = vllm_config.model_config\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/model_executor/model_loader/base_loader.py:38\u001b[39m, in \u001b[36mBaseModelLoader.load_model\u001b[39m\u001b[34m(self, vllm_config, model_config)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_default_torch_dtype(model_config.dtype):\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m target_device:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         model = \u001b[43minitialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;66;03m# Quantization does not happen in `load_weights` but after it\u001b[39;00m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mself\u001b[39m.load_weights(model, model_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/model_executor/model_loader/utils.py:64\u001b[39m, in \u001b[36minitialize_model\u001b[39m\u001b[34m(vllm_config, prefix, model_class, model_config)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvllm_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_params \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mprefix\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_params:\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# new-style model class\u001b[39;00m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_current_vllm_config(vllm_config,\n\u001b[32m     62\u001b[39m                                  check_compile=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     63\u001b[39m                                  prefix=prefix):\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m msg = (\u001b[33m\"\u001b[39m\u001b[33mvLLM model class should accept `vllm_config` and `prefix` as \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     67\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33minput arguments. Possibly you have an old-style model class\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     68\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33m registered from out of tree and it is used for new vLLM version. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     69\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33mCheck https://docs.vllm.ai/en/latest/design/arch_overview.html \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33mfor the design and update the model class accordingly.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m warnings.warn(msg, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/model_executor/models/qwen2.py:448\u001b[39m, in \u001b[36mQwen2ForCausalLM.__init__\u001b[39m\u001b[34m(self, vllm_config, prefix)\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;28mself\u001b[39m.lora_config = lora_config\n\u001b[32m    447\u001b[39m \u001b[38;5;28mself\u001b[39m.quant_config = quant_config\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mQwen2Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_prefix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group().is_last_rank:\n\u001b[32m    452\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.tie_word_embeddings:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/compilation/decorators.py:152\u001b[39m, in \u001b[36m_support_torch_compile.<locals>.__init__\u001b[39m\u001b[34m(self, vllm_config, prefix, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *, vllm_config: VllmConfig, prefix: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[43mold_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28mself\u001b[39m.vllm_config = vllm_config\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# for CompilationLevel.DYNAMO_AS_IS , the upper level model runner\u001b[39;00m\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# will handle the compilation, so we don't need to do anything here.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/model_executor/models/qwen2.py:306\u001b[39m, in \u001b[36mQwen2Model.__init__\u001b[39m\u001b[34m(self, vllm_config, prefix, decoder_layer_type)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28mself\u001b[39m.vocab_size = config.vocab_size\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group().is_first_rank \u001b[38;5;129;01mor\u001b[39;00m (config.tie_word_embeddings\n\u001b[32m    305\u001b[39m                                     \u001b[38;5;129;01mand\u001b[39;00m get_pp_group().is_last_rank):\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     \u001b[38;5;28mself\u001b[39m.embed_tokens = \u001b[43mVocabParallelEmbedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.embed_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    313\u001b[39m     \u001b[38;5;28mself\u001b[39m.embed_tokens = PPMissingLayer()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/model_executor/layers/vocab_parallel_embedding.py:266\u001b[39m, in \u001b[36mVocabParallelEmbedding.__init__\u001b[39m\u001b[34m(self, num_embeddings, embedding_dim, params_dtype, org_num_embeddings, padding_size, quant_config, prefix)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;28mself\u001b[39m.num_org_embeddings_per_partition = (\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.shard_indices.org_vocab_end_index -\n\u001b[32m    261\u001b[39m     \u001b[38;5;28mself\u001b[39m.shard_indices.org_vocab_start_index)\n\u001b[32m    262\u001b[39m \u001b[38;5;28mself\u001b[39m.num_added_embeddings_per_partition = (\n\u001b[32m    263\u001b[39m     \u001b[38;5;28mself\u001b[39m.shard_indices.added_vocab_end_index -\n\u001b[32m    264\u001b[39m     \u001b[38;5;28mself\u001b[39m.shard_indices.added_vocab_start_index)\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquant_method\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m                                 \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_embeddings_per_partition\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m                                 \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m                                 \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_embeddings_padded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mweight_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/model_executor/layers/vocab_parallel_embedding.py:34\u001b[39m, in \u001b[36mUnquantizedEmbeddingMethod.create_weights\u001b[39m\u001b[34m(self, layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype, **extra_weight_attrs)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, layer: torch.nn.Module,\n\u001b[32m     29\u001b[39m                    input_size_per_partition: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m     30\u001b[39m                    output_partition_sizes: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], input_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m     31\u001b[39m                    output_size: \u001b[38;5;28mint\u001b[39m, params_dtype: torch.dtype,\n\u001b[32m     32\u001b[39m                    **extra_weight_attrs):\n\u001b[32m     33\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create weights for embedding layer.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     weight = Parameter(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     37\u001b[39m                        requires_grad=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     38\u001b[39m     set_weight_attrs(weight, {\u001b[33m\"\u001b[39m\u001b[33minput_dim\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33moutput_dim\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m})\n\u001b[32m     39\u001b[39m     layer.register_parameter(\u001b[33m\"\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m\"\u001b[39m, weight)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/utils/_device.py:104\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    103\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 446.00 MiB. GPU 0 has a total capacity of 47.40 GiB of which 440.00 MiB is free. Process 3487030 has 35.53 GiB memory in use. Including non-PyTorch memory, this process has 11.43 GiB memory in use. Of the allocated memory 10.97 GiB is allocated by PyTorch, and 144.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# 初始化GPU检查\n",
    "if not initialize_gpu_check():\n",
    "    print(\"使用默认设备配置\")\n",
    "    gpu_id = None  # 使用默认设备\n",
    "else:\n",
    "    # 查找可用GPU (显存使用率低于10%且利用率低于10%)\n",
    "    gpu_id = get_available_gpu(mem_threshold=0.1, util_threshold=10)\n",
    "    pynvml.nvmlShutdown()  # 关闭NVML连接\n",
    "\n",
    "# 设置CUDA设备\n",
    "if gpu_id is not None:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "    print(f\"自动选择GPU设备 {gpu_id}\")\n",
    "else:\n",
    "    print(\"未找到可用GPU，使用默认设备配置\")\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "# 模型路径\n",
    "model_path = \"/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/\"\n",
    "\n",
    "# 初始化tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 初始化LLM\n",
    "llm = LLM(\n",
    "    model=model_path,\n",
    "    enable_steer_vector=True,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5893ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define negative queries (that should be refused by the model)\n",
    "negative = [\n",
    "    \"Fans of creature feature films have to endure a lot of awful movies lately. Blood Surf shamelessly joins the list of stupid, redundant pulp-horror titles about ridiculously big animals that want to turn the food chain upside down. Crocodiles are particularly successful as we already had to struggle our way through the abysmal 'Crocodile' (directed by a disappointing Tobe Hooper) and 'Lake Placid'. Blood Surf is every bit as bad as these other films and  on top of that  it likes to exaggerate tremendously. The saltwater-crocodile supposedly is 90 years old, over 30 ft long (!) and it kills for fun! During the film, he amuses himself by devouring a bunch of utterly stupid surfer-dudes & dudettes who came to seek new thrills by surfing in a shark-congested area. The only beautiful aspect about this film is the tropical location. Even though it's a completely inappropriate setting for a film like this, the lagoons and nature looks marvelous. Every other aspect is simply disastrous. There's a quite a bit of gore but it all looks fake and laughable. The dialogues are downright painful to listen to! You won't believe some of the lines these actors have to say! I know surfers are supposed to be a mentally underdeveloped group but I hope for their own sake they're not that stupid! Early in the film, one of the characters refers to Jaws as being a 'mechanical toy' but the croc here looks at least 10 times less real than Spielberg's great white shark. The visual effects in 'Blood Surf' are amateurish and the massacres fail to impress. I won't say too much about the acting since it's secondary in flicks like this. The girls look sexy in wet shirts and their boobs joyfully bounce while running away from the beast. You guessed right: Blood Surf is a very bad film. So bad it becomes fun again. But 'funny' for a whole other reason than James Hickox intended.\",\n",
    "    \"You get 5 writers together, have each write a different story with a different genre, and then you try to make one movie out of it. It's action, it's adventure, it's sci-fi, it's western, it's a mess. Sorry, but this movie absolutely stinks. 4.5 is giving it an awefully high rating. That said, it's movies like this that make me think I could write movies, and I can barely write.\",\n",
    "    \"I'm usually not one to say that a film is not worth watching, but this is certainly an extenuating circumstance. The only true upside to this film is Cornelia Sharpe, looking rather attractive, and the fact that this film is REALLY short.<br /><br />The plot in the film is unbelievably boring and goes virtually nowhere throughout the film. None of the characters are even remotely interesting and there is no reason to care about anyone. I'm not sure why on earth Sean Connery agreed to do this film, but he should have definitely passed on this one.<br /><br />The only reason I could see for seeing this film is if you are a die-hard Sean Connery fan and simply want to see everything he's done. Save this one for last though.<br /><br />Well, if you by some miracle end up seeing this despite my review (or any of the other reviews on this site), then I hope you enjoy it more than I did. Thanks for reading.\"\n",
    "]\n",
    "\n",
    "# Define positive queries (that should get helpful responses)\n",
    "positive = [\n",
    "    \"This is a excellent start to the film career of Mickey Rooney. His talents here shows that a long career is ahead for him. The car and truck chase is exciting for the 1937 era. This start of the Andy Hardy series is an American treasure in my book. Spring Byington performance is excellent as usual. Please Mr Rooney or owners of the film rights, take a chance and get this produced on DVD. I think it would be a winner.\",\n",
    "    \"Tintin and I recently aired as an episode of PBS's P.O.V. series. It's based on a taped interview of Georges Remi a.k.a. Herge, Tintin's creator, from 1971 in which in discusses his various experiences publishing his popular character, first in a Catholic newspaper, then in his own series of comic books. Awesome sweeping views of various comic pages and surreal images of Herge's dreams. I first encountered Tintin in the pages of Children's Digest at my local elementary school library reading The Secrets of the Unicorn. My mom later got a subscription to CD and I read the entire Red Rackham's Treasure every month in 1978. I remember seeing some Tintin comic books in a local book store after that but for some reason I didn't get any probably because I was 12 and I thought I was outgrowing them. I do have Breaking Free, a book written and drawn by J. Daniels, published in 1989, six years after Herge's death. Haven't read it yet. This film also covers the artist's personal life as when he left his first wife after his affair with a colorist in his employ (whom he later married). Her name is Fanny and she is interviewed here. If you love Tintin and his creator, this film is definitely worth a look. Update: 9/4/07-I've now read Breaking Free. Tintin and The Captain are the only regular characters that appear here and they are tailored to the anti-capitalist views of Mr. Daniels with Tintin portrayed as a rabble rouser with a chip on his shoulder who nevertheless cares for The Captain who he's staying with. The Captain here is just trying to make ends meet with a wife and daughter that he loves dearly. They and other construction workers vow to strike after a fellow employee dies from a faulty equipment accident. The whole thing takes place in England with working-class cockney accents intact. Not the kind of thing Herge would approve of but an interesting read nonetheless. Oh, yes, dog Snowy only appears in the top left corner of the cover (which has Tintin running over the police!) and the dedication page.\",\n",
    "    \"Some thirty years ago, Author Numa Sadoul published a book length interview with the Belgian comic book artist Georges Remi (better known as Herge, the creator of Tintin). This movie catches up with Sadoul today as he recalls the interview, while we listen to the cassettes (Herge died in 1983) and see some old photos and footage of the man himself. Some parts of the interview were not published in the book at the request of Herge, and we now know these dealt with his separation from his wife, after he had an affair with one of his collaborators (who years later would become his second wife). An interesting thing the movie does not address well is the shift in the Tintin books from the early rightist and imperialist books (Tintin in the Congo, Tintin in the lands of the Soviets) to fairly anti-imperialist books just a few years later (The Blue Lotus). On the whole, I come out of this movie knowing a few more things about Herge and seeing him as a bit more unlikable than when I come in to the theater.\"\n",
    "]\n",
    "\n",
    "# Create properly formatted prompts for both harmful and normal queries\n",
    "# The format follows the model's expected chat template\n",
    "texts = [f\"<|im_start|>user\\n{x}please check the sentiment in the commitment.<|im_end|>\\n<|im_start|>assistant\\n\" for x in negative+ positive]\n",
    "\n",
    "# Generate responses for all queries\n",
    "# This will validate that the model correctly refuses harmful queries\n",
    "answers = llm.generate(\n",
    "    texts,\n",
    "    SamplingParams(\n",
    "        temperature=0,        # Deterministic generation\n",
    "        max_tokens=128,       # Limit response length\n",
    "        skip_special_tokens=False,  # Keep special tokens intact\n",
    "    ),\n",
    ")\n",
    "answers = [answer.outputs[0].text for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9784bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses for all queries\n",
    "# This will validate that the model correctly refuses harmful queries\n",
    "answers = llm.generate(\n",
    "    texts,\n",
    "    SamplingParams(\n",
    "        temperature=0,              # Deterministic generation\n",
    "        max_tokens=128,             # Limit response length\n",
    "        skip_special_tokens=False,  # Keep special tokens intact\n",
    "    ),\n",
    ")\n",
    "answers = [answer.outputs[0].text for answer in answers]\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a97531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(texts[0], add_special_tokens=True)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b9bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def find_commas_in_raw_text(text):\n",
    "    \"\"\"使用spaCy处理原始文本，返回逗号的字符位置和token索引\"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    commas = []\n",
    "    for token in doc:\n",
    "        if token.text == \",\":\n",
    "            commas.append({\n",
    "                \"token_index\": token.i,          # token在文档中的索引\n",
    "                \"start_char\": token.idx,         # 逗号的起始字符位置\n",
    "                \"end_char\": token.idx + 1,       # 逗号的结束字符位置（Python切片右开）\n",
    "                \"preceding_token\": token.nbor(-1).text if token.i > 0 else None,  # 前一个token\n",
    "                \"following_token\": token.nbor(1).text if token.i < len(doc) - 1 else None  # 后一个token\n",
    "            })\n",
    "    \n",
    "    return commas\n",
    "\n",
    "# 示例用法\n",
    "text = \"Hello, world! How are you, today?\"\n",
    "commas = find_commas_in_raw_text(text)\n",
    "\n",
    "for comma in commas:\n",
    "    print(f\"逗号位置: 字符 {comma['start_char']}, Token索引 {comma['token_index']}\")\n",
    "    print(f\"前后文: ... {comma['preceding_token']}, {comma['following_token']} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easysteer.hidden_states import get_all_hidden_states\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/\",\n",
    "    task=\"reward\",  # Use reward task to get hidden states\n",
    "    tensor_parallel_size=1\n",
    ")\n",
    "all_hidden_states, outputs = get_all_hidden_states(llm, texts)\n",
    "pos = find_commas_in_raw_text(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8562c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easysteer.steer import extract_diffmean_control_vector, extract_pca_control_vector, extract_linear_probe_control_vector, StatisticalControlVector\n",
    "\n",
    "\n",
    "# DiffMean computes the difference between harmful and normal hidden states\n",
    "control_vector = extract_diffmean_control_vector(\n",
    "    all_hidden_states=all_hidden_states,    # 3D list [sample][layer][token]\n",
    "    positive_indices=[0, 1, 2],             # Indices of harmful examples (should be refused)\n",
    "    negative_indices=[3, 4, 5],             # Indices of normal examples (should be answered)\n",
    "    model_type=\"qwen2.5\",                   # Model type\n",
    "    token_pos= pos['token_index'],          # Target the last token position\n",
    "    normalize=True                          # Normalize the resulting vector\n",
    ")\n",
    "control_vector.export_gguf(\"diffmean.gguf\")  # Save vector to file\n",
    "\n",
    "\n",
    "control_vector = extract_pca_control_vector(\n",
    "    all_hidden_states=all_hidden_states,\n",
    "    positive_indices=[0, 1, 2],\n",
    "    negative_indices=[3, 4, 5],\n",
    "    model_type=\"qwen2.5\",\n",
    "    token_pos=pos['token_index'],                  \n",
    "    normalize=True\n",
    ")\n",
    "control_vector.export_gguf(\"pca.gguf\")\n",
    "\n",
    "\n",
    "control_vector = extract_linear_probe_control_vector(\n",
    "    all_hidden_states=all_hidden_states,\n",
    "    positive_indices=[0, 1, 2],\n",
    "    negative_indices=[3, 4, 5],\n",
    "    model_type=\"qwen2.5\",\n",
    "    token_pos=pos['token_index'],                 \n",
    "    normalize=True\n",
    ")\n",
    "control_vector.export_gguf(\"lb.gguf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
