{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c795f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 15:53:31 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pynvml\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest, VectorConfig\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c380149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_gpu_check():\n",
    "    \"\"\"初始化GPU检查环境\"\"\"\n",
    "    try:\n",
    "        pynvml.nvmlInit()\n",
    "    except pynvml.NVMLError as e:\n",
    "        print(f\"无法初始化NVML库: {e}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_available_gpu(mem_threshold=0.1, util_threshold=10):\n",
    "    \"\"\"获取第一个可用GPU的ID\"\"\"\n",
    "    try:\n",
    "        gpu_count = pynvml.nvmlDeviceGetCount()\n",
    "        for i in range(gpu_count):\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "            \n",
    "            # 获取显存信息\n",
    "            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "            mem_used_percent = mem_info.used / mem_info.total\n",
    "            \n",
    "            # 获取GPU利用率\n",
    "            util_info = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "            gpu_util = util_info.gpu\n",
    "            \n",
    "            # 判断是否空闲\n",
    "            if mem_used_percent < mem_threshold and gpu_util < util_threshold:\n",
    "                return i\n",
    "    except Exception as e:\n",
    "        print(f\"检查GPU时出错: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e15b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自动选择GPU设备 4\n",
      "INFO 07-14 15:53:41 [config.py:841] This model supports multiple tasks: {'embed', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 07-14 15:53:41 [config.py:1472] Using max model len 32768\n",
      "WARNING 07-14 15:53:41 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-14 15:53:41 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294) with config: model='/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-14 15:53:42 [cuda.py:351] Using Flash Attention backend.\n",
      "INFO 07-14 15:53:42 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 07-14 15:53:42 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd620306b0394a1da288e55e8e4c0dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 15:53:44 [default_loader.py:272] Loading weights took 0.97 seconds\n",
      "INFO 07-14 15:53:44 [model_runner.py:1255] Model loading took 2.8876 GiB and 1.117002 seconds\n",
      "INFO 07-14 15:53:46 [worker.py:295] Memory profiling takes 1.53 seconds\n",
      "INFO 07-14 15:53:46 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.40GiB) x gpu_memory_utilization (0.90) = 42.66GiB\n",
      "INFO 07-14 15:53:46 [worker.py:295] model weights take 2.89GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 2.02GiB; the rest of the memory reserved for KV Cache is 37.69GiB.\n",
      "INFO 07-14 15:53:46 [executor_base.py:115] # cuda blocks: 88220, # CPU blocks: 9362\n",
      "INFO 07-14 15:53:46 [executor_base.py:120] Maximum concurrency for 32768 tokens per request: 43.08x\n",
      "INFO 07-14 15:53:50 [llm_engine.py:430] init engine (profile, create kv cache, warmup model) took 5.43 seconds\n"
     ]
    }
   ],
   "source": [
    "# 初始化GPU检查\n",
    "if not initialize_gpu_check():\n",
    "    print(\"使用默认设备配置\")\n",
    "    gpu_id = None  # 使用默认设备\n",
    "else:\n",
    "    # 查找可用GPU (显存使用率低于10%且利用率低于10%)\n",
    "    gpu_id = get_available_gpu(mem_threshold=0.1, util_threshold=10)\n",
    "    pynvml.nvmlShutdown()  # 关闭NVML连接\n",
    "\n",
    "# 设置CUDA设备\n",
    "if gpu_id is not None:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "    print(f\"自动选择GPU设备 {gpu_id}\")\n",
    "else:\n",
    "    print(\"未找到可用GPU，使用默认设备配置\")\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "# 模型路径\n",
    "model_path = \"/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/\"\n",
    "\n",
    "# 初始化tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 初始化LLM\n",
    "llm = LLM(\n",
    "    model=model_path,\n",
    "    enable_steer_vector=True,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f80fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 15:53:51 [config.py:841] This model supports multiple tasks: {'embed', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 07-14 15:53:51 [config.py:1472] Using max model len 131072\n",
      "WARNING 07-14 15:53:51 [arg_utils.py:1577] The model has a long context length (131072). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value.\n",
      "WARNING 07-14 15:53:51 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-14 15:53:51 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294) with config: model='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-14 15:53:52 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4400245c311b47b5b80a59b8c59fd24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 15:53:54 [default_loader.py:272] Loading weights took 1.28 seconds\n",
      "INFO 07-14 15:53:54 [model_runner.py:1255] Model loading took 3.3466 GiB and 1.330331 seconds\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.38 GiB. GPU 0 has a total capacity of 47.40 GiB of which 2.08 GiB is free. Including non-PyTorch memory, this process has 45.30 GiB memory in use. Of the allocated memory 44.69 GiB is allocated by PyTorch, and 310.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initialize LLM with steering vector capability\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_steer_vector\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Create a simple example prompt for testing\u001b[39;00m\n\u001b[32m     10\u001b[39m example = \u001b[33m\"\u001b[39m\u001b[33mPlease reason step by step, and put your final answer within \u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mboxed\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mUser: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m2 + 3 = ?\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAssistant: <think>\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/entrypoints/llm.py:272\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, task, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    242\u001b[39m engine_args = EngineArgs(\n\u001b[32m    243\u001b[39m     model=model,\n\u001b[32m    244\u001b[39m     task=task,\n\u001b[32m   (...)\u001b[39m\u001b[32m    268\u001b[39m     **kwargs,\n\u001b[32m    269\u001b[39m )\n\u001b[32m    271\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    276\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/engine/llm_engine.py:503\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    500\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv1\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[32m    501\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/engine/llm_engine.py:479\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    471\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_vllm_config\u001b[39m(\n\u001b[32m    473\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    477\u001b[39m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    478\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLLMEngine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_executor_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/engine/llm_engine.py:270\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs)\u001b[39m\n\u001b[32m    267\u001b[39m \u001b[38;5;28mself\u001b[39m.model_executor = executor_class(vllm_config=vllm_config)\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_config.runner_type != \u001b[33m\"\u001b[39m\u001b[33mpooling\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_kv_caches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# If usage stat is enabled, collect relevant info.\u001b[39;00m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_usage_stats_enabled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/engine/llm_engine.py:415\u001b[39m, in \u001b[36mLLMEngine._initialize_kv_caches\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Initialize the KV cache in the worker(s).\u001b[39;00m\n\u001b[32m    409\u001b[39m \n\u001b[32m    410\u001b[39m \u001b[33;03mThe workers will determine the number of blocks in both the GPU cache\u001b[39;00m\n\u001b[32m    411\u001b[39m \u001b[33;03mand the swap CPU cache.\u001b[39;00m\n\u001b[32m    412\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    413\u001b[39m start = time.time()\n\u001b[32m    414\u001b[39m num_gpu_blocks, num_cpu_blocks = (\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetermine_num_available_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache_config.num_gpu_blocks_override \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    418\u001b[39m     num_gpu_blocks_override = \u001b[38;5;28mself\u001b[39m.cache_config.num_gpu_blocks_override\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/executor/executor_base.py:106\u001b[39m, in \u001b[36mExecutorBase.determine_num_available_blocks\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdetermine_num_available_blocks\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m     94\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Determine the number of available blocks for the GPU KV cache and\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[33;03m    swappable CPU KV cache.\u001b[39;00m\n\u001b[32m     96\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    104\u001b[39m \u001b[33;03m    appended to.\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdetermine_num_available_blocks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     a = \u001b[38;5;28mmin\u001b[39m([r[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results])\n\u001b[32m    108\u001b[39m     b = \u001b[38;5;28mmin\u001b[39m([r[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/executor/uniproc_executor.py:57\u001b[39m, in \u001b[36mUniProcExecutor.collective_rpc\u001b[39m\u001b[34m(self, method, timeout, args, kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     56\u001b[39m     kwargs = {}\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m answer = \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/utils/__init__.py:2736\u001b[39m, in \u001b[36mrun_method\u001b[39m\u001b[34m(obj, method, args, kwargs)\u001b[39m\n\u001b[32m   2734\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2735\u001b[39m     func = partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2736\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/worker/worker.py:257\u001b[39m, in \u001b[36mWorker.determine_num_available_blocks\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;66;03m# Execute a forward pass with dummy inputs to profile the memory usage\u001b[39;00m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# of the model.\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m memory_profiling(\n\u001b[32m    255\u001b[39m         \u001b[38;5;28mself\u001b[39m.baseline_snapshot,\n\u001b[32m    256\u001b[39m         weights_memory=\u001b[38;5;28mself\u001b[39m.model_runner.model_memory_usage) \u001b[38;5;28;01mas\u001b[39;00m result:\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofile_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[38;5;28mself\u001b[39m._assert_memory_footprint_increased_during_profiling()\n\u001b[32m    261\u001b[39m memory_for_current_instance = total_gpu_memory * \\\n\u001b[32m    262\u001b[39m     \u001b[38;5;28mself\u001b[39m.cache_config.gpu_memory_utilization\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/worker/model_runner.py:1360\u001b[39m, in \u001b[36mGPUModelRunnerBase.profile_run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1357\u001b[39m max_num_batched_tokens = \\\n\u001b[32m   1358\u001b[39m     \u001b[38;5;28mself\u001b[39m.scheduler_config.max_num_batched_tokens\n\u001b[32m   1359\u001b[39m max_num_seqs = \u001b[38;5;28mself\u001b[39m.scheduler_config.max_num_seqs\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dummy_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_num_batched_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_seqs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/worker/model_runner.py:1486\u001b[39m, in \u001b[36mGPUModelRunnerBase._dummy_run\u001b[39m\u001b[34m(self, max_num_batched_tokens, max_num_seqs)\u001b[39m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_input.attn_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1484\u001b[39m     model_input.attn_metadata.enable_kv_scales_calculation = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1486\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1487\u001b[39m torch.cuda.synchronize()\n\u001b[32m   1488\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lora_config:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/worker/model_runner.py:1942\u001b[39m, in \u001b[36mModelRunner.execute_model\u001b[39m\u001b[34m(self, model_input, kv_caches, intermediate_tensors, num_steps, **kwargs)\u001b[39m\n\u001b[32m   1934\u001b[39m     is_decode_phase = (model_input.attn_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m   1935\u001b[39m                        model_input.attn_metadata.decode_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m   1936\u001b[39m                        model_input.attn_metadata.prefill_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1937\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_forward_context(model_input.attn_metadata,\n\u001b[32m   1938\u001b[39m                              \u001b[38;5;28mself\u001b[39m.vllm_config, virtual_engine,\n\u001b[32m   1939\u001b[39m                              current_tokens=model_input.input_tokens, \u001b[38;5;66;03m# 新增\u001b[39;00m\n\u001b[32m   1940\u001b[39m                              is_decode=is_decode_phase \u001b[38;5;66;03m# 新增\u001b[39;00m\n\u001b[32m   1941\u001b[39m                              ):\n\u001b[32m-> \u001b[39m\u001b[32m1942\u001b[39m         hidden_or_intermediate_states = \u001b[43mmodel_executable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m.\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m            \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1947\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mMultiModalKwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_kwargs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1948\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmulti_modal_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1949\u001b[39m \u001b[43m                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1950\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1951\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mseqlen_agnostic_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1952\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1953\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1955\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.observability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1956\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.observability_config.collect_model_forward_time):\n\u001b[32m   1957\u001b[39m     model_forward_end.record()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/model_executor/models/qwen2.py:478\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, positions, intermediate_tensors, inputs_embeds)\u001b[39m\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    472\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    473\u001b[39m     input_ids: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    476\u001b[39m     inputs_embeds: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    477\u001b[39m ) -> Union[torch.Tensor, IntermediateTensors]:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m                               \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/compilation/decorators.py:173\u001b[39m, in \u001b[36m_support_torch_compile.<locals>.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# torch.compiler.is_compiling() means we are inside the compilation\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# e.g. TPU has the compilation logic in model runner, so we don't\u001b[39;00m\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# need to compile the model inside.\u001b[39;00m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.do_not_compile \u001b[38;5;129;01mor\u001b[39;00m torch.compiler.is_compiling():\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m     \u001b[38;5;66;03m# the first compilation needs to have dynamic shapes marked\u001b[39;00m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.compiled_codes) < \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/model_executor/models/qwen2.py:355\u001b[39m, in \u001b[36mQwen2Model.forward\u001b[39m\u001b[34m(self, input_ids, positions, intermediate_tensors, inputs_embeds)\u001b[39m\n\u001b[32m    353\u001b[39m     residual = intermediate_tensors[\u001b[33m\"\u001b[39m\u001b[33mresidual\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[\u001b[38;5;28mself\u001b[39m.start_layer:\u001b[38;5;28mself\u001b[39m.end_layer]:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     hidden_states, residual = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group().is_last_rank:\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m IntermediateTensors({\n\u001b[32m    362\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhidden_states\u001b[39m\u001b[33m\"\u001b[39m: hidden_states,\n\u001b[32m    363\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresidual\u001b[39m\u001b[33m\"\u001b[39m: residual\n\u001b[32m    364\u001b[39m     })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/steer_vectors/layers.py:193\u001b[39m, in \u001b[36mDecoderLayerWithSteerVector.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    192\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"包装DecoderLayer的forward方法\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m     \u001b[38;5;66;03m# 动态获取当前激活的算法并应用\u001b[39;00m\n\u001b[32m    196\u001b[39m     active_algo = \u001b[38;5;28mself\u001b[39m._get_or_create_algorithm(\u001b[38;5;28mself\u001b[39m.active_algorithm_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/model_executor/models/qwen2.py:262\u001b[39m, in \u001b[36mQwen2DecoderLayer.forward\u001b[39m\u001b[34m(self, positions, hidden_states, residual)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[32m    260\u001b[39m hidden_states, residual = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(\n\u001b[32m    261\u001b[39m     hidden_states, residual)\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states, residual\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/model_executor/models/qwen2.py:92\u001b[39m, in \u001b[36mQwen2MLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     gate_up, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgate_up_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.act_fn(gate_up)\n\u001b[32m     94\u001b[39m     x, _ = \u001b[38;5;28mself\u001b[39m.down_proj(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/model_executor/layers/linear.py:510\u001b[39m, in \u001b[36mColumnParallelLinear.forward\u001b[39m\u001b[34m(self, input_)\u001b[39m\n\u001b[32m    508\u001b[39m \u001b[38;5;66;03m# Matrix multiply.\u001b[39;00m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.quant_method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m output_parallel = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquant_method\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gather_output:\n\u001b[32m    512\u001b[39m     \u001b[38;5;66;03m# All-gather across the partitions.\u001b[39;00m\n\u001b[32m    513\u001b[39m     output = tensor_model_parallel_all_gather(output_parallel)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/model_executor/layers/linear.py:226\u001b[39m, in \u001b[36mUnquantizedLinearMethod.apply\u001b[39m\u001b[34m(self, layer, x, bias)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    222\u001b[39m           layer: torch.nn.Module,\n\u001b[32m    223\u001b[39m           x: torch.Tensor,\n\u001b[32m    224\u001b[39m           bias: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_unquantized_gemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/zju-46/zhourui/EasySteer/vllm-steer/vllm/model_executor/layers/utils.py:70\u001b[39m, in \u001b[36mdefault_unquantized_gemm\u001b[39m\u001b[34m(layer, x, weight, bias)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_unquantized_gemm\u001b[39m(layer: torch.nn.Module,\n\u001b[32m     67\u001b[39m                              x: torch.Tensor,\n\u001b[32m     68\u001b[39m                              weight: torch.Tensor,\n\u001b[32m     69\u001b[39m                              bias: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 4.38 GiB. GPU 0 has a total capacity of 47.40 GiB of which 2.08 GiB is free. Including non-PyTorch memory, this process has 45.30 GiB memory in use. Of the allocated memory 44.69 GiB is allocated by PyTorch, and 310.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Create a test prompt for a benign question (should get normal response)\n",
    "example = \"<|im_start|>user\\nRumour has it that around the time that ABBA  the multi-award winning Swedish disco favourites 's star had reached its zenith, the band grew disillusioned with singing in English and yearned to perform in their native tongue. Soon after, problems began to emerge in the onetime-wed locked-watertight partnership and recordings became less and less frequent. The band dissolved, albeit unofficially, in 1982 and pop lost one of its most celebrated artists. Although they have never admitted that there's any truth in those rumours, the fact remains that ABBA would never have been so successful had they only recorded in their native tongue. If you want to appeal to the largest money-making media market in the entire world, then you must cater for English speaking audiences.<br /><br />It's amazing for me how such a small island that's located a stone-throw away from the European continent could have created perhaps the most recognised, although not most widely spoken, language in the world. Everyone speaks a little bit of English; whether it be simply 'hello' or a common swear word - you'll find an English speaker almost everywhere. Pedro Galindo obviously didn't agree, because Trampa Infernal was never subtitled for global consumption until it was released recently on budget DVD. That's a real shame, because it's actually a decent slasher movie that's a lot better than many of its English-speaking genre compatriots.<br /><br />The film launches in the somewhat unfamiliar territory of a pistol duel. Two unidentified characters are shown sneaking around a dilapidated complex searching out one another for the inevitable final showdown. After some suspense and a couple of near misses, one of the pistoleers emerges victoriously. Next we learn that they were only paintball guns and the two competitors are actually youngsters from the local town. Nacho and Mauricio are fiercest rivals and Mauricio is always trying to prove himself to be better than his soft-spoken opponent, but as of yet he hasn't succeeded.<br /><br />Later that night, whilst the victorious gunslinger celebrates his triumph with his girlfriend Alejandra and his buddy Charly, Mauricio enters the bar and says that he has one last challenge for his glorious nemesis. He says that this will be the competition that will prove to the town once and for all who deserves the uttermost respect. Nacho is at first reluctant because Alejandra warns him of the perils of continual competitiveness, but he eventually succumbs to the weight of peer pressure and agrees; much to the distaste of his morally superior partner.<br /><br />They plan to head out to the remote region of Filo de Caballo, because recent press coverage has reported that numerous people have been butchered by what locals believe to be a vicious bear. Mauricio proposes that whoever murders the animal can be regarded as the greatest and he also promises that it will be the last battle that he wages against his adversary.<br /><br />After visiting the armoury to stock up on weapons and ignoring the warnings of the elderly store-keeper, the group set out to the remoteness of the secluded woodland. Hunters become hunted as they learn that the 'bear' is actually a homicidal Vietnam vet who is still unaware that the war has ended and considers all humans as his enemy. What started as a competitive adventure suddenly becomes a battle for survival as they are stalked and slaughtered by the malevolent assassin.<br /><br />I picked up Trampa whilst studying in Madrid from a Mexican student who lived in the dorm room next-door to me. I remember that the copy I watched was faulty and the tape ended about 10 minutes before the final credits rolled, which meant I never got to see the final scenes. Thankfully I came across the budget DVD recently on Amazon and immediately added it to my collection. <br /><br />Gallindo's slasher is a surprisingly good effort that excels from its skillful direction and enthusiastic plot, which attempts to cover areas not usually approached by slasher movies. It is in fact so good that it reminded me on more than one occasion of the Arnold Schwarzenegger classic Predator. This is especially evident in the scenes that show the creepily-masked assassin jogging through the forest and stalking the panic-stricken troupe as they struggle to escape the maniac's playground.<br /><br />Despite Gallindo's obvious awareness of genre platitudes (the bogeyman even uses a claw-fingered glove a la Freddy Kruegar); Trampa also attempts to add something different to the standard template. Whilst the majority of the runtime plays by the concrete rules of the category, the final third heralds a significant step in individuality as the maniac arms himself with a machine gun and entices the hero to his lair for the final showdown. From here on, the film rapidly swaps genres and becomes almost an action film, which depending on your taste will either excite or disappoint you. The last slasher that tried to crossbreed the two styles was that shoddy eighties entry 'The Majorettes', which is not necessarily a good thing.<br /><br />As is the case with many Latin films (especially Spanish flicks by Almodovar and Amenabar), Trampa has a subtle undercurrent of a moral to its story, which is conveyed successfully without being rammed down the viewer's throat. Over indulge in the temptations of competitive masculinity and you may not always be the winner. It's a sugar-coated point, but it's handled delicately enough not to detract from the fun of the feature.<br /><br />Trampa may be cheesy, but it deserves to be seen and recognised as one of the better late slashers. The killer looks great in creepy army fatigues and white Valentine-style mask and the attempts at originality just about work. It may lack the gore that most sincere horror fans enjoy, but it has enough in terms of suspense and creativity to warrant at least one viewing.please check the sentiment in the commitment.<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "# Generate baseline response without any steering\n",
    "example_answers = llm.generate(\n",
    "    example,\n",
    "    SamplingParams(\n",
    "        temperature=0,         # Deterministic generation\n",
    "        max_tokens=128,        # Limit response length\n",
    "        skip_special_tokens=False,  # Preserve special tokens\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Display the original model response (without steering)\n",
    "print(\"=====Baseline=====\")\n",
    "print(example_answers[0].outputs[0].text)\n",
    "\n",
    "# Configure steering vectors to trigger refusal behavior\n",
    "\n",
    "# Target all layers of the model\n",
    "layers = list(range(0,28))\n",
    "\n",
    "# Create steering vector configuration\n",
    "sv_request = SteerVectorRequest(\n",
    "    # Basic configuration\n",
    "    steer_vector_name=\"represent_sentiment\",\n",
    "    steer_vector_id=1,\n",
    "    \n",
    "    # Apply four steering vectors at different token positions\n",
    "    vector_configs=[\n",
    "        # Vector for token position -1 (the last token)\n",
    "        VectorConfig(\n",
    "            path=\"diffmean-1.gguf\",      # The direction vector file\n",
    "            scale=2.0,                   # Strength of steering (positive reinforces refusal)\n",
    "            target_layers=layers,        # Apply to all model layers\n",
    "            prefill_trigger_positions=[-1],  # Position of token to modify\n",
    "            algorithm=\"direct\",          # Direct vector application\n",
    "            normalize=False              # Don't normalize vectors\n",
    "        ),\n",
    "        \n",
    "        # Vector for token position -2 (second-to-last token)\n",
    "        VectorConfig(\n",
    "            path=\"diffmean-2.gguf\",\n",
    "            scale=2.0,\n",
    "            target_layers=layers,\n",
    "            prefill_trigger_positions=[-2],\n",
    "            algorithm=\"direct\",\n",
    "            normalize=False\n",
    "        ),\n",
    "        \n",
    "        # Vector for token position -3 (third-to-last token)\n",
    "        VectorConfig(\n",
    "            path=\"diffmean-3.gguf\",\n",
    "            scale=2.0,\n",
    "            target_layers=layers,\n",
    "            prefill_trigger_positions=[-3],\n",
    "            algorithm=\"direct\",\n",
    "            normalize=False\n",
    "        ),\n",
    "\n",
    "        # Vector for token position -4 (fourth-to-last token)\n",
    "        VectorConfig(\n",
    "            path=\"diffmean-4.gguf\",\n",
    "            scale=2.0,\n",
    "            target_layers=layers,\n",
    "            prefill_trigger_positions=[-4],\n",
    "            algorithm=\"direct\",\n",
    "            normalize=False\n",
    "        ),\n",
    "    ], \n",
    "    \n",
    "    # Additional parameters\n",
    "    debug=False,                      # Don't output debug info\n",
    "    conflict_resolution=\"sequential\"   # Apply vectors in sequence\n",
    ")\n",
    "# Generate response using refusal direction steering\n",
    "# This should cause the model to refuse even benign requests\n",
    "output = llm.generate(\n",
    "    example, \n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=128,\n",
    "        skip_special_tokens=False,\n",
    "    ), \n",
    "    steer_vector_request=sv_request\n",
    ")\n",
    "\n",
    "# Display the steered response (should show refusal behavior)\n",
    "print(\"=====Refusal Direction Steered=====\")\n",
    "print(output[0].outputs[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
