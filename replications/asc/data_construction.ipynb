{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meixinyu/anaconda3/envs/easysteer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-15 15:18:07 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import vllm\n",
    "import json\n",
    "from easysteer.hidden_states import get_all_hidden_states\n",
    "from vllm import LLM,SamplingParams\n",
    "\n",
    "from easysteer.steer import extract_pca_control_vector,StatisticalControlVector\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "file_path = \"../../temp/test.jsonl\" #MATH-500\n",
    "# file_path = \"../../temp/test.jsonl\" #GSM8K\n",
    "\n",
    "model_path = \"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\"\n",
    "model_path2 = \"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/\"\n",
    "\n",
    "\n",
    "# vector_path = \"vectors/thinking_switch_pca_MATH-500.gguf\" #MATH-500\n",
    "vector_path = \"MATH500.gguf\"\n",
    "\n",
    "num_question = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_list = []\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        line_count = 0\n",
    "        for line in f:\n",
    "            if line_count >= num_question:\n",
    "                break\n",
    "            stripped_line = line.strip()\n",
    "            if not stripped_line:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                data = json.loads(stripped_line)\n",
    "                if \"problem\" in data:\n",
    "                    problem_list.append(data[\"problem\"])\n",
    "                    line_count += 1\n",
    "                elif \"question\" in data:\n",
    "                    problem_list.append(data[\"question\"])\n",
    "                    line_count += 1\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"skip: {line[:50]}...\")\n",
    "                continue\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"{str(e)}\")\n",
    "# problem_list = [\"Find the roots of $(x - 3)^3 + (x -7)^3 = (2x - 10)^3.$\",\"A regular hexagon can be divided into six equilateral triangles. If the perimeter of one of the triangles is 21 inches, what is the perimeter, in inches, of the regular hexagon?\",\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-15 15:18:17 [config.py:1472] Using max model len 131072\n",
      "WARNING 07-15 15:18:17 [arg_utils.py:1566] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 15:18:17,452\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-15 15:18:17 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 07-15 15:18:17 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-15 15:18:17 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294.d20250712) with config: model='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-15 15:18:18 [cuda.py:351] Using Flash Attention backend.\n",
      "INFO 07-15 15:18:19 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 07-15 15:18:19 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [01:00<01:00, 60.20s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [02:18<00:00, 70.72s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [02:18<00:00, 69.14s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-15 15:20:38 [default_loader.py:272] Loading weights took 138.52 seconds\n",
      "INFO 07-15 15:20:38 [model_runner.py:1255] Model loading took 14.2717 GiB and 138.692121 seconds\n",
      "INFO 07-15 15:20:40 [worker.py:295] Memory profiling takes 1.04 seconds\n",
      "INFO 07-15 15:20:40 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.44GiB) x gpu_memory_utilization (0.90) = 42.69GiB\n",
      "INFO 07-15 15:20:40 [worker.py:295] model weights take 14.27GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.43GiB; the rest of the memory reserved for KV Cache is 26.92GiB.\n",
      "INFO 07-15 15:20:40 [executor_base.py:115] # cuda blocks: 31498, # CPU blocks: 4681\n",
      "INFO 07-15 15:20:40 [executor_base.py:120] Maximum concurrency for 131072 tokens per request: 3.84x\n",
      "INFO 07-15 15:20:44 [llm_engine.py:430] init engine (profile, create kv cache, warmup model) took 5.38 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 50/50 [00:00<00:00, 1637.50it/s]\n",
      "Processed prompts: 100%|██████████| 50/50 [01:56<00:00,  2.33s/it, est. speed input: 66.14 toks/s, output: 663.60 toks/s] \n"
     ]
    }
   ],
   "source": [
    "texts_fast = []\n",
    "texts_slow = []\n",
    "\n",
    "for prob in problem_list:\n",
    "    texts_slow.append(\"<|User|>\"+prob+\"\\n<|Assistant|><think>\\n Let's think step by step.\\n\")\n",
    "\n",
    "for prob in problem_list:\n",
    "    texts_fast.append(\"<|System|>\"\n",
    "    +\"You are an expert competition mathematician. \"\n",
    "    +\"When you give a solution, express it **primarily in formal math notation\"\n",
    "    +\"** with *minimal* surrounding English. \"\n",
    "    +\"Return the final answer in a boxed format.\\n\"\n",
    "    +\"<|User|>\"\n",
    "    +\"Solve the following problem step by step. \"\n",
    "    +\"**Answer almost entirely in math notation**; keep English words to the bare minimum.\\n\\n\"\n",
    "    +\"Problem:\\n\"+prob\n",
    "    +\"\\n<|Assistant|><think>\\n\")\n",
    "\n",
    "texts=texts_fast+texts_slow\n",
    "llm = LLM(model=model_path2,task=\"generate\",tensor_parallel_size=1,enforce_eager=True)\n",
    "\n",
    "answers_fast = llm.generate(\n",
    "    texts_fast,\n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=4096,\n",
    "        skip_special_tokens=False,\n",
    "        repetition_penalty=1.1\n",
    "    ),\n",
    ")\n",
    "\n",
    "answers_fast = [answer.outputs[0].text for answer in answers_fast]\n",
    "qa_pairs_fast = [texts_fast[i] + answers_fast[i] for i in range(len(texts_fast))]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-15 15:25:05 [config.py:1472] Using max model len 131072\n",
      "WARNING 07-15 15:25:05 [arg_utils.py:1566] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.\n",
      "INFO 07-15 15:25:05 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 07-15 15:25:05 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-15 15:25:05 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294.d20250712) with config: model='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-15 15:25:07 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.39s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-15 15:25:09 [default_loader.py:272] Loading weights took 1.53 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-15 15:25:09 [model_runner.py:1255] Model loading took 3.3148 GiB and 1.634453 seconds\n",
      "INFO 07-15 15:25:10 [worker.py:295] Memory profiling takes 0.41 seconds\n",
      "INFO 07-15 15:25:10 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.44GiB) x gpu_memory_utilization (0.90) = 42.69GiB\n",
      "INFO 07-15 15:25:10 [worker.py:295] model weights take 3.31GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.38GiB; the rest of the memory reserved for KV Cache is 38.00GiB.\n",
      "INFO 07-15 15:25:10 [executor_base.py:115] # cuda blocks: 88932, # CPU blocks: 9362\n",
      "INFO 07-15 15:25:10 [executor_base.py:120] Maximum concurrency for 131072 tokens per request: 10.86x\n",
      "INFO 07-15 15:25:10 [llm_engine.py:430] init engine (profile, create kv cache, warmup model) took 1.12 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 50/50 [00:00<00:00, 1774.89it/s]\n",
      "Processed prompts: 100%|██████████| 50/50 [01:22<00:00,  1.65s/it, est. speed input: 53.51 toks/s, output: 539.60 toks/s] \n"
     ]
    }
   ],
   "source": [
    "del llm\n",
    "\n",
    "llm = LLM(model=model_path,task=\"generate\",tensor_parallel_size=1,enforce_eager=True)\n",
    "\n",
    "answers_slow = llm.generate(\n",
    "    texts_slow,\n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=4096,\n",
    "        skip_special_tokens=False,\n",
    "        repetition_penalty=1.1\n",
    "    ),\n",
    ")\n",
    "# texts=texts_fast+texts_slow\n",
    "\n",
    "\n",
    "answers_slow = [answer.outputs[0].text for answer in answers_slow]\n",
    "qa_pairs_slow = [texts_slow[i] + answers_slow[i] for i in range(len(texts_slow))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-15 15:26:49 [config.py:1472] Using max model len 16384\n",
      "INFO 07-15 15:26:49 [config.py:4615] Only \"last\" pooling supports chunked prefill and prefix caching; disabling both.\n",
      "INFO 07-15 15:26:49 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294.d20250712) with config: model='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type=None, normalize=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-15 15:26:50 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.01it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-15 15:26:52 [default_loader.py:272] Loading weights took 1.09 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-15 15:26:53 [model_runner.py:1255] Model loading took 2.8793 GiB and 1.197712 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 50/50 [00:00<00:00, 200.44it/s]\n",
      "Processed prompts: 100%|██████████| 50/50 [00:08<00:00,  5.92it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Adding requests: 100%|██████████| 50/50 [00:00<00:00, 341.63it/s]\n",
      "Processed prompts: 100%|██████████| 50/50 [00:05<00:00,  8.99it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n"
     ]
    }
   ],
   "source": [
    "del llm\n",
    "llm = LLM(model=model_path,task=\"reward\",tensor_parallel_size=1)\n",
    "\n",
    "hidden_states_fast, _= get_all_hidden_states(llm, qa_pairs_fast)\n",
    "hidden_states_slow, _= get_all_hidden_states(llm, qa_pairs_slow)\n",
    "all_hidden_states=hidden_states_fast+hidden_states_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing DiffMean directions: 100%|██████████| 28/28 [00:00<00:00, 8588.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from easysteer.steer import extract_diffmean_control_vector\n",
    "control_vector = extract_diffmean_control_vector(\n",
    "    all_hidden_states=all_hidden_states,\n",
    "    positive_indices=list(range(num_question)), \n",
    "    negative_indices=list(range(num_question,2*num_question)),\n",
    "    model_type=\"qwen2.5\",\n",
    "    token_pos=-1,\n",
    "    normalize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StatisticalControlVector(model_type='qwen2.5', method='diffmean', directions={0: memmap([-0.37762213,  0.3621289 ,  0.07671386, ...,  0.09187746,\n",
      "         0.01702148, -0.02408692], shape=(1536,), dtype=float32), 1: memmap([-0.37708008,  0.3185938 ,  0.00083008, ...,  0.10798828,\n",
      "        -0.12316895,  0.07854492], shape=(1536,), dtype=float32), 2: memmap([-0.50412107,  0.2696436 , -0.17655273, ...,  0.13890624,\n",
      "        -0.12346679,  0.2603125 ], shape=(1536,), dtype=float32), 3: memmap([-0.6581836 ,  0.06841797, -0.3773291 , ...,  0.09156249,\n",
      "        -0.11788085,  0.07102537], shape=(1536,), dtype=float32), 4: memmap([-0.67054677, -0.11105469, -0.1010547 , ..., -0.02114257,\n",
      "        -0.01477537, -0.14418948], shape=(1536,), dtype=float32), 5: memmap([-0.7164061 , -0.07879883, -0.20362793, ...,  0.02737793,\n",
      "        -0.156875  ,  0.0581836 ], shape=(1536,), dtype=float32), 6: memmap([-0.2943945 , -0.01407228, -0.28964844, ...,  0.01804686,\n",
      "         0.31013674, -0.04800782], shape=(1536,), dtype=float32), 7: memmap([-0.33347654, -0.03626953, -0.26561522, ...,  0.18441403,\n",
      "         0.28577155, -0.14757568], shape=(1536,), dtype=float32), 8: memmap([-0.15789056,  0.03645509, -0.10894534, ...,  0.34830078,\n",
      "         0.23494142, -0.04214356], shape=(1536,), dtype=float32), 9: memmap([-0.16039062, -0.39820313, -0.174541  , ...,  0.04435551,\n",
      "         0.19143066,  0.1734277 ], shape=(1536,), dtype=float32), 10: memmap([-0.3876953 , -0.2825781 , -0.2665186 , ..., -0.26649413,\n",
      "        -0.33214843,  0.14291012], shape=(1536,), dtype=float32), 11: memmap([-0.2562108 , -0.10544923, -0.28228033, ..., -0.33841795,\n",
      "        -0.01007812, -0.04265625], shape=(1536,), dtype=float32), 12: memmap([-0.32992196,  0.21152341, -0.16746104, ..., -0.25414062,\n",
      "        -0.18600097,  0.04989257], shape=(1536,), dtype=float32), 13: memmap([-0.29429698,  0.02669436, -0.11333987, ..., -0.3220703 ,\n",
      "         0.24908197,  0.3820703 ], shape=(1536,), dtype=float32), 14: memmap([-0.27222657, -0.37456053, -0.4354297 , ..., -0.21980476,\n",
      "         0.05334961,  0.21835935], shape=(1536,), dtype=float32), 15: memmap([-0.25539064, -0.20105469, -0.92452157, ..., -0.36464846,\n",
      "         0.1477539 , -0.09858894], shape=(1536,), dtype=float32), 16: memmap([-0.25945306, -0.20824218, -1.0262793 , ..., -0.04824221,\n",
      "         0.11625004, -0.01763675], shape=(1536,), dtype=float32), 17: memmap([-0.3380468 ,  0.24728513, -1.0696876 , ..., -0.11441398,\n",
      "        -0.01070315,  0.3734082 ], shape=(1536,), dtype=float32), 18: memmap([-1.28912926e-03,  5.06445348e-01, -1.41105473e+00, ...,\n",
      "        -1.02695310e+00,  1.00722656e-01,  5.67470670e-01],\n",
      "       shape=(1536,), dtype=float32), 19: memmap([ 0.57054687,  0.7983008 , -1.6722364 , ..., -1.0267577 ,\n",
      "         0.07367194,  1.1113281 ], shape=(1536,), dtype=float32), 20: memmap([ 0.47769523,  0.4148047 , -2.0434375 , ..., -1.1351172 ,\n",
      "        -0.00756836,  1.344414  ], shape=(1536,), dtype=float32), 21: memmap([ 0.64171886,  0.6799023 , -1.0577343 , ...,  0.06789062,\n",
      "        -2.1176562 ,  0.8701807 ], shape=(1536,), dtype=float32), 22: memmap([ 0.62886715,  0.44992188,  0.58054686, ...,  0.49066412,\n",
      "        -2.2940621 ,  1.2171484 ], shape=(1536,), dtype=float32), 23: memmap([ 0.00714827,  2.179375  ,  0.35304677, ...,  0.5946094 ,\n",
      "        -1.3303127 ,  2.6116407 ], shape=(1536,), dtype=float32), 24: memmap([-0.95078135,  2.2371094 ,  0.77324224, ...,  0.21453142,\n",
      "        -2.6921873 ,  3.6129687 ], shape=(1536,), dtype=float32), 25: memmap([-0.56441414,  2.3504686 , -0.1184375 , ...,  0.07265663,\n",
      "        -2.5759373 ,  2.2974803 ], shape=(1536,), dtype=float32), 26: memmap([-1.6063282 ,  3.5953321 ,  0.40812504, ..., -1.9807806 ,\n",
      "        -2.0478125 ,  2.42375   ], shape=(1536,), dtype=float32), 27: memmap([-5.460781  ,  2.8114843 ,  1.0229688 , ..., -1.0696092 ,\n",
      "        -0.31312466,  6.6971874 ], shape=(1536,), dtype=float32)}, metadata={})\n"
     ]
    }
   ],
   "source": [
    "control_vector.export_gguf(vector_path)\n",
    "control_vector = StatisticalControlVector.import_gguf(vector_path)\n",
    "print(control_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easysteer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
