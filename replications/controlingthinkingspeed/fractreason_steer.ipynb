{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meixinyu/anaconda3/envs/easysteer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-12 19:08:16 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "from vllm.steer_vectors.request import SteerVectorRequest\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from easysteer.steer import StatisticalControlVector\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StatisticalControlVector(model_type='qwen2.5', method='pca', directions={0: memmap([ 0.00432168, -0.02199909, -0.02321268, ..., -0.00344263,\n",
      "        -0.01230501,  0.00480626], shape=(1536,), dtype=float32), 1: memmap([ 0.01987046,  0.03280366,  0.03120032, ..., -0.02368647,\n",
      "         0.01520148, -0.01653092], shape=(1536,), dtype=float32), 2: memmap([-0.00510607,  0.01795737, -0.03604069, ..., -0.04757744,\n",
      "         0.04633784, -0.01485594], shape=(1536,), dtype=float32), 3: memmap([-0.01489841, -0.01592376, -0.03618836, ...,  0.0018658 ,\n",
      "         0.01329398, -0.00049858], shape=(1536,), dtype=float32), 4: memmap([ 0.02282213,  0.00764716, -0.02953233, ..., -0.00735084,\n",
      "         0.01396071,  0.00707858], shape=(1536,), dtype=float32), 5: memmap([ 0.01595395,  0.06010181, -0.03764697, ..., -0.01724483,\n",
      "        -0.02025383, -0.04356156], shape=(1536,), dtype=float32), 6: memmap([ 0.01427394,  0.00484354, -0.05836817, ...,  0.01964263,\n",
      "        -0.01654702, -0.00549133], shape=(1536,), dtype=float32), 7: memmap([-0.00215209,  0.02768967, -0.01454685, ..., -0.01137744,\n",
      "        -0.0233161 ,  0.01457334], shape=(1536,), dtype=float32), 8: memmap([ 0.03900833,  0.01726998,  0.00013723, ..., -0.0009217 ,\n",
      "        -0.00527933,  0.03357431], shape=(1536,), dtype=float32), 9: memmap([ 0.01464165,  0.00812319, -0.02496852, ...,  0.00595126,\n",
      "        -0.00857502,  0.04010152], shape=(1536,), dtype=float32), 10: memmap([ 6.6218927e-05, -1.8610926e-02, -4.7240043e-03, ...,\n",
      "         3.8327489e-02,  9.8267561e-03,  1.8115893e-02],\n",
      "       shape=(1536,), dtype=float32), 11: memmap([ 0.0001913 , -0.01233018, -0.00302569, ..., -0.02088097,\n",
      "         0.00920268,  0.03150215], shape=(1536,), dtype=float32), 12: memmap([-0.01555365, -0.00014105,  0.0063071 , ...,  0.00608727,\n",
      "         0.00287829,  0.00453475], shape=(1536,), dtype=float32), 13: memmap([-0.01501279,  0.00582042, -0.00782634, ...,  0.01866285,\n",
      "         0.00367598,  0.00753365], shape=(1536,), dtype=float32), 14: memmap([ 0.01622559, -0.01524682,  0.02636553, ..., -0.02205137,\n",
      "         0.00232118,  0.01825984], shape=(1536,), dtype=float32), 15: memmap([ 0.01237252, -0.02814317,  0.04346382, ...,  0.00435374,\n",
      "        -0.03173552,  0.00766844], shape=(1536,), dtype=float32), 16: memmap([ 0.01718647,  0.00976165,  0.02704445, ...,  0.00786501,\n",
      "        -0.01327   ,  0.00765437], shape=(1536,), dtype=float32), 17: memmap([-0.02888759,  0.00219032, -0.01749246, ..., -0.01718363,\n",
      "         0.00907855, -0.00758874], shape=(1536,), dtype=float32), 18: memmap([-0.02763386,  0.00605183, -0.00529192, ...,  0.00271007,\n",
      "         0.00532006,  0.02207435], shape=(1536,), dtype=float32), 19: memmap([-0.05963835,  0.02912919, -0.00664181, ...,  0.0309985 ,\n",
      "         0.00488341,  0.0106551 ], shape=(1536,), dtype=float32), 20: memmap([-2.8872067e-02,  5.7454342e-03, -1.1348785e-02, ...,\n",
      "         3.0534720e-02,  3.2626082e-05, -7.8236535e-03],\n",
      "       shape=(1536,), dtype=float32), 21: memmap([-0.02946358, -0.00812638, -0.01993266, ...,  0.01331535,\n",
      "        -0.0047349 , -0.03515902], shape=(1536,), dtype=float32), 22: memmap([-0.01292057, -0.00517956, -0.0094889 , ..., -0.00644009,\n",
      "        -0.00206262, -0.0056418 ], shape=(1536,), dtype=float32), 23: memmap([0.01884711, 0.0113545 , 0.00825269, ..., 0.01677592, 0.00681699,\n",
      "        0.00727489], shape=(1536,), dtype=float32), 24: memmap([-0.01521389,  0.0054974 , -0.0054059 , ..., -0.02459772,\n",
      "         0.00039497, -0.01502328], shape=(1536,), dtype=float32), 25: memmap([ 0.0043712 , -0.00924025,  0.00716575, ...,  0.02253545,\n",
      "        -0.0080556 ,  0.02760252], shape=(1536,), dtype=float32), 26: memmap([ 0.0041931 ,  0.00098312,  0.00587025, ...,  0.01257139,\n",
      "        -0.02207087,  0.0192768 ], shape=(1536,), dtype=float32), 27: memmap([-0.01656114, -0.00343759, -0.01521988, ..., -0.01773208,\n",
      "         0.01900256, -0.01441186], shape=(1536,), dtype=float32)}, metadata={})\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\"\n",
    "# vector_path=\"/home/meixinyu/EasySteer/vectors/thinking_switch_pca_MATH-500.gguf\" #change to your path\n",
    "vector_path=\"GSM8K.gguf\" #change to your path\n",
    "\n",
    "control_vector = StatisticalControlVector.import_gguf(vector_path)\n",
    "print(control_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "steer_vector_request_pos = SteerVectorRequest(\n",
    "            steer_vector_name=\"thinking_switch_pos\",\n",
    "            steer_vector_id=1,\n",
    "            steer_vector_local_path=vector_path,\n",
    "            scale=8.0,\n",
    "            prefill_trigger_tokens=\"-1\",\n",
    "            prefill_trigger_positions=[-1],\n",
    "            generate_trigger_tokens=\"-1\",\n",
    "            debug=False,\n",
    "            algorithm='direct'\n",
    "        )\n",
    "steer_vector_request_neg = SteerVectorRequest(\n",
    "            steer_vector_name=\"thinking_switch_neg\",\n",
    "            steer_vector_id=2,\n",
    "            steer_vector_local_path=vector_path,\n",
    "            scale=-8.0,\n",
    "            prefill_trigger_tokens=\"-1\",\n",
    "            prefill_trigger_positions=[-1],\n",
    "            generate_trigger_tokens=\"-1\",\n",
    "            debug=False,\n",
    "            algorithm='direct'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.0,max_tokens=4096)\n",
    "prompt_template = \"<|im_start|>user\\n%s<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "# prompt = \"Find the constant term in the expansion of $$\\\\left(10x^3-\\\\frac{1}{2x^2}\\\\right)^{5}$$\"\n",
    "prompt = \"Darrell and Allen's ages are in the ratio of 7:11. If their total age now is 162, calculate Allen's age 10 years from now.\"\n",
    "input = prompt_template % prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-12 19:08:36 [config.py:841] This model supports multiple tasks: {'generate', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 07-12 19:08:36 [config.py:1472] Using max model len 32768\n",
      "WARNING 07-12 19:08:36 [arg_utils.py:1770] --enable-steer-vector is not supported by the V1 Engine. Falling back to V0. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 19:08:36,719\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-12 19:08:36 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294.d20250712) with config: model='/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-12 19:08:37 [cuda.py:351] Using Flash Attention backend.\n",
      "INFO 07-12 19:08:38 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 07-12 19:08:38 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.12it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-12 19:08:39 [default_loader.py:272] Loading weights took 0.99 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-12 19:08:40 [model_runner.py:1255] Model loading took 2.8876 GiB and 1.147044 seconds\n",
      "INFO 07-12 19:08:41 [worker.py:295] Memory profiling takes 1.39 seconds\n",
      "INFO 07-12 19:08:41 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.44GiB) x gpu_memory_utilization (0.90) = 42.69GiB\n",
      "INFO 07-12 19:08:41 [worker.py:295] model weights take 2.89GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 2.05GiB; the rest of the memory reserved for KV Cache is 37.68GiB.\n",
      "INFO 07-12 19:08:42 [executor_base.py:115] # cuda blocks: 88185, # CPU blocks: 9362\n",
      "INFO 07-12 19:08:42 [executor_base.py:120] Maximum concurrency for 32768 tokens per request: 43.06x\n",
      "INFO 07-12 19:08:45 [model_runner.py:1592] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:20<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-12 19:09:06 [model_runner.py:1755] Graph capturing finished in 21 secs, took 0.24 GiB\n",
      "INFO 07-12 19:09:06 [llm_engine.py:430] init engine (profile, create kv cache, warmup model) took 26.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 231.12it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it, est. speed input: 21.86 toks/s, output: 146.51 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1294.54it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.51s/it, est. speed input: 13.40 toks/s, output: 79.57 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 677.27it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s, est. speed input: 85.14 toks/s, output: 72.46 toks/s]\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_path, enable_steer_vector=True, tensor_parallel_size=1)\n",
    "output_base = llm.generate(\n",
    "    input,\n",
    "    sampling_params\n",
    ")\n",
    "generated_text_base = output_base[0].outputs[0].text\n",
    "\n",
    "output_pos = llm.generate(\n",
    "                input,\n",
    "                sampling_params,\n",
    "                steer_vector_request=steer_vector_request_pos\n",
    "            )\n",
    "generated_text_pos = output_pos[0].outputs[0].text\n",
    "\n",
    "output_neg = llm.generate(\n",
    "                input,\n",
    "                sampling_params,\n",
    "                steer_vector_request=steer_vector_request_neg\n",
    "            )\n",
    "generated_text_neg = output_neg[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine Allen's age 10 years from now, we first need to find their current ages. We know that Darrell and Allen's ages are in the ratio of 7:11 and their total age now is 162.\n",
      "\n",
      "Let's denote Darrell's age by \\(7x\\) and Allen's age by \\(11x\\). According to the problem, the sum of their ages is 162. Therefore, we can write the equation:\n",
      "\\[\n",
      "7x + 11x = 162\n",
      "\\]\n",
      "Combining like terms, we get:\n",
      "\\[\n",
      "18x = 162\n",
      "\\]\n",
      "To find \\(x\\), we divide both sides of the equation by 18:\n",
      "\\[\n",
      "x = \\frac{162}{18} = 9\n",
      "\\]\n",
      "Now that we have the value of \\(x\\), we can find Darrell's and Allen's current ages. Darrell's age is:\n",
      "\\[\n",
      "7x = 7 \\times 9 = 63\n",
      "\\]\n",
      "And Allen's age is:\n",
      "\\[\n",
      "11x = 11 \\times 9 = 99\n",
      "\\]\n",
      "Next, we need to calculate Allen's age 10 years from now. Adding 10 years to Allen's current age, we get:\n",
      "\\[\n",
      "99 + 10 = 109\n",
      "\\]\n",
      "Therefore, Allen's age 10 years from now is \\(\\boxed{109}\\).\n"
     ]
    }
   ],
   "source": [
    "print(generated_text_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To calculate Allen's age 10 years from now, we first need to determine their current ages based on the given ratio and total age.\n",
      "\n",
      "Given:\n",
      "- The ratio of Darrell's age to Allen's age is 7:11.\n",
      "- Their total age now is 162.\n",
      "\n",
      "Let's denote Darrell's age as \\(7x\\) and Allen's age as \\(11x\\), where \\(x\\) is a common multiplier.\n",
      "\n",
      "The sum of their ages is given by:\n",
      "\\[7x + 11x = 162\\]\n",
      "\n",
      "Combining like terms, we get:\n",
      "\\[18x = 162\\]\n",
      "\n",
      "To find \\(x\\), we divide both sides by 18:\n",
      "\\[x = \\frac{162}{18} = 9\\]\n",
      "\n",
      "Now we can find their current ages:\n",
      "- Darrell's age: \\(7x = 7 \\times 9 = 63\\)\n",
      "- Allen's age: \\(11x = 11 \\times 9 = 99\\)\n",
      "\n",
      "To find Allen's age 10 years from now, we add 10 to his current age:\n",
      "\\[99 + 10 = 109\\]\n",
      "\n",
      "Therefore, Allen's age 10 years from now will be 109 years old.\n"
     ]
    }
   ],
   "source": [
    "print(generated_text_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/18=9\n",
      "9*11=99\n",
      "\n",
      "99-10=89\n",
      "\n",
      "89 is Allen's age 10 years from now.\n"
     ]
    }
   ],
   "source": [
    "print(generated_text_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename=f'fractreason_GSM8K_{timestamp}.txt'\n",
    "# filename=f'../../temp/fractreason_GSM8K_{timestamp}.txt'\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=== Base ===\\n\")\n",
    "    f.write(generated_text_base + \"\\n\\n\")\n",
    "    f.write(\"=== Positive: Slow thinking ===\\n\")\n",
    "    f.write(generated_text_pos + \"\\n\\n\")\n",
    "    f.write(\"=== Negative: Fast thinking ===\\n\")\n",
    "    f.write(generated_text_neg + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easysteer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
