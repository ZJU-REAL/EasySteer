{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c6e958-554f-4378-a9ac-c574bf72e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inputs_for_source = [\n",
    "    \"What is the capital of the UK?\", \"Which city is the capital of the United Kingdom?\",\n",
    "    \"Could you tell me the capital city of Great Britain?\", \"Identify the primary city of the United Kingdom.\",\n",
    "    \"Where does the UK Parliament convene?\", \"The UK Prime Minister's residence, 10 Downing Street, is in what city?\",\n",
    "    \"In which city is Buckingham Palace located?\", \"Where is the headquarters of the UK's government?\",\n",
    "    \"The famous landmark Tower Bridge is in which city?\", \"What city is the Shard located in?\",\n",
    "    \"The River Thames famously flows through which major city?\", \"I want to visit the British Museum. What city should I travel to?\",\n",
    "    \"Where can I find the London Eye?\", \"Heathrow (LHR) is the main international airport for which city?\",\n",
    "    \"King's Cross Station is a major railway hub in...\", \"The Tube is the name of the subway system in what city?\",\n",
    "    \"In which city did the 2012 Summer Olympics take place?\", \"Sherlock Holmes lived at 221B Baker Street, located in...\",\n",
    "    \"The historical Globe Theatre, associated with Shakespeare, is in...\", \"What city is known for its iconic red double-decker buses?\",\n",
    "    \"I'm writing a report. The capital of the United Kingdom is\", \"Complete the fact: The largest city in Great Britain is\",\n",
    "    \"If I fly into Gatwick Airport, what major city am I near?\", \"The financial heart of the UK is known as the City of...\",\n",
    "    \"A famous play, The Mousetrap, has been running for decades in\", \"Wembley Stadium, the national stadium of England, is located in\",\n",
    "    \"The Notting Hill Carnival takes place every year in\", \"Hyde Park is a massive green space in the middle of\",\n",
    "    \"Which city's metro system is called the Underground?\", \"The West End theatre district is a famous part of\"\n",
    "]\n",
    "\n",
    "assistant_starts_for_source = [\n",
    "    \"The capital of the UK is\", \"The capital city of the United Kingdom is\",\n",
    "    \"The capital city of Great Britain is\", \"The primary city of the United Kingdom is\",\n",
    "    \"The UK Parliament convenes in\", \"10 Downing Street is in the city of\",\n",
    "    \"Buckingham Palace is located in\", \"The headquarters of the UK's government is in\",\n",
    "    \"Tower Bridge is in the city of\", \"The Shard is located in\",\n",
    "    \"The River Thames flows through\", \"You should travel to the city of\",\n",
    "    \"You can find the London Eye in\", \"Heathrow is the main airport for\",\n",
    "    \"King's Cross Station is a major railway hub in\", \"The Tube is the subway system in\",\n",
    "    \"The 2012 Summer Olympics took place in\", \"Sherlock Holmes lived in\",\n",
    "    \"The Globe Theatre is in\", \"The city known for its red double-decker buses is\",\n",
    "    \"The capital of the United Kingdom is\", \"The largest city in Great Britain is\",\n",
    "    \"You would be near the city of\", \"The financial heart of the UK is the City of\",\n",
    "    \"The Mousetrap has been running in\", \"Wembley Stadium is located in\",\n",
    "    \"The Notting Hill Carnival takes place in\", \"Hyde Park is in\",\n",
    "    \"That would be the metro system of\", \"The West End theatre district is in\"\n",
    "]\n",
    "\n",
    "# Knowledge editing target (old information → new information)\n",
    "old_object = \"London\"\n",
    "new_object = \"Paris\"\n",
    "\n",
    "# Use assistant starting phrases as source prompts\n",
    "source_prompts_for_base = assistant_starts_for_source\n",
    "\n",
    "# Create target prompts by instructing the model to mention Paris instead of London\n",
    "target_prompts_for_base = []\n",
    "for p in source_prompts_for_base:\n",
    "    # Format: Instruct model to avoid the old object and repeat the sentence with the new object\n",
    "    prompt = f\"Do not mention {old_object}. Repeat this sentence: {p.strip()} {new_object}. {p.strip()}\"\n",
    "    target_prompts_for_base.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba602c63-12cd-445a-8417-565258f6e1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 23:17:51 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-14 23:18:04 [config.py:1472] Using max model len 4096\n",
      "INFO 07-14 23:18:04 [config.py:4615] Only \"last\" pooling supports chunked prefill and prefix caching; disabling both.\n",
      "INFO 07-14 23:18:04 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294) with config: model='/data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type=None, normalize=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-14 23:18:05 [cuda.py:351] Using Flash Attention backend.\n",
      "INFO 07-14 23:18:06 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 07-14 23:18:06 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fc16aa3bf040399207714372edf7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 23:18:10 [default_loader.py:272] Loading weights took 4.10 seconds\n",
      "INFO 07-14 23:18:11 [model_runner.py:1255] Model loading took 12.3082 GiB and 4.278845 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24d572715c04494b5b1065084cd4a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3f3dd012f64699980a41a4a16ad708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/60 [00:00<?, ?it/s, est. speed input: 0.00 toks/s,"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# According to the SAKE paper, we only need to extract the hidden states \n",
    "# from the last layer at the final position\n",
    "import easysteer.hidden_states as hs\n",
    "from vllm import LLM\n",
    "import os\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Create a new LLM instance in reward mode to extract hidden states\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/\",\n",
    "    task=\"reward\",  # Using reward task enables hidden state extraction\n",
    "    tensor_parallel_size=1\n",
    ")\n",
    "\n",
    "# Extract hidden states for both source and target prompts\n",
    "# We'll combine both lists into a single batch for extraction\n",
    "all_hidden_states, outputs = hs.get_all_hidden_states(llm, source_prompts_for_base+target_prompts_for_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f00e2477-a6c4-46e3-8c39-ff3a451f33a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting POT\n",
      "  Downloading POT-0.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: numpy>=1.16 in /data/zju-48b/xhl/anaconda3/envs/vllm-test/lib/python3.10/site-packages (from POT) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.6 in /data/zju-48b/xhl/anaconda3/envs/vllm-test/lib/python3.10/site-packages (from POT) (1.15.3)\n",
      "Downloading POT-0.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (865 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.6/865.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: POT\n",
      "Successfully installed POT-0.9.5\n"
     ]
    }
   ],
   "source": [
    "!pip install POT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5a95e7-8b52-44ff-a298-51d0e0bc73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ot  # Optimal Transport library\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Extract the last token's hidden state from the last layer for each prompt\n",
    "# First half are source prompts (London)\n",
    "source_hidden_states = [all_hidden_states[x][-1][-1] for x in range(len(source_prompts_for_base))]\n",
    "# Second half are target prompts (Paris)\n",
    "target_hidden_states = [all_hidden_states[x][-1][-1] for x in range(len(source_prompts_for_base), 2*len(source_prompts_for_base))]\n",
    "\n",
    "# Convert PyTorch tensors to NumPy arrays for optimal transport\n",
    "Xs = np.array([t.to(torch.float32).numpy() for t in source_hidden_states])\n",
    "Xt = np.array([t.to(torch.float32).numpy() for t in target_hidden_states])\n",
    "\n",
    "# Create a linear transport model with regularization parameter 0.6\n",
    "# This learns a linear transformation from source to target distributions\n",
    "linear_mapper = ot.da.LinearTransport(reg=0.6)\n",
    "linear_mapper.fit(Xs=Xs, Xt=Xt)\n",
    "\n",
    "# Test the transformation on the first example\n",
    "original_vector = Xs[0:1, :] \n",
    "transformed_vector = linear_mapper.transform(Xs=original_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9da8887-1ba2-4bd9-9600-0c54535f6e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "映射器已成功保存到: edit_uk_capital_to_paris.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Define the path to save the linear mapper\n",
    "mapper_save_path = \"edit_uk_capital_to_paris.pkl\"\n",
    "\n",
    "# Save the linear transformation model to a pickle file\n",
    "# This will be loaded during inference to apply the transformation\n",
    "with open(mapper_save_path, 'wb') as f:\n",
    "    pickle.dump(linear_mapper, f)\n",
    "\n",
    "print(f\"Linear mapper successfully saved to: {mapper_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec675f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAKE: Structure-Aware Knowledge Editing for Large Language Models\n",
    "#\n",
    "# This notebook implements the data preparation for SAKE knowledge editing:\n",
    "#\n",
    "# 1. We prepare source prompts about the capital of the UK (London)\n",
    "# 2. We create target prompts that should refer to Paris instead\n",
    "# 3. We extract hidden states from both sets of prompts\n",
    "# 4. We learn a linear transformation using optimal transport to map \n",
    "#    \"London\" knowledge to \"Paris\" knowledge\n",
    "# 5. We save this transformation for use during inference\n",
    "#\n",
    "# This demonstrates how to create knowledge editing vectors that can\n",
    "# modify a model's factual knowledge without any fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b463bc-5aca-4175-b605-71808e7237fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
