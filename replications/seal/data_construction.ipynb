{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "998c244b-7069-4b97-8387-11c6e27f2f48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-12 17:11:02 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 07-12 17:11:16 [config.py:841] This model supports multiple tasks: {'classify', 'reward', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 07-12 17:11:16 [config.py:1472] Using max model len 131072\n",
      "WARNING 07-12 17:11:16 [arg_utils.py:1577] The model has a long context length (131072). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value.\n",
      "WARNING 07-12 17:11:16 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-12 17:11:16 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294) with config: model='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-12 17:11:17 [cuda.py:351] Using Flash Attention backend.\n",
      "INFO 07-12 17:11:18 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 07-12 17:11:18 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12fe51a6d8c4d32b10771aeb9a15915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-12 17:11:19 [default_loader.py:272] Loading weights took 1.10 seconds\n",
      "INFO 07-12 17:11:20 [model_runner.py:1255] Model loading took 3.3461 GiB and 1.255827 seconds\n",
      "INFO 07-12 17:11:24 [worker.py:295] Memory profiling takes 3.95 seconds\n",
      "INFO 07-12 17:11:24 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.44GiB) x gpu_memory_utilization (0.90) = 42.69GiB\n",
      "INFO 07-12 17:11:24 [worker.py:295] model weights take 3.35GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 8.07GiB; the rest of the memory reserved for KV Cache is 31.22GiB.\n",
      "INFO 07-12 17:11:24 [executor_base.py:115] # cuda blocks: 73063, # CPU blocks: 9362\n",
      "INFO 07-12 17:11:24 [executor_base.py:120] Maximum concurrency for 131072 tokens per request: 8.92x\n",
      "INFO 07-12 17:11:30 [llm_engine.py:430] init engine (profile, create kv cache, warmup model) took 9.88 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeaea48e65bd415fba40f9c1e9682ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8fb87bf1d34e4b9cae052266c09668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Initialize LLM\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\",\n",
    "    enable_steer_vector=True,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1\n",
    ")\n",
    "\n",
    "# Define math problems for testing\n",
    "problems = [\n",
    "    \"Chandra has four bowls.  Each one is a different color (red, blue, yellow, green).  She also has exactly one glass the same color as each bowl.  If she chooses a bowl and a glass from the cupboard, how many pairings are possible?  One such pairing is a blue bowl and a yellow glass.\",\n",
    "    \"The distance between two cities on a map is 15 inches. If the scale is 0.25 inches = 3 miles, how many miles apart are the actual cities?\",\n",
    "    \"How many prime numbers are between 20 and 30?\"\n",
    "]\n",
    "\n",
    "# Create prompt texts from problems\n",
    "texts = [\"Please reason step by step, and put your final answer within \\\\boxed{}.\\nUser: \" + problem + \"\\nAssistant: <think>\" for problem in problems]\n",
    "\n",
    "# Generate answers using the LLM\n",
    "answers = llm.generate(\n",
    "    texts,\n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=4096,\n",
    "        skip_special_tokens=False,\n",
    "    ),\n",
    ")\n",
    "answers = [answer.outputs[0].text for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c364a9-0d14-4fe1-af4b-ffac0192e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Create QA pairs by combining prompts and answers\n",
    "qa_pairs = [texts[i] + answers[i] for i in range(len(texts))]\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\")\n",
    "\n",
    "# The newline token suffix in tokenizer vocabulary\n",
    "target_suffix = \"ĊĊ\"  # \"\\n\\n\" is tokenized as \"ĊĊ\"\n",
    "\n",
    "# Process each QA pair to find newline positions\n",
    "all_tokens_list = []\n",
    "all_newline_positions = []\n",
    "for qa in qa_pairs:\n",
    "    # Tokenize the QA pair\n",
    "    tokens = tokenizer.tokenize(qa, add_special_tokens=True)\n",
    "    all_tokens_list.append(tokens)\n",
    "    \n",
    "    # Find all positions of \"ĊĊ\" in the tokens\n",
    "    # These represent potential paragraph breaks in the text\n",
    "    positions = [\n",
    "        i for i, token in enumerate(tokens) \n",
    "        if isinstance(token, str) and token.endswith(target_suffix)\n",
    "    ]\n",
    "    all_newline_positions.append(positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fa65041-c059-4fd6-8405-309d59649711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Summary of classification results for all samples ---\n",
      "\n",
      "--- Analysis of QA Pair 1 ---\n",
      "Transition positions: [442, 1139, 1729, 1924, 2114, 2519, 2683, 2925, 3089, 3331, 3495, 3737, 3884, 4031, 4178]\n",
      "Reflection positions: [240, 681, 800, 850, 1061, 1233, 1282, 1339, 1439, 1656, 1681, 1853, 1964, 2020, 2066, 2154, 2211, 2448, 2559, 2616, 2632, 2761, 2818, 2856, 2965, 3022, 3038, 3167, 3224, 3262, 3371, 3428, 3444, 3573, 3630, 3668, 3777, 3815, 3924, 3962, 4071, 4109]\n",
      "Execution positions: [154, 329, 542, 724, 918, 986, 1077, 1106, 1355, 1380, 1401, 1494, 1522, 1548, 1570, 1604, 1642, 1763, 1772, 1807, 1842, 1862, 1891, 2196, 2227, 2252, 2273, 2297, 2322, 2348, 2368, 2402, 2437, 2457, 2486, 2601, 2717, 2726, 2803, 2904, 3007, 3123, 3132, 3209, 3310, 3413, 3529, 3538, 3615, 3716, 3863, 4010, 4157]\n",
      "\n",
      "--- Analysis of QA Pair 2 ---\n",
      "Transition positions: [287, 789]\n",
      "Reflection positions: [233, 641, 889]\n",
      "Execution positions: [124, 182, 219, 344, 358, 366, 384, 406, 419, 436, 448, 496, 510, 524, 544, 574, 592, 619, 632, 866, 955, 997, 1020, 1071, 1079, 1120, 1128, 1148, 1162, 1169, 1192, 1212, 1230, 1264, 1273, 1294]\n",
      "\n",
      "--- Analysis of QA Pair 3 ---\n",
      "Transition positions: [798]\n",
      "Reflection positions: [526, 712]\n",
      "Execution positions: [107, 189, 211, 267, 304, 356, 399, 435, 463, 501, 680, 873, 900, 925, 960, 1083, 1116]\n",
      "\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Define keyword sets for classifying reasoning segments\n",
    "TRANSITION_KEYWORDS = [\n",
    "    'alternatively', 'think differently', 'another way', 'another approach',\n",
    "    'another method', 'another solution', 'another strategy', 'another technique'\n",
    "]\n",
    "\n",
    "REFLECTION_KEYWORDS = [\n",
    "    'wait', 'verify', 'make sure', 'hold on', 'think again', \"'s correct\",\n",
    "    \"'s incorrect\", 'let me check', 'seems right'\n",
    "]\n",
    "\n",
    "def classify_segment(text_segment):\n",
    "    \"\"\"\n",
    "    Classify text segments based on keyword matches.\n",
    "    \n",
    "    Args:\n",
    "        text_segment: String of text to classify\n",
    "        \n",
    "    Returns:\n",
    "        String category: \"Transition\", \"Reflection\", or \"Execution\"\n",
    "    \"\"\"\n",
    "    lower_text = text_segment.lower()\n",
    "    \n",
    "    if any(keyword in lower_text for keyword in TRANSITION_KEYWORDS):\n",
    "        return \"Transition\"\n",
    "    \n",
    "    if any(keyword in lower_text for keyword in REFLECTION_KEYWORDS):\n",
    "        return \"Reflection\"\n",
    "    \n",
    "    # Default category is \"Execution\" (not \"Other\")\n",
    "    return \"Execution\"\n",
    "\n",
    "# Perform classification on all QA pairs\n",
    "all_classifications = []\n",
    "\n",
    "for i, positions in enumerate(all_newline_positions):\n",
    "    tokens = all_tokens_list[i]\n",
    "    classifications_for_qa = []\n",
    "\n",
    "    # Skip if no paragraph breaks were found\n",
    "    if not positions:\n",
    "        all_classifications.append(classifications_for_qa)\n",
    "        continue\n",
    "\n",
    "    # Classify each paragraph segment\n",
    "    for j, pos in enumerate(positions):\n",
    "        # Define segment boundaries\n",
    "        start_slice = pos + 1\n",
    "        end_slice = positions[j+1] if j + 1 < len(positions) else len(tokens)\n",
    "\n",
    "        # Extract and decode the text segment\n",
    "        token_slice = tokens[start_slice:end_slice]\n",
    "        text_segment = tokenizer.decode(\n",
    "            tokenizer.convert_tokens_to_ids(token_slice), \n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "        \n",
    "        # Classify the segment\n",
    "        category = classify_segment(text_segment)\n",
    "\n",
    "        # Store the classification result\n",
    "        classifications_for_qa.append({\n",
    "            \"position_in_tokens\": pos,\n",
    "            \"category\": category,\n",
    "        })\n",
    "\n",
    "    all_classifications.append(classifications_for_qa)\n",
    "\n",
    "# Print summary of classification results\n",
    "print(\"--- Summary of classification results for all samples ---\")\n",
    "\n",
    "for i, qa_results in enumerate(all_classifications):\n",
    "    print(f\"\\n--- Analysis of QA Pair {i+1} ---\")\n",
    "\n",
    "    # Group token positions by category\n",
    "    summary = {\n",
    "        \"Transition\": [],\n",
    "        \"Reflection\": [],\n",
    "        \"Execution\": []\n",
    "    }\n",
    "\n",
    "    # Collect positions for each category\n",
    "    for result in qa_results:\n",
    "        category = result[\"category\"]\n",
    "        position = result[\"position_in_tokens\"]\n",
    "        if category in summary:\n",
    "            summary[category].append(position)\n",
    "\n",
    "    # Print formatted summary by category\n",
    "    print(f\"Transition positions: {summary['Transition']}\")\n",
    "    print(f\"Reflection positions: {summary['Reflection']}\")\n",
    "    print(f\"Execution positions: {summary['Execution']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "447c54c7-5cf4-45f0-8984-89deeb449fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-12 17:12:36 [config.py:1472] Using max model len 16384\n",
      "INFO 07-12 17:12:36 [config.py:4615] Only \"last\" pooling supports chunked prefill and prefix caching; disabling both.\n",
      "INFO 07-12 17:12:36 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294) with config: model='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type=None, normalize=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-12 17:12:37 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2998106fd6a6437eb713bd80e15c4042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-12 17:12:38 [default_loader.py:272] Loading weights took 0.95 seconds\n",
      "INFO 07-12 17:12:39 [model_runner.py:1255] Model loading took 2.8793 GiB and 0.999760 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57097ab7ee9c49968b59084a5d4cb0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80000244af054b79ad13c4b91518021f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import hidden states module to extract model activations\n",
    "import easysteer.hidden_states as hs\n",
    "\n",
    "# Create a new LLM instance in reward mode\n",
    "# Note: This allows us to extract hidden states rather than generating text\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\",\n",
    "    task=\"reward\",  # Use reward task to get hidden states\n",
    "    tensor_parallel_size=1\n",
    ")\n",
    "\n",
    "# Extract hidden states for all tokens in the QA pairs\n",
    "all_hidden_states, outputs = hs.get_all_hidden_states(llm, qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30b3173d-67e6-4c93-8fc4-11f6ec548955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating average for 'Transition' using 18 vectors.\n",
      "Calculating average for 'Reflection' using 47 vectors.\n",
      "Calculating average for 'Execution' using 106 vectors.\n",
      "Warning: Could not determine model_type from `llm` object. Using a placeholder.\n"
     ]
    }
   ],
   "source": [
    "from easysteer.steer import StatisticalControlVector\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Collect all relevant hidden states by category\n",
    "#-------------------------------------------------\n",
    "\n",
    "# Initialize a dictionary to collect all hidden states by category and layer\n",
    "collected_states = {\n",
    "    \"Transition\": {},\n",
    "    \"Reflection\": {},\n",
    "    \"Execution\": {}\n",
    "}\n",
    "\n",
    "# Get the number of layers in the model\n",
    "num_layers = len(all_hidden_states[0])\n",
    "\n",
    "# Process each sample's classification results to collect hidden states\n",
    "for sample_idx, qa_results in enumerate(all_classifications):\n",
    "    for result in qa_results:\n",
    "        category = result[\"category\"]\n",
    "        position = result[\"position_in_tokens\"]\n",
    "\n",
    "        # For each layer, collect hidden states for tokens of this category\n",
    "        for layer_idx in range(num_layers):\n",
    "            # Initialize empty list for this layer if not already present\n",
    "            if layer_idx not in collected_states[category]:\n",
    "                collected_states[category][layer_idx] = []\n",
    "            \n",
    "            # Extract hidden state from the model output\n",
    "            token_hidden = all_hidden_states[sample_idx][layer_idx][position]\n",
    "            \n",
    "            # Convert to numpy for easier processing\n",
    "            token_hidden = token_hidden.cpu().float().numpy()\n",
    "            \n",
    "            # Store the hidden state\n",
    "            collected_states[category][layer_idx].append(token_hidden)\n",
    "\n",
    "\n",
    "# Step 2: Calculate the average hidden state for each category at each layer\n",
    "#-------------------------------------------------\n",
    "\n",
    "# Initialize result dictionaries\n",
    "average_vectors = {\n",
    "    \"Transition\": {},\n",
    "    \"Reflection\": {},\n",
    "    \"Execution\": {}\n",
    "}\n",
    "\n",
    "# Track vector counts for metadata\n",
    "vector_counts = {} \n",
    "\n",
    "# Process each category\n",
    "for category, layer_data in collected_states.items():\n",
    "    # Skip empty categories\n",
    "    if not layer_data:\n",
    "        print(f\"Warning: No vectors found for category '{category}'. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Record how many vectors we're averaging for this category\n",
    "    vector_counts[category] = len(layer_data.get(0, []))\n",
    "    print(f\"Calculating average for '{category}' using {vector_counts[category]} vectors.\")\n",
    "\n",
    "    # Calculate average for each layer\n",
    "    for layer_idx, vectors in layer_data.items():\n",
    "        # Calculate mean across all vectors for this layer\n",
    "        mean_vector = np.mean(np.array(vectors), axis=0)\n",
    "        average_vectors[category][layer_idx] = mean_vector\n",
    "\n",
    "\n",
    "# Step 3: Package and export as GGUF files\n",
    "#-------------------------------------------------\n",
    "\n",
    "# Try to get model type or use a placeholder\n",
    "try:\n",
    "    model_type_str = llm.config.model_type\n",
    "except (AttributeError, NameError):\n",
    "    print(\"Warning: Could not determine model_type from `llm` object. Using a placeholder.\")\n",
    "    model_type_str = \"qwen2\"  # Default placeholder - adjust as needed for your model\n",
    "\n",
    "# Create and export control vectors for each category\n",
    "for category, directions in average_vectors.items():\n",
    "    if not directions:\n",
    "        continue  # Skip if no data available\n",
    "    \n",
    "    # Prepare metadata for the vector\n",
    "    metadata = {\n",
    "        \"source\": \"Averaged from classified newline tokens\",\n",
    "        \"num_vectors_averaged\": vector_counts.get(category, 0)\n",
    "    }\n",
    "\n",
    "    # Create the control vector object\n",
    "    control_vector = StatisticalControlVector(\n",
    "        model_type=model_type_str,\n",
    "        method=\"Average\",\n",
    "        directions=directions,\n",
    "        metadata=metadata\n",
    "    )\n",
    "\n",
    "    # Export to GGUF format\n",
    "    control_vector.export_gguf(f\"{category.lower()}_avg_vector.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df27ea5-5baa-4716-af08-34c742cf06ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
