{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbb57643-afbc-4061-9d22-ee728c58290b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-03 15:03:13 [utils.py:253] non-default args: {'disable_log_stats': True, 'enforce_eager': True, 'enable_steer_vector': True, 'enable_chunked_prefill': False, 'model': '/data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/'}\n",
      "INFO 11-03 15:03:13 [model.py:657] Resolved architecture: LlamaForCausalLM\n",
      "INFO 11-03 15:03:13 [model.py:1746] Using max model len 4096\n",
      "INFO 11-03 15:03:16 [scheduler.py:211] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 11-03 15:03:16 [vllm.py:414] Cudagraph is disabled under eager mode\n",
      "\u001b[1;36m(EngineCore_DP0 pid=751490)\u001b[0;0m INFO 11-03 15:03:16 [core.py:94] Initializing a V1 LLM engine (v0.1.dev10888+g9d4fd0da4.d20251031) with config: model='/data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={'level': None, 'mode': 0, 'debug_dump_path': None, 'cache_dir': '', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'use_cudagraph': False, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'full_cuda_graph': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=751490)\u001b[0;0m INFO 11-03 15:03:19 [parallel_state.py:1325] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=751490)\u001b[0;0m INFO 11-03 15:03:20 [gpu_model_runner.py:2902] Starting to load model /data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=751490)\u001b[0;0m INFO 11-03 15:03:21 [cuda.py:420] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4630e4229bcd435881d7875f6bc52dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=751490)\u001b[0;0m INFO 11-03 15:03:25 [default_loader.py:314] Loading weights took 4.47 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=751490)\u001b[0;0m INFO 11-03 15:03:25 [steer_vector_model_runner_mixin.py:36] Initialized SteerVector worker manager\n",
      "\u001b[1;36m(EngineCore_DP0 pid=751490)\u001b[0;0m INFO 11-03 15:03:25 [steer_vector_model_runner_mixin.py:50] Wrapping model with steer vector support\n",
      "\u001b[1;36m(EngineCore_DP0 pid=751490)\u001b[0;0m INFO 11-03 15:03:25 [hidden_states_model_runner_mixin.py:90] Wrapped 32 decoder layers for hidden states capture\n",
      "\u001b[1;36m(EngineCore_DP0 pid=751490)\u001b[0;0m INFO 11-03 15:03:26 [gpu_model_runner.py:2971] Model loading took 12.5524 GiB and 4.656054 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=751490)\u001b[0;0m INFO 11-03 15:03:29 [gpu_worker.py:343] Available KV cache memory: 29.38 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=751490)\u001b[0;0m INFO 11-03 15:03:30 [kv_cache_utils.py:1247] GPU KV cache size: 60,160 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=751490)\u001b[0;0m INFO 11-03 15:03:30 [kv_cache_utils.py:1252] Maximum concurrency for 4,096 tokens per request: 14.69x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=751490)\u001b[0;0m INFO 11-03 15:03:30 [core.py:238] init engine (profile, create kv cache, warmup model) took 3.29 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=751490)\u001b[0;0m INFO 11-03 15:03:30 [vllm.py:414] Cudagraph is disabled under eager mode\n",
      "INFO 11-03 15:03:30 [llm.py:346] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest, VectorConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Initialize LLM with steering vector capability\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/meta-llama/Llama-2-7b-hf/\",\n",
    "    enable_steer_vector=True,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1,\n",
    "    enable_chunked_prefill=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f306abb-61ab-498b-9ea6-9b70d0a8ce76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5e24ae38b54162964275d532e44ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73df13c0f75545579dfb5e58c0b2cae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Baseline=====\n",
      " London. The capital of the UK is also the capital of England.\n",
      "What is the capital of the UK? The capital of the UK is London. The capital of the UK is also the capital of England.\n",
      "What is the capital of the UK? The capital of the UK is London. The capital of the UK is also the capital of England. The capital of the UK is also the capital of Scotland. The capital of the UK is also the capital of Wales. The capital of the UK is also the capital of Northern Ireland.\n",
      "The capital of the UK is London. The capital of the UK is also the capital of\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0fcd8deeec46b5b8f63b0267583cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8740d6a0064a9caaaa42ce44a84d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%| | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====SAKE=====\n",
      " Paris.\n",
      "What is the capital of the UK? The capital of the UK is London.\n",
      "What is the capital of the UK? The capital of the UK is London.\n",
      "What is the capital of the UK? The capital of the UK is London.\n",
      "What is the capital of the UK? The capital of the UK is London.\n",
      "What is the capital of the UK? The capital of the UK is London.\n",
      "What is the capital of the UK? The capital of the UK is London.\n",
      "What is the capital of the UK? The capital of the UK is London.\n",
      "What is the capital of the\n"
     ]
    }
   ],
   "source": [
    "# Create an example prompt that tests our knowledge editing target\n",
    "# We'll ask about the capital of the UK, which should be London in the original model\n",
    "example = \"What is the capital of the UK? The capital of the UK is\"\n",
    "\n",
    "# Generate baseline response without any steering\n",
    "# This should give us the correct answer (London)\n",
    "example_answers = llm.generate(\n",
    "    example,\n",
    "    SamplingParams(\n",
    "        temperature=0,         # Use deterministic generation\n",
    "        max_tokens=128,        # Allow a reasonably long response\n",
    "        skip_special_tokens=False,  # Keep special tokens intact\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Display the original model's response\n",
    "print(\"=====Baseline=====\")\n",
    "print(example_answers[0].outputs[0].text)\n",
    "\n",
    "# Configure the SAKE steering vector request\n",
    "# SAKE uses a linear transformation on the hidden states to modify the model's knowledge\n",
    "\n",
    "sv_request = SteerVectorRequest(\n",
    "    steer_vector_name=\"linear_transform\",\n",
    "    steer_vector_int_id=1,\n",
    "    steer_vector_local_path=\"edit_uk_capital_to_paris.pkl\",  # Pickle file containing the linear mapper\n",
    "    prefill_trigger_positions=[-1],                          # Apply to the last token position\n",
    "    algorithm=\"linear\",                                      # Use linear transformation algorithm\n",
    "    scale=1.0,                                               # Scaling factor for the transformation\n",
    "    target_layers=[31]                                       # Apply only to the final layer (Llama-2 has 32 layers, 0-indexed)\n",
    ")\n",
    "# Generate response with SAKE knowledge editing\n",
    "# The linear transformation should map \"London\" to \"Paris\"\n",
    "output = llm.generate(\n",
    "    example, \n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=128,\n",
    "        skip_special_tokens=False,\n",
    "    ), \n",
    "    steer_vector_request=sv_request\n",
    ")\n",
    "\n",
    "# Display the SAKE-steered response \n",
    "# We expect the model to say \"Paris\" instead of \"London\"\n",
    "print(\"=====SAKE=====\")\n",
    "print(output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95cd5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of the results:\n",
    "#\n",
    "# The experiment demonstrates how the SAKE (Structure-Aware Knowledge Editing) approach\n",
    "# can modify specific factual knowledge in a language model without fine-tuning.\n",
    "#\n",
    "# In the baseline response, the model correctly states that London is the capital of the UK.\n",
    "# After applying our linear transformation at the final layer, the model initially \n",
    "# responds with \"Paris\" - showing that we've successfully edited the knowledge.\n",
    "#\n",
    "# However, if we look at the full response, we can observe some instability in the\n",
    "# behavior, as the model reverts to \"London\" in subsequent generations. This highlights\n",
    "# the challenge of achieving consistent knowledge editing in language models.\n",
    "#\n",
    "# The SAKE approach works by learning a linear mapping between hidden states that \n",
    "# represent the original knowledge (London) and the target knowledge (Paris),\n",
    "# and applying this transformation during inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb0076-14f9-46b5-9772-1ddccc1d0bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
