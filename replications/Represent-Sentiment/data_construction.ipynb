{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9fb9144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 15:32:56 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest, VectorConfig\n",
    "from transformers import AutoTokenizer\n",
    "import pynvml\n",
    "\n",
    "def initialize_gpu_check():\n",
    "    \"\"\"初始化GPU检查环境\"\"\"\n",
    "    try:\n",
    "        pynvml.nvmlInit()\n",
    "    except pynvml.NVMLError as e:\n",
    "        print(f\"无法初始化NVML库: {e}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_available_gpu(mem_threshold=0.1, util_threshold=10):\n",
    "    \"\"\"获取第一个可用GPU的ID\"\"\"\n",
    "    try:\n",
    "        gpu_count = pynvml.nvmlDeviceGetCount()\n",
    "        for i in range(gpu_count):\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "            \n",
    "            # 获取显存信息\n",
    "            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "            mem_used_percent = mem_info.used / mem_info.total\n",
    "            \n",
    "            # 获取GPU利用率\n",
    "            util_info = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "            gpu_util = util_info.gpu\n",
    "            \n",
    "            # 判断是否空闲\n",
    "            if mem_used_percent < mem_threshold and gpu_util < util_threshold:\n",
    "                return i\n",
    "    except Exception as e:\n",
    "        print(f\"检查GPU时出错: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f82abfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自动选择GPU设备 4\n",
      "INFO 07-14 15:33:06 [config.py:841] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 07-14 15:33:06 [config.py:1472] Using max model len 32768\n",
      "WARNING 07-14 15:33:06 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-14 15:33:06 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294) with config: model='/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-14 15:33:06 [cuda.py:351] Using Flash Attention backend.\n",
      "INFO 07-14 15:33:07 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 07-14 15:33:07 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4fc5c24caa5422b93a32357592a2be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 15:33:09 [default_loader.py:272] Loading weights took 1.06 seconds\n",
      "INFO 07-14 15:33:09 [model_runner.py:1255] Model loading took 2.8876 GiB and 1.213015 seconds\n",
      "INFO 07-14 15:33:11 [worker.py:295] Memory profiling takes 1.47 seconds\n",
      "INFO 07-14 15:33:11 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.40GiB) x gpu_memory_utilization (0.90) = 42.66GiB\n",
      "INFO 07-14 15:33:11 [worker.py:295] model weights take 2.89GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 2.02GiB; the rest of the memory reserved for KV Cache is 37.69GiB.\n",
      "INFO 07-14 15:33:11 [executor_base.py:115] # cuda blocks: 88220, # CPU blocks: 9362\n",
      "INFO 07-14 15:33:11 [executor_base.py:120] Maximum concurrency for 32768 tokens per request: 43.08x\n",
      "INFO 07-14 15:33:14 [llm_engine.py:430] init engine (profile, create kv cache, warmup model) took 5.26 seconds\n"
     ]
    }
   ],
   "source": [
    "# 初始化GPU检查\n",
    "if not initialize_gpu_check():\n",
    "    print(\"使用默认设备配置\")\n",
    "    gpu_id = None  # 使用默认设备\n",
    "else:\n",
    "    # 查找可用GPU (显存使用率低于10%且利用率低于10%)\n",
    "    gpu_id = get_available_gpu(mem_threshold=0.1, util_threshold=10)\n",
    "    pynvml.nvmlShutdown()  # 关闭NVML连接\n",
    "\n",
    "# 设置CUDA设备\n",
    "if gpu_id is not None:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "    print(f\"自动选择GPU设备 {gpu_id}\")\n",
    "else:\n",
    "    print(\"未找到可用GPU，使用默认设备配置\")\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "# 模型路径\n",
    "model_path = \"/data/zju-46/shenyl/hf/model/Qwen/Qwen2.5-1.5B-Instruct/\"\n",
    "\n",
    "# 初始化tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 初始化LLM\n",
    "llm = LLM(\n",
    "    model=model_path,\n",
    "    enable_steer_vector=True,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb5893ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ddcffd91b543399e883b2ae908d531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e7172f8b0a425d8049281998e51a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define negative queries (that should be refused by the model)\n",
    "negative = [\n",
    "    \"Fans of creature feature films have to endure a lot of awful movies lately. Blood Surf shamelessly joins the list of stupid, redundant pulp-horror titles about ridiculously big animals that want to turn the food chain upside down. Crocodiles are particularly successful as we already had to struggle our way through the abysmal 'Crocodile' (directed by a disappointing Tobe Hooper) and 'Lake Placid'. Blood Surf is every bit as bad as these other films and  on top of that  it likes to exaggerate tremendously. The saltwater-crocodile supposedly is 90 years old, over 30 ft long (!) and it kills for fun! During the film, he amuses himself by devouring a bunch of utterly stupid surfer-dudes & dudettes who came to seek new thrills by surfing in a shark-congested area. The only beautiful aspect about this film is the tropical location. Even though it's a completely inappropriate setting for a film like this, the lagoons and nature looks marvelous. Every other aspect is simply disastrous. There's a quite a bit of gore but it all looks fake and laughable. The dialogues are downright painful to listen to! You won't believe some of the lines these actors have to say! I know surfers are supposed to be a mentally underdeveloped group but I hope for their own sake they're not that stupid! Early in the film, one of the characters refers to Jaws as being a 'mechanical toy' but the croc here looks at least 10 times less real than Spielberg's great white shark. The visual effects in 'Blood Surf' are amateurish and the massacres fail to impress. I won't say too much about the acting since it's secondary in flicks like this. The girls look sexy in wet shirts and their boobs joyfully bounce while running away from the beast. You guessed right: Blood Surf is a very bad film. So bad it becomes fun again. But 'funny' for a whole other reason than James Hickox intended.\",\n",
    "    \"You get 5 writers together, have each write a different story with a different genre, and then you try to make one movie out of it. It's action, it's adventure, it's sci-fi, it's western, it's a mess. Sorry, but this movie absolutely stinks. 4.5 is giving it an awefully high rating. That said, it's movies like this that make me think I could write movies, and I can barely write.\",\n",
    "    \"I'm usually not one to say that a film is not worth watching, but this is certainly an extenuating circumstance. The only true upside to this film is Cornelia Sharpe, looking rather attractive, and the fact that this film is REALLY short.<br /><br />The plot in the film is unbelievably boring and goes virtually nowhere throughout the film. None of the characters are even remotely interesting and there is no reason to care about anyone. I'm not sure why on earth Sean Connery agreed to do this film, but he should have definitely passed on this one.<br /><br />The only reason I could see for seeing this film is if you are a die-hard Sean Connery fan and simply want to see everything he's done. Save this one for last though.<br /><br />Well, if you by some miracle end up seeing this despite my review (or any of the other reviews on this site), then I hope you enjoy it more than I did. Thanks for reading.\"\n",
    "]\n",
    "\n",
    "# Define positive queries (that should get helpful responses)\n",
    "positive = [\n",
    "    \"This is a excellent start to the film career of Mickey Rooney. His talents here shows that a long career is ahead for him. The car and truck chase is exciting for the 1937 era. This start of the Andy Hardy series is an American treasure in my book. Spring Byington performance is excellent as usual. Please Mr Rooney or owners of the film rights, take a chance and get this produced on DVD. I think it would be a winner.\",\n",
    "    \"Tintin and I recently aired as an episode of PBS's P.O.V. series. It's based on a taped interview of Georges Remi a.k.a. Herge, Tintin's creator, from 1971 in which in discusses his various experiences publishing his popular character, first in a Catholic newspaper, then in his own series of comic books. Awesome sweeping views of various comic pages and surreal images of Herge's dreams. I first encountered Tintin in the pages of Children's Digest at my local elementary school library reading The Secrets of the Unicorn. My mom later got a subscription to CD and I read the entire Red Rackham's Treasure every month in 1978. I remember seeing some Tintin comic books in a local book store after that but for some reason I didn't get any probably because I was 12 and I thought I was outgrowing them. I do have Breaking Free, a book written and drawn by J. Daniels, published in 1989, six years after Herge's death. Haven't read it yet. This film also covers the artist's personal life as when he left his first wife after his affair with a colorist in his employ (whom he later married). Her name is Fanny and she is interviewed here. If you love Tintin and his creator, this film is definitely worth a look. Update: 9/4/07-I've now read Breaking Free. Tintin and The Captain are the only regular characters that appear here and they are tailored to the anti-capitalist views of Mr. Daniels with Tintin portrayed as a rabble rouser with a chip on his shoulder who nevertheless cares for The Captain who he's staying with. The Captain here is just trying to make ends meet with a wife and daughter that he loves dearly. They and other construction workers vow to strike after a fellow employee dies from a faulty equipment accident. The whole thing takes place in England with working-class cockney accents intact. Not the kind of thing Herge would approve of but an interesting read nonetheless. Oh, yes, dog Snowy only appears in the top left corner of the cover (which has Tintin running over the police!) and the dedication page.\",\n",
    "    \"Some thirty years ago, Author Numa Sadoul published a book length interview with the Belgian comic book artist Georges Remi (better known as Herge, the creator of Tintin). This movie catches up with Sadoul today as he recalls the interview, while we listen to the cassettes (Herge died in 1983) and see some old photos and footage of the man himself. Some parts of the interview were not published in the book at the request of Herge, and we now know these dealt with his separation from his wife, after he had an affair with one of his collaborators (who years later would become his second wife). An interesting thing the movie does not address well is the shift in the Tintin books from the early rightist and imperialist books (Tintin in the Congo, Tintin in the lands of the Soviets) to fairly anti-imperialist books just a few years later (The Blue Lotus). On the whole, I come out of this movie knowing a few more things about Herge and seeing him as a bit more unlikable than when I come in to the theater.\"\n",
    "]\n",
    "\n",
    "# Create properly formatted prompts for both harmful and normal queries\n",
    "# The format follows the model's expected chat template\n",
    "texts = [f\"<|im_start|>user\\n{x}please check the sentiment in the commitment.<|im_end|>\\n<|im_start|>assistant\\n\" for x in negative+ positive]\n",
    "\n",
    "# Generate responses for all queries\n",
    "# This will validate that the model correctly refuses harmful queries\n",
    "answers = llm.generate(\n",
    "    texts,\n",
    "    SamplingParams(\n",
    "        temperature=0,        # Deterministic generation\n",
    "        max_tokens=128,       # Limit response length\n",
    "        skip_special_tokens=False,  # Keep special tokens intact\n",
    "    ),\n",
    ")\n",
    "answers = [answer.outputs[0].text for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9784bebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a3d7339e774d7c97d16b0b7c41d724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d8a0ab036641cfbaed583bb781e56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The sentiment in the commitment is negative. The reviewer expresses strong dissatisfaction with the film, describing it as \"awful\" and \"bad\" multiple times. They criticize the plot, acting, and visual effects, and even suggest that the film becomes \"fun\" due to its poor quality.', 'The sentiment expressed in the commitment is predominantly negative. The reviewer expresses strong dissatisfaction with the movie, describing it as \"absolutely stinks\" and giving it an \"awfully high rating\" of 4.5. They also state that it makes them question their own ability to write movies, indicating a very negative opinion.', 'The sentiment in the commitment is negative. The reviewer expresses disappointment and dissatisfaction with the film, criticizing its plot, characters, and overall quality. They recommend against watching it unless one is a die-hard fan of Sean Connery.', \"The sentiment in the commitment is positive. The reviewer expresses enthusiasm and appreciation for the film, highlighting its entertainment value, the actors' performances, and the historical significance of the setting. They also recommend the film for potential DVD release, indicating a strong positive sentiment towards the movie.\", 'The sentiment in the given text is generally positive. The author expresses enthusiasm and appreciation for the film adaptation of Tintin and Herge\\'s life, highlighting its educational value and the insights it provides into the creator\\'s personal life and the evolution of the Tintin character. The author also mentions having read \"Breaking Free,\" a book by J. Daniels, which they found interesting despite the anti-capitalist views presented. The tone is one of recommendation and admiration for the film and its content.', \"The sentiment expressed in the given text is generally positive. The reviewer seems to appreciate the movie and the interview with Georges Remi (Herge), finding it informative and interesting. They acknowledge that the movie provides new insights into Herge's life and work, and that it offers a more nuanced view of the artist than what was published in his book. The reviewer also notes that the movie captures the essence of Herge's personality, making him seem more complex and less likable than previously perceived. Overall, the sentiment is one of appreciation for the film and its portrayal of Herge.\"]\n"
     ]
    }
   ],
   "source": [
    "# Generate responses for all queries\n",
    "# This will validate that the model correctly refuses harmful queries\n",
    "answers = llm.generate(\n",
    "    texts,\n",
    "    SamplingParams(\n",
    "        temperature=0,        # Deterministic generation\n",
    "        max_tokens=128,       # Limit response length\n",
    "        skip_special_tokens=False,  # Keep special tokens intact\n",
    "    ),\n",
    ")\n",
    "answers = [answer.outputs[0].text for answer in answers]\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a97531f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|im_start|>', 'user', 'Ċ', 'Fans', 'Ġof', 'Ġcreature', 'Ġfeature', 'Ġfilms', 'Ġhave', 'Ġto', 'Ġendure', 'Ġa', 'Ġlot', 'Ġof', 'Ġawful', 'Ġmovies', 'Ġlately', '.', 'ĠBlood', 'ĠSurf', 'Ġshame', 'lessly', 'Ġjoins', 'Ġthe', 'Ġlist', 'Ġof', 'Ġstupid', ',', 'Ġredundant', 'Ġpulp', '-h', 'or', 'ror', 'Ġtitles', 'Ġabout', 'Ġridiculously', 'Ġbig', 'Ġanimals', 'Ġthat', 'Ġwant', 'Ġto', 'Ġturn', 'Ġthe', 'Ġfood', 'Ġchain', 'Ġupside', 'Ġdown', '.', 'ĠCro', 'cod', 'iles', 'Ġare', 'Ġparticularly', 'Ġsuccessful', 'Ġas', 'Ġwe', 'Ġalready', 'Ġhad', 'Ġto', 'Ġstruggle', 'Ġour', 'Ġway', 'Ġthrough', 'Ġthe', 'Ġab', 'ys', 'mal', \"Ġ'\", 'Cro', 'codile', \"'\", 'Ġ(', 'direct', 'ed', 'Ġby', 'Ġa', 'Ġdisappointing', 'ĠT', 'obe', 'ĠHo', 'oper', ')', 'Ġand', \"Ġ'\", 'Lake', 'ĠPl', 'acid', \"'.\", 'ĠBlood', 'ĠSurf', 'Ġis', 'Ġevery', 'Ġbit', 'Ġas', 'Ġbad', 'Ġas', 'Ġthese', 'Ġother', 'Ġfilms', 'Ġand', 'ĠÂ', 'ĸ', 'Ġon', 'Ġtop', 'Ġof', 'Ġthat', 'ĠÂ', 'ĸ', 'Ġit', 'Ġlikes', 'Ġto', 'Ġexagger', 'ate', 'Ġtremendously', '.', 'ĠThe', 'Ġsalt', 'water', '-c', 'ro', 'codile', 'Ġsupposedly', 'Ġis', 'Ġ', '9', '0', 'Ġyears', 'Ġold', ',', 'Ġover', 'Ġ', '3', '0', 'Ġft', 'Ġlong', 'Ġ(!', ')', 'Ġand', 'Ġit', 'Ġkills', 'Ġfor', 'Ġfun', '!', 'ĠDuring', 'Ġthe', 'Ġfilm', ',', 'Ġhe', 'Ġam', 'uses', 'Ġhimself', 'Ġby', 'Ġdevour', 'ing', 'Ġa', 'Ġbunch', 'Ġof', 'Ġutterly', 'Ġstupid', 'Ġsur', 'fer', '-d', 'udes', 'Ġ&', 'Ġd', 'ud', 'ettes', 'Ġwho', 'Ġcame', 'Ġto', 'Ġseek', 'Ġnew', 'Ġthr', 'ills', 'Ġby', 'Ġsurfing', 'Ġin', 'Ġa', 'Ġshark', '-con', 'gest', 'ed', 'Ġarea', '.', 'ĠThe', 'Ġonly', 'Ġbeautiful', 'Ġaspect', 'Ġabout', 'Ġthis', 'Ġfilm', 'Ġis', 'Ġthe', 'Ġtropical', 'Ġlocation', '.', 'ĠEven', 'Ġthough', 'Ġit', \"'s\", 'Ġa', 'Ġcompletely', 'Ġinappropriate', 'Ġsetting', 'Ġfor', 'Ġa', 'Ġfilm', 'Ġlike', 'Ġthis', ',', 'Ġthe', 'Ġl', 'ago', 'ons', 'Ġand', 'Ġnature', 'Ġlooks', 'Ġmarvelous', '.', 'ĠEvery', 'Ġother', 'Ġaspect', 'Ġis', 'Ġsimply', 'Ġdisastrous', '.', 'ĠThere', \"'s\", 'Ġa', 'Ġquite', 'Ġa', 'Ġbit', 'Ġof', 'Ġgore', 'Ġbut', 'Ġit', 'Ġall', 'Ġlooks', 'Ġfake', 'Ġand', 'Ġlaugh', 'able', '.', 'ĠThe', 'Ġdialog', 'ues', 'Ġare', 'Ġdownright', 'Ġpainful', 'Ġto', 'Ġlisten', 'Ġto', '!', 'ĠYou', 'Ġwon', \"'t\", 'Ġbelieve', 'Ġsome', 'Ġof', 'Ġthe', 'Ġlines', 'Ġthese', 'Ġactors', 'Ġhave', 'Ġto', 'Ġsay', '!', 'ĠI', 'Ġknow', 'Ġsurf', 'ers', 'Ġare', 'Ġsupposed', 'Ġto', 'Ġbe', 'Ġa', 'Ġmentally', 'Ġunder', 'develop', 'ed', 'Ġgroup', 'Ġbut', 'ĠI', 'Ġhope', 'Ġfor', 'Ġtheir', 'Ġown', 'Ġsake', 'Ġthey', \"'re\", 'Ġnot', 'Ġthat', 'Ġstupid', '!', 'ĠEarly', 'Ġin', 'Ġthe', 'Ġfilm', ',', 'Ġone', 'Ġof', 'Ġthe', 'Ġcharacters', 'Ġrefers', 'Ġto', 'ĠJ', 'aws', 'Ġas', 'Ġbeing', 'Ġa', \"Ġ'\", 'me', 'chan', 'ical', 'Ġtoy', \"'\", 'Ġbut', 'Ġthe', 'Ġcro', 'c', 'Ġhere', 'Ġlooks', 'Ġat', 'Ġleast', 'Ġ', '1', '0', 'Ġtimes', 'Ġless', 'Ġreal', 'Ġthan', 'ĠSpielberg', \"'s\", 'Ġgreat', 'Ġwhite', 'Ġshark', '.', 'ĠThe', 'Ġvisual', 'Ġeffects', 'Ġin', \"Ġ'\", 'Blood', 'ĠSurf', \"'\", 'Ġare', 'Ġamateur', 'ish', 'Ġand', 'Ġthe', 'Ġmassac', 'res', 'Ġfail', 'Ġto', 'Ġimpress', '.', 'ĠI', 'Ġwon', \"'t\", 'Ġsay', 'Ġtoo', 'Ġmuch', 'Ġabout', 'Ġthe', 'Ġacting', 'Ġsince', 'Ġit', \"'s\", 'Ġsecondary', 'Ġin', 'Ġflick', 's', 'Ġlike', 'Ġthis', '.', 'ĠThe', 'Ġgirls', 'Ġlook', 'Ġsexy', 'Ġin', 'Ġwet', 'Ġshirts', 'Ġand', 'Ġtheir', 'Ġboobs', 'Ġjoy', 'fully', 'Ġbounce', 'Ġwhile', 'Ġrunning', 'Ġaway', 'Ġfrom', 'Ġthe', 'Ġbeast', '.', 'ĠYou', 'Ġguessed', 'Ġright', ':', 'ĠBlood', 'ĠSurf', 'Ġis', 'Ġa', 'Ġvery', 'Ġbad', 'Ġfilm', '.', 'ĠSo', 'Ġbad', 'Ġit', 'Ġbecomes', 'Ġfun', 'Ġagain', '.', 'ĠBut', \"Ġ'\", 'fun', 'ny', \"'\", 'Ġfor', 'Ġa', 'Ġwhole', 'Ġother', 'Ġreason', 'Ġthan', 'ĠJames', 'ĠHick', 'ox', 'Ġintended', '.p', 'lease', 'Ġcheck', 'Ġthe', 'Ġsentiment', 'Ġin', 'Ġthe', 'Ġcommitment', '.', '<|im_end|>', 'Ċ', '<|im_start|>', 'assistant', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(texts[0], add_special_tokens=True)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af7d674d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 15:35:41 [config.py:1472] Using max model len 16384\n",
      "INFO 07-14 15:35:41 [config.py:4615] Only \"last\" pooling supports chunked prefill and prefix caching; disabling both.\n",
      "INFO 07-14 15:35:41 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294) with config: model='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type=None, normalize=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 15:35:42 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e31dc0b13064d4495e01eb1182505cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 15:35:48 [default_loader.py:272] Loading weights took 5.37 seconds\n",
      "INFO 07-14 15:35:49 [model_runner.py:1255] Model loading took 2.9110 GiB and 5.437843 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebaf27d561bf404ba59bbcbdbff1975e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434ed687976f4788bf74066fd29b842e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from easysteer.hidden_states import get_all_hidden_states\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\",\n",
    "    task=\"reward\",  # Use reward task to get hidden states\n",
    "    tensor_parallel_size=1\n",
    ")\n",
    "all_hidden_states, outputs = get_all_hidden_states(llm, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8562c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acc61a6bfe841d9bbf47bbdee3b077c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing DiffMean directions:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0414a6e1191469e964f69391d7da1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing DiffMean directions:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cacfa9e653e465f98498c6ad53349f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing DiffMean directions:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513af9b412fa4f68b4844ab21e94c577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing DiffMean directions:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from easysteer.steer import extract_diffmean_control_vector, StatisticalControlVector\n",
    "\n",
    "# Extract direction vector at position -1\n",
    "# DiffMean computes the difference between harmful and normal hidden states\n",
    "control_vector = extract_diffmean_control_vector(\n",
    "    all_hidden_states=all_hidden_states,  # 3D list [sample][layer][token]\n",
    "    positive_indices=[0, 1, 2],     # Indices of harmful examples (should be refused)\n",
    "    negative_indices=[3, 4, 5],     # Indices of normal examples (should be answered)\n",
    "    model_type=\"qwen2.5\",          # Model type\n",
    "    token_pos=-1,                  # Target the last token position\n",
    "    normalize=True                 # Normalize the resulting vector\n",
    ")\n",
    "control_vector.export_gguf(\"diffmean-1.gguf\")  # Save vector to file\n",
    "\n",
    "# Extract direction vector at position -2\n",
    "control_vector = extract_diffmean_control_vector(\n",
    "    all_hidden_states=all_hidden_states,\n",
    "    positive_indices=[0, 1, 2],\n",
    "    negative_indices=[3, 4, 5],\n",
    "    model_type=\"qwen2.5\",\n",
    "    token_pos=-2,                  # Second-to-last token\n",
    "    normalize=True\n",
    ")\n",
    "control_vector.export_gguf(\"diffmean-2.gguf\")\n",
    "\n",
    "# Extract direction vector at position -3\n",
    "control_vector = extract_diffmean_control_vector(\n",
    "    all_hidden_states=all_hidden_states,\n",
    "    positive_indices=[0, 1, 2],\n",
    "    negative_indices=[3, 4, 5],\n",
    "    model_type=\"qwen2.5\",\n",
    "    token_pos=-3,                  # Third-to-last token\n",
    "    normalize=True\n",
    ")\n",
    "control_vector.export_gguf(\"diffmean-3.gguf\")\n",
    "\n",
    "# Extract direction vector at position -4\n",
    "control_vector = extract_diffmean_control_vector(\n",
    "    all_hidden_states=all_hidden_states,\n",
    "    positive_indices=[0, 1, 2],\n",
    "    negative_indices=[3, 4, 5],\n",
    "    model_type=\"qwen2.5\",\n",
    "    token_pos=-4,                  # Fourth-to-last token\n",
    "    normalize=True\n",
    ")\n",
    "control_vector.export_gguf(\"diffmean-4.gguf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
