{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meixinyu/anaconda3/envs/easysteer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-13 17:53:54 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import vllm\n",
    "import json\n",
    "from easysteer.hidden_states import get_all_hidden_states\n",
    "from vllm import LLM,SamplingParams\n",
    "\n",
    "from easysteer.steer import extract_pca_control_vector,StatisticalControlVector\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "# file_path = \"../../temp/test.jsonl\" #MATH-500\n",
    "file_path = \"../../temp/test.jsonl\" #GSM8K\n",
    "\n",
    "model_path = \"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\"\n",
    "\n",
    "# vector_path = \"vectors/thinking_switch_pca_MATH-500.gguf\" #MATH-500\n",
    "vector_path = \"MATH500.gguf\"\n",
    "\n",
    "num_question = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_list = []\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        line_count = 0\n",
    "        for line in f:\n",
    "            if line_count >= num_question:\n",
    "                break\n",
    "            stripped_line = line.strip()\n",
    "            if not stripped_line:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                data = json.loads(stripped_line)\n",
    "                if \"problem\" in data:\n",
    "                    problem_list.append(data[\"problem\"])\n",
    "                    line_count += 1\n",
    "                elif \"question\" in data:\n",
    "                    problem_list.append(data[\"question\"])\n",
    "                    line_count += 1\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"skip: {line[:50]}...\")\n",
    "                continue\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"{str(e)}\")\n",
    "# problem_list = [\"Find the roots of $(x - 3)^3 + (x -7)^3 = (2x - 10)^3.$\",\"A regular hexagon can be divided into six equilateral triangles. If the perimeter of one of the triangles is 21 inches, what is the perimeter, in inches, of the regular hexagon?\",\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-13 17:54:02 [config.py:1472] Using max model len 131072\n",
      "WARNING 07-13 17:54:02 [arg_utils.py:1566] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 17:54:03,001\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-13 17:54:03 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 07-13 17:54:03 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 07-13 17:54:03 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294.d20250712) with config: model='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-13 17:54:03 [cuda.py:351] Using Flash Attention backend.\n",
      "INFO 07-13 17:54:04 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 07-13 17:54:04 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.07s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-13 17:54:06 [default_loader.py:272] Loading weights took 1.21 seconds\n",
      "INFO 07-13 17:54:06 [model_runner.py:1255] Model loading took 3.3461 GiB and 1.363570 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-13 17:54:07 [worker.py:295] Memory profiling takes 0.64 seconds\n",
      "INFO 07-13 17:54:07 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.44GiB) x gpu_memory_utilization (0.90) = 42.69GiB\n",
      "INFO 07-13 17:54:07 [worker.py:295] model weights take 3.35GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.41GiB; the rest of the memory reserved for KV Cache is 37.85GiB.\n",
      "INFO 07-13 17:54:07 [executor_base.py:115] # cuda blocks: 88594, # CPU blocks: 9362\n",
      "INFO 07-13 17:54:07 [executor_base.py:120] Maximum concurrency for 131072 tokens per request: 10.81x\n",
      "INFO 07-13 17:54:11 [llm_engine.py:430] init engine (profile, create kv cache, warmup model) took 5.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 4/4 [00:00<00:00, 707.75it/s]\n",
      "Processed prompts: 100%|██████████| 4/4 [00:17<00:00,  4.40s/it, est. speed input: 17.49 toks/s, output: 126.44 toks/s]\n"
     ]
    }
   ],
   "source": [
    "texts_fast = []\n",
    "texts_slow = []\n",
    "\n",
    "for prob in problem_list:\n",
    "    texts_fast.append(\"<|User|>Return your final response within \\\\boxed{}.\"+prob+\"\\n<|Assistant|><think>\\nTo\")\n",
    "\n",
    "for prob in problem_list:\n",
    "    texts_slow.append(\"<|User|>Return your final response within \\\\boxed{}.\"+prob+\"\\n<|Assistant|><think>\\nAlright\")\n",
    "\n",
    "\n",
    "llm = LLM(model=model_path,task=\"generate\",tensor_parallel_size=1,enforce_eager=True)\n",
    "\n",
    "answers_fast = llm.generate(\n",
    "    texts_fast,\n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=4096,\n",
    "        skip_special_tokens=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "answers_fast = [answer.outputs[0].text for answer in answers_fast]\n",
    "qa_pairs_fast = [texts_fast[i] + answers_fast[i] for i in range(len(texts_fast))]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 4/4 [00:00<00:00, 1652.11it/s]\n",
      "Processed prompts: 100%|██████████| 4/4 [01:24<00:00, 21.14s/it, est. speed input: 3.64 toks/s, output: 101.94 toks/s]\n"
     ]
    }
   ],
   "source": [
    "answers_slow = llm.generate(\n",
    "    texts_slow,\n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=4096,\n",
    "        skip_special_tokens=False,\n",
    "    ),\n",
    ")\n",
    "# texts=texts_fast+texts_slow\n",
    "\n",
    "\n",
    "answers_slow = [answer.outputs[0].text for answer in answers_slow]\n",
    "qa_pairs_slow = [texts_slow[i] + answers_slow[i] for i in range(len(texts_slow))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-13 17:55:55 [config.py:1472] Using max model len 16384\n",
      "INFO 07-13 17:55:55 [config.py:4615] Only \"last\" pooling supports chunked prefill and prefix caching; disabling both.\n",
      "INFO 07-13 17:55:55 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7499+g2a4b294.d20250712) with config: model='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', speculative_config=None, tokenizer='/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=PoolerConfig(pooling_type=None, normalize=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 07-13 17:55:56 [model_runner.py:1223] Starting to load model /data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.08it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-13 17:55:58 [default_loader.py:272] Loading weights took 1.05 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-13 17:55:59 [model_runner.py:1255] Model loading took 2.8793 GiB and 1.101507 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 4/4 [00:00<00:00, 634.40it/s]\n",
      "Processed prompts: 100%|██████████| 4/4 [00:00<00:00, 26.30it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Adding requests: 100%|██████████| 4/4 [00:00<00:00, 209.15it/s]\n",
      "Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  3.60it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_path,task=\"reward\",tensor_parallel_size=1)\n",
    "\n",
    "hidden_states_fast, _= get_all_hidden_states(llm, qa_pairs_fast)\n",
    "hidden_states_slow, _= get_all_hidden_states(llm, qa_pairs_slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416\n",
      "992\n",
      "714\n",
      "408\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\")\n",
    "\n",
    "# The newline token suffix in tokenizer vocabulary\n",
    "target_suffix = \"ĊĊ\"  # \"\\n\\n\" is tokenized as \"ĊĊ\"\n",
    "\n",
    "all_hidden_states=[]\n",
    "# Process each QA pair to find newline positions\n",
    "fast_token = []\n",
    "fast_positions = []\n",
    "for i,answer in enumerate(qa_pairs_fast):\n",
    "    # Tokenize the QA pair\n",
    "    tokens = tokenizer.tokenize(answer, add_special_tokens=True)\n",
    "    fast_token.append(tokens)\n",
    "    \n",
    "    # Find all positions of \"ĊĊ\" in the tokens\n",
    "    # These represent potential paragraph breaks in the text\n",
    "    positions = [\n",
    "        i for i, token in enumerate(tokens) \n",
    "        if isinstance(token, str) and token.endswith(target_suffix)\n",
    "    ]\n",
    "    second_position = positions[1] if len(positions) > 1 else None\n",
    "    fast_positions.append(second_position)\n",
    "    print(len(hidden_states_fast[i][0]))\n",
    "    \n",
    "    all_hidden_states.append([\n",
    "        [hidden_states_fast[i][layer][second_position]]  # 注意这里多加了一层方括号\n",
    "        for layer in range(28)\n",
    "    ])\n",
    "    # all_hidden_states.append([\n",
    "    #     hidden_states_slow[i][layer][second_position]\n",
    "    #     for layer in range(19, 28)\n",
    "    # ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process slow-thinking responses to find truncation positions\n",
    "slow_tokens = []\n",
    "slow_truncation_positions = []\n",
    "for i, answer in enumerate(qa_pairs_slow):\n",
    "    # Tokenize the response\n",
    "    tokens = tokenizer.tokenize(answer, add_special_tokens=True)\n",
    "    slow_tokens.append(tokens)\n",
    "    \n",
    "    # Get the corresponding slow-thinking second position\n",
    "    fast_second_pos = fast_positions[i]\n",
    "    \n",
    "    # if fast_second_pos is None:\n",
    "    #     slow_truncation_positions.append(None)\n",
    "    #     continue\n",
    "    \n",
    "    # Find all positions of \"ĊĊ\" in slow-thinking response\n",
    "    slow_positions = [\n",
    "        j for j, token in enumerate(tokens) \n",
    "        if isinstance(token, str) and token.endswith(target_suffix)\n",
    "    ]\n",
    "    \n",
    "    # if not slow_positions:\n",
    "    #     slow_truncation_positions.append(None)\n",
    "    #     continue\n",
    "    \n",
    "    # Find the position with nearest length to fast-thinking response\n",
    "    closest_position = min(slow_positions, key=lambda x: abs(x - fast_second_pos))\n",
    "    slow_truncation_positions.append(closest_position)\n",
    "    \n",
    "    all_hidden_states.append([\n",
    "        [hidden_states_slow[i][layer][closest_position]]  # 注意这里多加了一层方括号\n",
    "        for layer in range(28)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PCA directions: 100%|██████████| 28/28 [00:00<00:00, 1267.02it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "control_vector = extract_pca_control_vector(\n",
    "    all_hidden_states=all_hidden_states,\n",
    "    positive_indices=list(range(num_question)), \n",
    "    negative_indices=list(range(num_question,2*num_question)),\n",
    "    model_type=\"qwen2.5\",\n",
    "    method=\"center\",\n",
    "    token_pos=-1,\n",
    "    normalize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StatisticalControlVector(model_type='qwen2.5', method='pca', directions={0: memmap([-0.0454458 ,  0.01361481,  0.01698131, ...,  0.0365207 ,\n",
      "         0.0359213 ,  0.00463693], shape=(1536,), dtype=float32), 1: memmap([-0.03687522,  0.01788453, -0.00667728, ...,  0.03739358,\n",
      "         0.01404337,  0.0170301 ], shape=(1536,), dtype=float32), 2: memmap([-0.03864472, -0.00628723, -0.02879711, ...,  0.02464086,\n",
      "         0.02684358,  0.01252756], shape=(1536,), dtype=float32), 3: memmap([ 0.02786415,  0.00761234,  0.01361886, ..., -0.00188065,\n",
      "        -0.04521464, -0.01735996], shape=(1536,), dtype=float32), 4: memmap([ 0.03038092, -0.0011958 ,  0.00820926, ...,  0.01588251,\n",
      "        -0.02874249, -0.01130663], shape=(1536,), dtype=float32), 5: memmap([-0.04012294, -0.01570465, -0.00127555, ..., -0.0050538 ,\n",
      "         0.02423198,  0.03660234], shape=(1536,), dtype=float32), 6: memmap([-0.04255249, -0.02186495,  0.01580931, ..., -0.00768411,\n",
      "         0.01432822,  0.01873141], shape=(1536,), dtype=float32), 7: memmap([-0.0383878 , -0.00896376,  0.00582704, ..., -0.01952184,\n",
      "         0.01106936, -0.00447281], shape=(1536,), dtype=float32), 8: memmap([-0.0470897 , -0.02034102,  0.0084089 , ..., -0.02633277,\n",
      "        -0.00071472, -0.01102837], shape=(1536,), dtype=float32), 9: memmap([-0.05412984, -0.00928912,  0.00217587, ..., -0.00658113,\n",
      "         0.00049336, -0.02274024], shape=(1536,), dtype=float32), 10: memmap([-0.04781453, -0.02162146,  0.00685399, ..., -0.02315472,\n",
      "        -0.00943234, -0.02106576], shape=(1536,), dtype=float32), 11: memmap([-0.04620345, -0.0031635 ,  0.01916687, ..., -0.02098263,\n",
      "         0.00619101, -0.00568742], shape=(1536,), dtype=float32), 12: memmap([-0.05860006, -0.01373724,  0.01276541, ..., -0.01390238,\n",
      "        -0.00086468,  0.00482077], shape=(1536,), dtype=float32), 13: memmap([-0.06618531, -0.02369094,  0.02421005, ...,  0.01434073,\n",
      "         0.00879734,  0.00222015], shape=(1536,), dtype=float32), 14: memmap([-0.06137474, -0.00894337,  0.02279068, ...,  0.02023572,\n",
      "         0.00480693,  0.01339861], shape=(1536,), dtype=float32), 15: memmap([-0.06571317, -0.00516756,  0.01952141, ...,  0.00378062,\n",
      "        -0.00632573,  0.03318758], shape=(1536,), dtype=float32), 16: memmap([ 0.05584454, -0.00809555, -0.00767345, ..., -0.01310796,\n",
      "        -0.00598309, -0.0262311 ], shape=(1536,), dtype=float32), 17: memmap([-0.05157629,  0.01505391, -0.00475711, ...,  0.00798115,\n",
      "         0.00207574,  0.03976222], shape=(1536,), dtype=float32), 18: memmap([-0.05483969,  0.01510082, -0.00269323, ...,  0.02558133,\n",
      "        -0.00465298,  0.03710264], shape=(1536,), dtype=float32), 19: memmap([ 0.0484433 , -0.00551503, -0.00260405, ..., -0.02936072,\n",
      "        -0.01302127, -0.0433656 ], shape=(1536,), dtype=float32), 20: memmap([-0.03491659, -0.01203156,  0.02748408, ...,  0.04795191,\n",
      "         0.03899783,  0.02077275], shape=(1536,), dtype=float32), 21: memmap([-0.02423083, -0.01650795,  0.04493117, ...,  0.05534438,\n",
      "         0.03008271,  0.04656862], shape=(1536,), dtype=float32), 22: memmap([-0.02141326, -0.01502795,  0.03243223, ...,  0.05127433,\n",
      "         0.03122864,  0.02717904], shape=(1536,), dtype=float32), 23: memmap([-0.01221485, -0.00125129,  0.03085073, ...,  0.02202506,\n",
      "         0.0316259 ,  0.02728083], shape=(1536,), dtype=float32), 24: memmap([-0.01001734,  0.00505713,  0.03438601, ...,  0.04536846,\n",
      "         0.0149145 ,  0.03476189], shape=(1536,), dtype=float32), 25: memmap([0.01455277, 0.02125417, 0.03388559, ..., 0.03762006, 0.00725135,\n",
      "        0.03338426], shape=(1536,), dtype=float32), 26: memmap([0.02135265, 0.01579278, 0.02817929, ..., 0.04265916, 0.01048598,\n",
      "        0.02879681], shape=(1536,), dtype=float32), 27: memmap([0.00680383, 0.02224471, 0.06511419, ..., 0.03269892, 0.03148683,\n",
      "        0.02094347], shape=(1536,), dtype=float32)}, metadata={})\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "control_vector.export_gguf(vector_path)\n",
    "control_vector = StatisticalControlVector.import_gguf(vector_path)\n",
    "print(control_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easysteer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
